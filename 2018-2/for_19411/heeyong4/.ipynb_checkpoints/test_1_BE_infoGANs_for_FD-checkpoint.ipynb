{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum infoGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_infoGANs_for_FD'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     11,
     44,
     88,
     120,
     154
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_epoch = 100\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.001\n",
    "c_size = 10\n",
    "\n",
    "\n",
    "def G(x,c,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "        x_concat = tf.concat([x,c],3)\n",
    "        conv1 = tf.layers.conv2d_transpose(x_concat,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "        #\n",
    "        \n",
    "        fc0  = tf.reshape(conv4, (-1, 4*4*512))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[4*4*512, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "        fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "        \n",
    "        \n",
    "    r5 = tf.nn.tanh(tf.layers.batch_normalization(conv5,training=isTrain), name = name)#4*4*512\n",
    "  \n",
    "    return r5, tf.reshape(fc1,(-1,1,1,c_size))\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "def Q_cat(x,reuse = False, name = 'Q_cat') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('Q_cat', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        fc0  = tf.reshape(x, (-1, 100))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[100, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "    fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "    \n",
    "    return tf.reshape(fc1, (-1,1,1,c_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "c = tf.placeholder(tf.float32,shape=(None,1,1,c_size),name = 'c')    #x_z = G(z,c)\n",
    "\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,c,name='G_sample') # G(z)\n",
    "E_z, E_c = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z,E_c, isTrain, reuse=True, name ='re_image')\n",
    "re_z, re_c = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "\n",
    "re_z_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z - z)**2, axis=[1,2,3])) , name = 're_z_loss') \n",
    "re_c_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(re_c + 1e-8), axis = [1,2,3]),name = 're_c_loss')\n",
    "\n",
    "E_loss = tf.add(re_z_loss, re_c_loss, name = 'E_loss')            \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "Q_fake = Q_cat(D_enc(G_sample, isTrain,reuse=True), reuse=False, name='Q_fake')\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "Q_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(Q_fake + 1e-8), axis = (1,2,3)),name = 'Q_loss')\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "Q_vars = [var for var in T_vars if var.name.startswith('Q')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss + Q_loss, var_list=G_vars+Q_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -143.102, D_real_e : 51.487, D_fake_e : 35.457, G_e : 35.858, Q_e : 2.010, E_e : 6.952, new_measure : 66.013, k_curr : 0.512624\n",
      "D_e : 28.546, D_real_e : 39.033, D_fake_e : 26.575, G_e : 27.360, Q_e : 2.069, E_e : 5.144, new_measure : 40.819, k_curr : 0.410065\n",
      "D_e : 27.132, D_real_e : 34.325, D_fake_e : 23.477, G_e : 24.063, Q_e : 1.971, E_e : 4.253, new_measure : 35.640, k_curr : 0.309858\n",
      "D_e : 24.895, D_real_e : 29.729, D_fake_e : 20.427, G_e : 20.848, Q_e : 1.800, E_e : 3.806, new_measure : 31.201, k_curr : 0.205842\n",
      "D_e : 22.804, D_real_e : 25.774, D_fake_e : 17.797, G_e : 18.092, Q_e : 1.610, E_e : 3.528, new_measure : 27.339, k_curr : 0.064200\n",
      "D_e : 20.739, D_real_e : 22.644, D_fake_e : 15.715, G_e : 15.909, Q_e : 1.443, E_e : 3.501, new_measure : 24.236, k_curr : -0.100971\n",
      "D_e : 18.805, D_real_e : 20.094, D_fake_e : 13.832, G_e : 14.016, Q_e : 1.303, E_e : 3.375, new_measure : 21.605, k_curr : 0.038546\n",
      "D_e : 17.064, D_real_e : 17.594, D_fake_e : 12.326, G_e : 12.401, Q_e : 1.188, E_e : 3.283, new_measure : 19.088, k_curr : -0.200633\n",
      "D_e : 15.360, D_real_e : 15.551, D_fake_e : 10.743, G_e : 10.833, Q_e : 1.089, E_e : 3.307, new_measure : 16.845, k_curr : -0.054603\n",
      "D_e : 13.329, D_real_e : 13.668, D_fake_e : 9.477, G_e : 9.562, Q_e : 1.002, E_e : 3.270, new_measure : 14.792, k_curr : -0.038989\n",
      "D_e : 12.146, D_real_e : 12.260, D_fake_e : 8.490, G_e : 8.576, Q_e : 0.927, E_e : 3.248, new_measure : 13.319, k_curr : -0.022452\n",
      "D_e : 10.944, D_real_e : 11.195, D_fake_e : 7.744, G_e : 7.812, Q_e : 0.861, E_e : 3.254, new_measure : 12.058, k_curr : 0.047032\n",
      "D_e : 9.987, D_real_e : 10.244, D_fake_e : 7.108, G_e : 7.180, Q_e : 0.804, E_e : 3.217, new_measure : 10.954, k_curr : 0.022906\n",
      "D_e : 9.560, D_real_e : 9.537, D_fake_e : 6.651, G_e : 6.689, Q_e : 0.753, E_e : 3.298, new_measure : 10.360, k_curr : -0.015398\n",
      "D_e : 8.826, D_real_e : 9.081, D_fake_e : 6.257, G_e : 6.357, Q_e : 0.708, E_e : 3.308, new_measure : 9.715, k_curr : -0.016595\n",
      "D_e : 8.427, D_real_e : 8.514, D_fake_e : 5.862, G_e : 5.938, Q_e : 0.668, E_e : 3.301, new_measure : 9.137, k_curr : 0.043192\n",
      "D_e : 7.922, D_real_e : 8.122, D_fake_e : 5.590, G_e : 5.671, Q_e : 0.632, E_e : 3.295, new_measure : 8.666, k_curr : 0.082669\n",
      "D_e : 7.991, D_real_e : 8.086, D_fake_e : 5.607, G_e : 5.672, Q_e : 0.600, E_e : 3.299, new_measure : 8.759, k_curr : 0.050340\n",
      "D_e : 7.394, D_real_e : 7.572, D_fake_e : 5.206, G_e : 5.285, Q_e : 0.571, E_e : 3.401, new_measure : 7.993, k_curr : 0.091697\n",
      "D_e : 6.875, D_real_e : 7.141, D_fake_e : 4.914, G_e : 5.020, Q_e : 0.544, E_e : 3.484, new_measure : 7.468, k_curr : 0.033199\n",
      "D_e : 6.361, D_real_e : 6.622, D_fake_e : 4.516, G_e : 4.622, Q_e : 0.520, E_e : 3.602, new_measure : 6.862, k_curr : 0.072624\n",
      "D_e : 5.916, D_real_e : 6.152, D_fake_e : 4.221, G_e : 4.320, Q_e : 0.498, E_e : 3.595, new_measure : 6.375, k_curr : 0.036268\n",
      "D_e : 5.738, D_real_e : 5.933, D_fake_e : 4.043, G_e : 4.145, Q_e : 0.478, E_e : 3.598, new_measure : 6.183, k_curr : 0.057920\n",
      "D_e : 5.599, D_real_e : 5.786, D_fake_e : 3.959, G_e : 4.055, Q_e : 0.459, E_e : 3.661, new_measure : 5.995, k_curr : 0.045357\n",
      "D_e : 5.442, D_real_e : 5.612, D_fake_e : 3.843, G_e : 3.929, Q_e : 0.442, E_e : 3.674, new_measure : 5.838, k_curr : 0.044372\n",
      "D_e : 5.381, D_real_e : 5.531, D_fake_e : 3.782, G_e : 3.876, Q_e : 0.426, E_e : 3.704, new_measure : 5.750, k_curr : 0.033081\n",
      "D_e : 5.247, D_real_e : 5.409, D_fake_e : 3.701, G_e : 3.789, Q_e : 0.411, E_e : 3.719, new_measure : 5.608, k_curr : 0.024295\n",
      "D_e : 5.203, D_real_e : 5.360, D_fake_e : 3.652, G_e : 3.749, Q_e : 0.396, E_e : 3.761, new_measure : 5.583, k_curr : 0.031105\n",
      "D_e : 5.138, D_real_e : 5.296, D_fake_e : 3.601, G_e : 3.699, Q_e : 0.383, E_e : 3.783, new_measure : 5.514, k_curr : 0.053399\n",
      "D_e : 5.019, D_real_e : 5.184, D_fake_e : 3.530, G_e : 3.630, Q_e : 0.371, E_e : 3.831, new_measure : 5.371, k_curr : 0.048598\n",
      "D_e : 4.960, D_real_e : 5.130, D_fake_e : 3.492, G_e : 3.593, Q_e : 0.359, E_e : 3.874, new_measure : 5.344, k_curr : 0.043365\n",
      "D_e : 4.891, D_real_e : 5.063, D_fake_e : 3.447, G_e : 3.545, Q_e : 0.348, E_e : 3.901, new_measure : 5.257, k_curr : 0.041520\n",
      "D_e : 4.850, D_real_e : 5.019, D_fake_e : 3.406, G_e : 3.508, Q_e : 0.338, E_e : 3.944, new_measure : 5.209, k_curr : 0.057320\n",
      "D_e : 4.814, D_real_e : 4.986, D_fake_e : 3.385, G_e : 3.492, Q_e : 0.328, E_e : 4.000, new_measure : 5.177, k_curr : 0.051483\n",
      "D_e : 4.756, D_real_e : 4.907, D_fake_e : 3.329, G_e : 3.434, Q_e : 0.319, E_e : 4.033, new_measure : 5.092, k_curr : 0.052613\n",
      "D_e : 4.716, D_real_e : 4.876, D_fake_e : 3.316, G_e : 3.413, Q_e : 0.311, E_e : 4.068, new_measure : 5.069, k_curr : 0.052901\n",
      "D_e : 4.696, D_real_e : 4.843, D_fake_e : 3.285, G_e : 3.388, Q_e : 0.302, E_e : 4.095, new_measure : 5.041, k_curr : 0.060697\n",
      "D_e : 4.645, D_real_e : 4.777, D_fake_e : 3.246, G_e : 3.349, Q_e : 0.295, E_e : 4.127, new_measure : 4.955, k_curr : 0.044287\n",
      "D_e : 4.615, D_real_e : 4.738, D_fake_e : 3.230, G_e : 3.322, Q_e : 0.287, E_e : 4.149, new_measure : 4.917, k_curr : 0.030500\n",
      "D_e : 4.570, D_real_e : 4.702, D_fake_e : 3.189, G_e : 3.292, Q_e : 0.280, E_e : 4.197, new_measure : 4.874, k_curr : 0.030519\n",
      "D_e : 4.547, D_real_e : 4.665, D_fake_e : 3.162, G_e : 3.265, Q_e : 0.274, E_e : 4.245, new_measure : 4.863, k_curr : 0.034163\n",
      "D_e : 4.512, D_real_e : 4.625, D_fake_e : 3.149, G_e : 3.239, Q_e : 0.267, E_e : 4.290, new_measure : 4.817, k_curr : 0.030950\n",
      "D_e : 4.493, D_real_e : 4.609, D_fake_e : 3.127, G_e : 3.223, Q_e : 0.261, E_e : 4.327, new_measure : 4.792, k_curr : 0.038318\n",
      "D_e : 4.461, D_real_e : 4.572, D_fake_e : 3.099, G_e : 3.196, Q_e : 0.255, E_e : 4.358, new_measure : 4.755, k_curr : 0.049696\n",
      "D_e : 4.415, D_real_e : 4.525, D_fake_e : 3.076, G_e : 3.176, Q_e : 0.250, E_e : 4.384, new_measure : 4.705, k_curr : 0.026697\n",
      "D_e : 4.409, D_real_e : 4.521, D_fake_e : 3.068, G_e : 3.165, Q_e : 0.244, E_e : 4.406, new_measure : 4.696, k_curr : 0.027670\n",
      "D_e : 4.348, D_real_e : 4.465, D_fake_e : 3.028, G_e : 3.121, Q_e : 0.239, E_e : 4.443, new_measure : 4.646, k_curr : 0.041651\n",
      "D_e : 4.341, D_real_e : 4.450, D_fake_e : 3.013, G_e : 3.117, Q_e : 0.235, E_e : 4.474, new_measure : 4.606, k_curr : 0.036371\n",
      "D_e : 4.321, D_real_e : 4.433, D_fake_e : 3.005, G_e : 3.104, Q_e : 0.230, E_e : 4.499, new_measure : 4.616, k_curr : 0.033035\n",
      "D_e : 4.277, D_real_e : 4.389, D_fake_e : 2.979, G_e : 3.071, Q_e : 0.225, E_e : 4.546, new_measure : 4.553, k_curr : 0.036657\n",
      "D_e : 4.237, D_real_e : 4.367, D_fake_e : 2.961, G_e : 3.054, Q_e : 0.221, E_e : 4.576, new_measure : 4.541, k_curr : 0.045554\n",
      "D_e : 4.239, D_real_e : 4.363, D_fake_e : 2.956, G_e : 3.054, Q_e : 0.217, E_e : 4.601, new_measure : 4.532, k_curr : 0.044753\n",
      "D_e : 4.228, D_real_e : 4.338, D_fake_e : 2.935, G_e : 3.039, Q_e : 0.213, E_e : 4.630, new_measure : 4.520, k_curr : 0.037257\n",
      "D_e : 4.217, D_real_e : 4.315, D_fake_e : 2.933, G_e : 3.023, Q_e : 0.209, E_e : 4.659, new_measure : 4.488, k_curr : 0.030800\n",
      "D_e : 4.193, D_real_e : 4.282, D_fake_e : 2.911, G_e : 3.003, Q_e : 0.205, E_e : 4.681, new_measure : 4.467, k_curr : 0.015701\n",
      "D_e : 4.159, D_real_e : 4.255, D_fake_e : 2.882, G_e : 2.967, Q_e : 0.202, E_e : 4.703, new_measure : 4.426, k_curr : 0.047540\n",
      "D_e : 4.141, D_real_e : 4.238, D_fake_e : 2.880, G_e : 2.972, Q_e : 0.198, E_e : 4.723, new_measure : 4.406, k_curr : 0.030904\n",
      "D_e : 4.124, D_real_e : 4.220, D_fake_e : 2.855, G_e : 2.955, Q_e : 0.195, E_e : 4.745, new_measure : 4.391, k_curr : 0.028346\n",
      "D_e : 4.106, D_real_e : 4.198, D_fake_e : 2.841, G_e : 2.938, Q_e : 0.191, E_e : 4.769, new_measure : 4.371, k_curr : 0.031629\n",
      "D_e : 4.077, D_real_e : 4.174, D_fake_e : 2.833, G_e : 2.925, Q_e : 0.188, E_e : 4.790, new_measure : 4.346, k_curr : 0.023900\n",
      "D_e : 4.063, D_real_e : 4.165, D_fake_e : 2.819, G_e : 2.914, Q_e : 0.185, E_e : 4.808, new_measure : 4.332, k_curr : 0.027491\n",
      "D_e : 4.042, D_real_e : 4.146, D_fake_e : 2.815, G_e : 2.901, Q_e : 0.182, E_e : 4.828, new_measure : 4.308, k_curr : 0.031874\n",
      "D_e : 4.047, D_real_e : 4.150, D_fake_e : 2.816, G_e : 2.901, Q_e : 0.179, E_e : 4.848, new_measure : 4.306, k_curr : 0.042890\n",
      "D_e : 4.005, D_real_e : 4.110, D_fake_e : 2.786, G_e : 2.874, Q_e : 0.176, E_e : 4.868, new_measure : 4.277, k_curr : 0.051154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 3.987, D_real_e : 4.093, D_fake_e : 2.778, G_e : 2.870, Q_e : 0.174, E_e : 4.886, new_measure : 4.252, k_curr : 0.039332\n",
      "D_e : 3.974, D_real_e : 4.076, D_fake_e : 2.773, G_e : 2.857, Q_e : 0.171, E_e : 4.904, new_measure : 4.226, k_curr : 0.028189\n",
      "D_e : 3.958, D_real_e : 4.065, D_fake_e : 2.759, G_e : 2.838, Q_e : 0.169, E_e : 4.923, new_measure : 4.210, k_curr : 0.048750\n",
      "D_e : 3.955, D_real_e : 4.056, D_fake_e : 2.761, G_e : 2.838, Q_e : 0.166, E_e : 4.938, new_measure : 4.225, k_curr : 0.051119\n",
      "D_e : 3.934, D_real_e : 4.034, D_fake_e : 2.747, G_e : 2.833, Q_e : 0.164, E_e : 4.953, new_measure : 4.191, k_curr : 0.025708\n",
      "D_e : 3.919, D_real_e : 4.006, D_fake_e : 2.712, G_e : 2.799, Q_e : 0.161, E_e : 4.967, new_measure : 4.175, k_curr : 0.040948\n",
      "D_e : 3.905, D_real_e : 3.998, D_fake_e : 2.705, G_e : 2.800, Q_e : 0.159, E_e : 4.984, new_measure : 4.159, k_curr : 0.038822\n",
      "D_e : 3.914, D_real_e : 3.995, D_fake_e : 2.707, G_e : 2.800, Q_e : 0.157, E_e : 4.999, new_measure : 4.165, k_curr : 0.029917\n",
      "D_e : 3.889, D_real_e : 3.979, D_fake_e : 2.704, G_e : 2.784, Q_e : 0.155, E_e : 5.014, new_measure : 4.149, k_curr : 0.033958\n",
      "D_e : 3.876, D_real_e : 3.953, D_fake_e : 2.686, G_e : 2.767, Q_e : 0.153, E_e : 5.028, new_measure : 4.109, k_curr : 0.034260\n",
      "D_e : 3.860, D_real_e : 3.953, D_fake_e : 2.685, G_e : 2.768, Q_e : 0.151, E_e : 5.051, new_measure : 4.118, k_curr : 0.031437\n",
      "D_e : 3.846, D_real_e : 3.938, D_fake_e : 2.676, G_e : 2.758, Q_e : 0.149, E_e : 5.086, new_measure : 4.111, k_curr : 0.027471\n",
      "D_e : 3.825, D_real_e : 3.922, D_fake_e : 2.666, G_e : 2.747, Q_e : 0.147, E_e : 5.101, new_measure : 4.086, k_curr : 0.021864\n",
      "D_e : 3.812, D_real_e : 3.906, D_fake_e : 2.637, G_e : 2.732, Q_e : 0.145, E_e : 5.114, new_measure : 4.074, k_curr : 0.026285\n",
      "D_e : 3.811, D_real_e : 3.899, D_fake_e : 2.642, G_e : 2.730, Q_e : 0.143, E_e : 5.134, new_measure : 4.058, k_curr : 0.024779\n",
      "D_e : 3.790, D_real_e : 3.882, D_fake_e : 2.630, G_e : 2.712, Q_e : 0.141, E_e : 5.146, new_measure : 4.034, k_curr : 0.041389\n",
      "D_e : 3.794, D_real_e : 3.886, D_fake_e : 2.636, G_e : 2.718, Q_e : 0.140, E_e : 5.158, new_measure : 4.046, k_curr : 0.046690\n",
      "D_e : 3.788, D_real_e : 3.873, D_fake_e : 2.626, G_e : 2.711, Q_e : 0.138, E_e : 5.170, new_measure : 4.033, k_curr : 0.045245\n",
      "D_e : 3.757, D_real_e : 3.836, D_fake_e : 2.613, G_e : 2.690, Q_e : 0.136, E_e : 5.181, new_measure : 3.993, k_curr : 0.031996\n",
      "D_e : 3.742, D_real_e : 3.830, D_fake_e : 2.602, G_e : 2.681, Q_e : 0.135, E_e : 5.192, new_measure : 3.970, k_curr : 0.031806\n",
      "D_e : 3.733, D_real_e : 3.820, D_fake_e : 2.596, G_e : 2.673, Q_e : 0.133, E_e : 5.203, new_measure : 3.987, k_curr : 0.032547\n",
      "D_e : 3.726, D_real_e : 3.811, D_fake_e : 2.593, G_e : 2.668, Q_e : 0.132, E_e : 5.213, new_measure : 3.972, k_curr : 0.032368\n",
      "D_e : 3.706, D_real_e : 3.795, D_fake_e : 2.580, G_e : 2.655, Q_e : 0.130, E_e : 5.224, new_measure : 3.943, k_curr : 0.036471\n",
      "D_e : 3.694, D_real_e : 3.785, D_fake_e : 2.577, G_e : 2.655, Q_e : 0.129, E_e : 5.235, new_measure : 3.933, k_curr : 0.020657\n",
      "D_e : 3.681, D_real_e : 3.767, D_fake_e : 2.559, G_e : 2.632, Q_e : 0.127, E_e : 5.246, new_measure : 3.907, k_curr : 0.036133\n",
      "D_e : 3.670, D_real_e : 3.765, D_fake_e : 2.563, G_e : 2.636, Q_e : 0.126, E_e : 5.256, new_measure : 3.905, k_curr : 0.036392\n",
      "D_e : 3.662, D_real_e : 3.749, D_fake_e : 2.540, G_e : 2.617, Q_e : 0.124, E_e : 5.265, new_measure : 3.898, k_curr : 0.056380\n",
      "D_e : 3.657, D_real_e : 3.749, D_fake_e : 2.562, G_e : 2.635, Q_e : 0.123, E_e : 5.274, new_measure : 3.901, k_curr : 0.027195\n",
      "D_e : 3.649, D_real_e : 3.730, D_fake_e : 2.535, G_e : 2.605, Q_e : 0.122, E_e : 5.283, new_measure : 3.878, k_curr : 0.043113\n",
      "D_e : 3.647, D_real_e : 3.730, D_fake_e : 2.545, G_e : 2.618, Q_e : 0.121, E_e : 5.292, new_measure : 3.887, k_curr : 0.022600\n",
      "D_e : 3.636, D_real_e : 3.710, D_fake_e : 2.524, G_e : 2.594, Q_e : 0.119, E_e : 5.304, new_measure : 3.865, k_curr : 0.030687\n",
      "D_e : 3.630, D_real_e : 3.709, D_fake_e : 2.524, G_e : 2.597, Q_e : 0.118, E_e : 5.317, new_measure : 3.862, k_curr : 0.027429\n",
      "D_e : 3.624, D_real_e : 3.694, D_fake_e : 2.517, G_e : 2.590, Q_e : 0.117, E_e : 5.325, new_measure : 3.846, k_curr : 0.015902\n",
      "D_e : 3.618, D_real_e : 3.684, D_fake_e : 2.504, G_e : 2.569, Q_e : 0.116, E_e : 5.334, new_measure : 3.837, k_curr : 0.043773\n",
      "D_e : 3.593, D_real_e : 3.666, D_fake_e : 2.502, G_e : 2.573, Q_e : 0.114, E_e : 5.341, new_measure : 3.812, k_curr : 0.024238\n",
      "D_e : 3.591, D_real_e : 3.657, D_fake_e : 2.492, G_e : 2.556, Q_e : 0.113, E_e : 5.349, new_measure : 3.802, k_curr : 0.036625\n",
      "total time :  10681.914887428284\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc4c85dd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "    \n",
    "    one_hot = np.eye(c_size)\n",
    "    temp2 = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4])\n",
    "    test_c = one_hot[temp2].reshape([-1,1,1,c_size])\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    Q_error=[]\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            temp1 = np.random.randint(0,10,(batch_size))                                                                                                                                     \n",
    "            c_ = one_hot[temp1].reshape([-1,1,1,c_size])\n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, c : c_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e,Q_e = sess.run([G_optim, G_loss,Q_loss], {u : u_, z : z_, c : c_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            Q_error.append(Q_e)\n",
    "            _ , E_e = sess.run([E_optim, E_loss], {u : u_, z : z_, c : c_,isTrain : True})\n",
    "            E_error.append(E_e)\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, Q_e : %.3f, E_e : %.3f, new_measure : %.3f, k_curr : %3f'\n",
    "              %(np.mean(D_error), np.mean(D_real_error),np.mean(D_fake_error), np.mean(G_error),\n",
    "                np.mean(Q_error),np.mean(E_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, c : test_c, isTrain : False})       \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        r = sess.run([re_image],feed_dict={u : test_normal_data[0:16],isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/origin_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        r = sess.run([re_image],feed_dict={u : test_anomalous_data[0:16],isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/anomlous_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "\n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_infoGANs_for_FD/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "c = sess.graph.get_tensor_by_name(\"c:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "k = sess.graph.get_tensor_by_name(\"k:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "G_sample = sess.graph.get_tensor_by_name(\"E_z:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "#\n",
    "\n",
    "\n",
    "G_sample = G(z,c,name='G_sample') # G(z)\n",
    "E_z, E_c = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z,E_c, isTrain, reuse=True, name ='re_image')\n",
    "re_z, re_c = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "\n",
    "re_z_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z - z)**2, axis=[1,2,3])) , name = 're_z_loss') \n",
    "re_c_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(re_c + 1e-8), axis = [1,2,3]),name = 're_c_loss')\n",
    "\n",
    "E_loss = tf.add(re_z_loss, re_c_loss, name = 'E_loss')            \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "Q_fake = Q_cat(D_enc(G_sample, isTrain,reuse=True), reuse=False, name='Q_fake')\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "Q_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(Q_fake + 1e-8), axis = (1,2,3)),name = 'Q_loss')\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "Q_vars = [var for var in T_vars if var.name.startswith('Q')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss + Q_loss, var_list=G_vars+Q_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(E_loss, var_list=E_vars, name='E_optim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 're_z:0' shape=(?, 1, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20751111984252929,\n",
       " 0.42166746902465818,\n",
       " 0.64129907226562499,\n",
       " 0.86600532531738283,\n",
       " 1.0942468872070312,\n",
       " 1.3260106887817382,\n",
       " 1.5593840789794922,\n",
       " 1.7951323013305664,\n",
       " 2.0319133949279786,\n",
       " 2.2700338363647461,\n",
       " 2.509229175567627,\n",
       " 2.748695281982422,\n",
       " 2.9890981407165529,\n",
       " 3.2280688934326172,\n",
       " 3.4668105163574219,\n",
       " 3.7045840606689451,\n",
       " 3.9402947463989255,\n",
       " 4.1753719673156739,\n",
       " 4.4078176383972165,\n",
       " 4.6403675765991208,\n",
       " 4.8700066413879393,\n",
       " 5.0971752662658689,\n",
       " 5.323414489746094,\n",
       " 5.545967250823975,\n",
       " 5.7664978027343752,\n",
       " 5.9861437110900884,\n",
       " 6.2037738914489751,\n",
       " 6.4183871040344247,\n",
       " 6.6322331695556649,\n",
       " 6.8441371345520023,\n",
       " 7.0524976539611819,\n",
       " 7.2605827102661138,\n",
       " 7.4650619316101077,\n",
       " 7.6673873825073242,\n",
       " 7.8682432212829587,\n",
       " 8.0697668304443351,\n",
       " 8.2664996833801254,\n",
       " 8.4628638801574692,\n",
       " 8.6567379570007308,\n",
       " 8.8475150413513166,\n",
       " 9.0355524749755833,\n",
       " 9.2211078529357877,\n",
       " 9.4062015228271445,\n",
       " 9.5871899948120074,\n",
       " 9.7665339698791467,\n",
       " 9.9441702041625941,\n",
       " 10.118093059539792,\n",
       " 10.29141278839111,\n",
       " 10.461536254882809,\n",
       " 10.630753742218014,\n",
       " 10.797626300811764,\n",
       " 10.962597194671627,\n",
       " 11.123696189880366,\n",
       " 11.280285617828364,\n",
       " 11.435255630493158,\n",
       " 11.586308456420893,\n",
       " 11.730347167968745,\n",
       " 11.867622814178462,\n",
       " 11.998385410308833,\n",
       " 12.120332927703853,\n",
       " 12.232768306732172,\n",
       " 12.336394264221186,\n",
       " 12.423847068786616,\n",
       " 12.50395148468017,\n",
       " 12.568475738525386,\n",
       " 12.628674186706538,\n",
       " 12.69262807846069,\n",
       " 12.748936923980709,\n",
       " 12.799190101623532,\n",
       " 12.861751861572262,\n",
       " 12.924061447143551,\n",
       " 12.981993865966793,\n",
       " 13.055782058715817,\n",
       " 13.107287162780759,\n",
       " 13.154241325378415,\n",
       " 13.198793830871578,\n",
       " 13.223537921905514,\n",
       " 13.237994888305661,\n",
       " 13.262466949462887,\n",
       " 13.284788433074947,\n",
       " 13.309185974121089,\n",
       " 13.31481567382812,\n",
       " 13.34264768981933,\n",
       " 13.35540552902221,\n",
       " 13.378868862152093,\n",
       " 13.384527767181389,\n",
       " 13.366210540771476,\n",
       " 13.330018978118888,\n",
       " 13.321718479156486,\n",
       " 13.31649583053588,\n",
       " 13.2848283920288,\n",
       " 13.25769838333129,\n",
       " 13.225356052398672,\n",
       " 13.197062370300284,\n",
       " 13.164698429107657,\n",
       " 13.111836513519279,\n",
       " 13.064842132568351,\n",
       " 13.020952903747551,\n",
       " 12.974674629211417,\n",
       " 12.929715141296379,\n",
       " 12.881146381378166,\n",
       " 12.830501716613762,\n",
       " 12.767634716033928,\n",
       " 12.70896187591552,\n",
       " 12.632918262481683,\n",
       " 12.561639301300042,\n",
       " 12.487390113830561,\n",
       " 12.405555770874017,\n",
       " 12.317770809173577,\n",
       " 12.236334373474115,\n",
       " 12.153137195587153,\n",
       " 12.072377189636224,\n",
       " 11.971126167297356,\n",
       " 11.867818317413324,\n",
       " 11.752918811798089,\n",
       " 11.654001857757562,\n",
       " 11.523508003234857,\n",
       " 11.406822109222405,\n",
       " 11.281779396057122,\n",
       " 11.176145004272454,\n",
       " 11.039164367675774,\n",
       " 10.925957427978508,\n",
       " 10.804597850799553,\n",
       " 10.668273944854729,\n",
       " 10.539642974853509,\n",
       " 10.408262794494622,\n",
       " 10.266457706451408,\n",
       " 10.133773998260491,\n",
       " 9.9836926918029718,\n",
       " 9.8478397254943779,\n",
       " 9.7002946243286061,\n",
       " 9.5592469291686939,\n",
       " 9.4106064109802166,\n",
       " 9.250578075408928,\n",
       " 9.0954220161437913,\n",
       " 8.9386287040710375,\n",
       " 8.780387214660637,\n",
       " 8.6235613021850508,\n",
       " 8.4838634910583419,\n",
       " 8.3057405357360761,\n",
       " 8.135964725494377,\n",
       " 7.9759547080993576,\n",
       " 7.8014457893371505,\n",
       " 7.6241493492126384,\n",
       " 7.4358544998168865,\n",
       " 7.2689263191223068,\n",
       " 7.1065750694274827,\n",
       " 6.915419708251946,\n",
       " 6.7466930618286058,\n",
       " 6.5580375938415454,\n",
       " 6.3779941177368089,\n",
       " 6.1978057174682544,\n",
       " 6.018003604888909,\n",
       " 5.8282642326354912,\n",
       " 5.6445270690917901,\n",
       " 5.4627189445495539,\n",
       " 5.2727936058044369,\n",
       " 5.0984008140563901,\n",
       " 4.9087817382812435,\n",
       " 4.708777381896966,\n",
       " 4.509413452148431,\n",
       " 4.3169950714111263,\n",
       " 4.1403217010497979,\n",
       " 3.9482994346618585,\n",
       " 3.7713867111205985,\n",
       " 3.5732762298583913,\n",
       " 3.3804470329284597,\n",
       " 3.2004328765869068,\n",
       " 3.0101487312316824,\n",
       " 2.8153703269958426,\n",
       " 2.632810535430901,\n",
       " 2.4373875160217215,\n",
       " 2.2294481048583914,\n",
       " 2.022679313659661,\n",
       " 1.8088063392639091,\n",
       " 1.607453567504876,\n",
       " 1.4112572898864677,\n",
       " 1.2195381851196219,\n",
       " 1.0181188774108816,\n",
       " 0.80615206909178971,\n",
       " 0.63129409408568615,\n",
       " 0.45116747665404544,\n",
       " 0.23666193008422121,\n",
       " 0.041651985168449673,\n",
       " -0.14952867507935311,\n",
       " -0.33245481109619879,\n",
       " -0.54416123962403085,\n",
       " -0.73896835708618913,\n",
       " -0.91739830398560318,\n",
       " -1.109687564849861,\n",
       " -1.3122152900695876,\n",
       " -1.5120923461914137,\n",
       " -1.7128622093200758,\n",
       " -1.9174911956787184,\n",
       " -2.1050011215210036,\n",
       " -2.3178458023071364,\n",
       " -2.5040698699951247,\n",
       " -2.6958908500671463,\n",
       " -2.8883838882446367,\n",
       " -3.0767976531982502,\n",
       " -3.2558920097351156,\n",
       " -3.4382334556579672,\n",
       " -3.6339929084777913,\n",
       " -3.8016768302917563,\n",
       " -3.9563492507934654,\n",
       " -4.1120274085998618,\n",
       " -4.2571736907959066,\n",
       " -4.3852807083129965,\n",
       " -4.5075944328308184,\n",
       " -4.6381638870239339,\n",
       " -4.7471023178100671,\n",
       " -4.859599285125741,\n",
       " -4.9422068138122643,\n",
       " -5.0003692283630459,\n",
       " -5.0478332405090418,\n",
       " -5.0919242248535239,\n",
       " -5.1335810279846275,\n",
       " -5.1397087440490807,\n",
       " -5.1318274803161703,\n",
       " -5.0981268844604575,\n",
       " -5.0509196853637777,\n",
       " -4.9766671714782795,\n",
       " -4.8948703498840409,\n",
       " -4.7961291046142653,\n",
       " -4.6763235931396556,\n",
       " -4.5510245094299391,\n",
       " -4.4155423583984446,\n",
       " -4.2962480239868235,\n",
       " -4.1435894126892157,\n",
       " -3.9643416519165107,\n",
       " -3.8037147369384834,\n",
       " -3.6375305404663156,\n",
       " -3.4328084526062081,\n",
       " -3.2441959915161203,\n",
       " -3.0525261402130197,\n",
       " -2.8770041370391914,\n",
       " -2.6711822662353586,\n",
       " -2.4677430801391673,\n",
       " -2.2664314899444653,\n",
       " -2.0695338249206614,\n",
       " -1.8642972679138254,\n",
       " -1.6635993843078685,\n",
       " -1.4577200622558666,\n",
       " -1.2669184131622386,\n",
       " -1.0572656688690258,\n",
       " -0.84184725761414303,\n",
       " -0.63927662658692141,\n",
       " -0.42000522232056403,\n",
       " -0.20605551719666268,\n",
       " 0.0085813808441087824,\n",
       " 0.21964999771117419,\n",
       " 0.42068432044982162,\n",
       " 0.63083639430999006,\n",
       " 0.83502831554412094,\n",
       " 1.0311248846054002,\n",
       " 1.2233800859451218,\n",
       " 1.3002736368179246,\n",
       " 1.2910126466751024,\n",
       " 1.2021133050918504,\n",
       " 1.0472699747085497,\n",
       " 0.86864914417266093,\n",
       " 0.68473323154448706,\n",
       " 0.47322058963774877,\n",
       " 0.28431102848052214,\n",
       " 0.077160603523246718,\n",
       " -0.077733092308052143,\n",
       " -0.19191971111298378,\n",
       " -0.25561125087738812,\n",
       " -0.25278058528900926,\n",
       " -0.20339119243622605,\n",
       " -0.15757725048065968,\n",
       " -0.08926293849945853,\n",
       " -0.017878954887397991,\n",
       " 0.050420708656303159,\n",
       " 0.13052530002593205,\n",
       " 0.21277870655059022,\n",
       " 0.2751832933425824,\n",
       " 0.35256792545317805,\n",
       " 0.44058465290068782,\n",
       " 0.51262445163726011,\n",
       " 0.58962563037871518,\n",
       " 0.68556363010405696,\n",
       " 0.70346594333647883,\n",
       " 0.73201884555815844,\n",
       " 0.74432884311675218,\n",
       " 0.73755357837676194,\n",
       " 0.69851428699492601,\n",
       " 0.67483436298369548,\n",
       " 0.60756255435942785,\n",
       " 0.55933287525176179,\n",
       " 0.49884556293486726,\n",
       " 0.40708829593657625,\n",
       " 0.34328798770903712,\n",
       " 0.27904571247100002,\n",
       " 0.16331084918974997,\n",
       " 0.10181893253325583,\n",
       " 0.0061981515884315941,\n",
       " -0.041500609397896543,\n",
       " -0.14828781032563093,\n",
       " -0.16448832225800397,\n",
       " -0.15488600063324812,\n",
       " -0.14276971721650009,\n",
       " -0.14074344921112902,\n",
       " -0.12006156444550406,\n",
       " -0.098013041496285314,\n",
       " -0.060562312126168145,\n",
       " -0.02820609188080684,\n",
       " -0.026155684471138901,\n",
       " 0.0069848375320349064,\n",
       " 0.053661423683157933,\n",
       " 0.11479248142241572,\n",
       " 0.16115131855010126,\n",
       " 0.175830098152152,\n",
       " 0.17532315540312857,\n",
       " 0.22845754718779651,\n",
       " 0.25317836093901719,\n",
       " 0.27695489597319689,\n",
       " 0.3068180742263707,\n",
       " 0.35759758090972027,\n",
       " 0.41121673679350929,\n",
       " 0.40283234882353858,\n",
       " 0.45409206104277683,\n",
       " 0.47462675189971043,\n",
       " 0.51756807804106786,\n",
       " 0.5471835145950229,\n",
       " 0.58120235157012057,\n",
       " 0.58013039493559904,\n",
       " 0.58466756534575526,\n",
       " 0.59090429019927082,\n",
       " 0.59112689304350907,\n",
       " 0.57514063739775712,\n",
       " 0.57554631900786446,\n",
       " 0.57299809741972962,\n",
       " 0.59882606983183895,\n",
       " 0.59120849323271785,\n",
       " 0.59947385883330373,\n",
       " 0.60174383258818653,\n",
       " 0.60981832599638963,\n",
       " 0.6013931932449248,\n",
       " 0.56192328739165331,\n",
       " 0.56518937778471967,\n",
       " 0.52089371967314735,\n",
       " 0.50253176212309847,\n",
       " 0.48364228153227812,\n",
       " 0.49551688098906521,\n",
       " 0.5111238679885769,\n",
       " 0.49359383106230736,\n",
       " 0.49696014499663349,\n",
       " 0.4638407335281276,\n",
       " 0.44914792728423109,\n",
       " 0.43099246692656501,\n",
       " 0.38661201190947514,\n",
       " 0.36296848773955326,\n",
       " 0.34777368640898682,\n",
       " 0.33115059375761963,\n",
       " 0.34274867725371339,\n",
       " 0.30996712207793209,\n",
       " 0.30868602466582273,\n",
       " 0.30481930255888912,\n",
       " 0.3210959291458032,\n",
       " 0.30440256595610593,\n",
       " 0.32348233890532468,\n",
       " 0.32604829502104732,\n",
       " 0.33698771762846919,\n",
       " 0.33667756366728757,\n",
       " 0.33357374286650632,\n",
       " 0.35022192096709226,\n",
       " 0.35924370670317624,\n",
       " 0.36908518695830317,\n",
       " 0.38332768726347893,\n",
       " 0.37393957996367422,\n",
       " 0.35255615329741447,\n",
       " 0.36763849353789296,\n",
       " 0.35510453128813707,\n",
       " 0.34637547016142806,\n",
       " 0.33832248401640852,\n",
       " 0.33544594669341049,\n",
       " 0.32189969348906478,\n",
       " 0.33178352832793195,\n",
       " 0.37326873493193585,\n",
       " 0.40511799716948466,\n",
       " 0.4011524248123069,\n",
       " 0.41436995029448465,\n",
       " 0.44181938266753151,\n",
       " 0.47693723773955299,\n",
       " 0.5052729959487815,\n",
       " 0.4982512331008811,\n",
       " 0.49875753116606664,\n",
       " 0.50905485630034397,\n",
       " 0.51241387653349824,\n",
       " 0.53596104526518762,\n",
       " 0.5277897958755392,\n",
       " 0.5184796304702658,\n",
       " 0.53396606922148648,\n",
       " 0.54309805011748258,\n",
       " 0.55687030887602751,\n",
       " 0.55260014820097869,\n",
       " 0.55964676380156464,\n",
       " 0.55302153491972872,\n",
       " 0.54408375263213105,\n",
       " 0.52019372653960172,\n",
       " 0.5000310411453146,\n",
       " 0.52319386386870326,\n",
       " 0.48987273883818566,\n",
       " 0.45792732334135949,\n",
       " 0.46786825275420124,\n",
       " 0.46613980197905475,\n",
       " 0.47086528873442585,\n",
       " 0.49862520313261915,\n",
       " 0.49204499912260935,\n",
       " 0.49999356174467968,\n",
       " 0.50305841541289253,\n",
       " 0.49210548877715032,\n",
       " 0.51190454196928903,\n",
       " 0.53709926319121282,\n",
       " 0.54979466724394721,\n",
       " 0.54114830875395692,\n",
       " 0.53535065174101748,\n",
       " 0.53406195163725767,\n",
       " 0.51218565082549006,\n",
       " 0.52204045963286305,\n",
       " 0.53322811603545095,\n",
       " 0.4863200845718279,\n",
       " 0.52901552677153496,\n",
       " 0.52265478229521656,\n",
       " 0.5142419290542497,\n",
       " 0.50551499652861487,\n",
       " 0.48025661182402501,\n",
       " 0.44465319156645661,\n",
       " 0.41586765003203274,\n",
       " 0.38586835193632962,\n",
       " 0.38189699268339988,\n",
       " 0.40009772968291157,\n",
       " 0.40523517704008927,\n",
       " 0.41677480411528456,\n",
       " 0.42442741870879042,\n",
       " 0.42822406101225718,\n",
       " 0.43758451938628062,\n",
       " 0.43832298755644666,\n",
       " 0.44269651126860488,\n",
       " 0.45553434848784313,\n",
       " 0.44697488880156383,\n",
       " 0.42152126789091926,\n",
       " 0.41906823825835088,\n",
       " 0.41877761554716925,\n",
       " 0.41242344760893684,\n",
       " 0.39375993824004035,\n",
       " 0.40295191287993293,\n",
       " 0.38933407688139776,\n",
       " 0.38629978656767705,\n",
       " 0.3720559053420911,\n",
       " 0.36848423480986453,\n",
       " 0.35805665302275513,\n",
       " 0.37423875141142698,\n",
       " 0.36470488452910277,\n",
       " 0.36303796482085077,\n",
       " 0.36559927082060661,\n",
       " 0.36482397556303825,\n",
       " 0.37877446842192497,\n",
       " 0.34157623386381947,\n",
       " 0.34118439960478625,\n",
       " 0.33309168338774525,\n",
       " 0.33802552890776477,\n",
       " 0.33267112827299911,\n",
       " 0.33443711948393656,\n",
       " 0.34365808582304785,\n",
       " 0.37054301548003027,\n",
       " 0.35730379199980561,\n",
       " 0.36024808979033296,\n",
       " 0.34699683475493254,\n",
       " 0.33582355594633878,\n",
       " 0.35268718433378993,\n",
       " 0.34074153232573329,\n",
       " 0.35685538196562583,\n",
       " 0.3691276407241707,\n",
       " 0.40074643039702224,\n",
       " 0.39252990818022537,\n",
       " 0.41737076282500074,\n",
       " 0.43071081256865307,\n",
       " 0.38847746372221748,\n",
       " 0.38825575733183659,\n",
       " 0.38520443820952216,\n",
       " 0.36774348163603582,\n",
       " 0.3661727571487311,\n",
       " 0.37330789661406311,\n",
       " 0.36383318042753965,\n",
       " 0.32038679981230522,\n",
       " 0.31671745395659234,\n",
       " 0.32971009159086967,\n",
       " 0.3390658311843755,\n",
       " 0.3342134904861333,\n",
       " 0.31593934726713913,\n",
       " 0.31281957721709031,\n",
       " 0.31251848316191455,\n",
       " 0.31879676532744189,\n",
       " 0.31516841983793992,\n",
       " 0.33107975864409223,\n",
       " 0.35181493854521528,\n",
       " 0.35704795551298868,\n",
       " 0.37596735477446325,\n",
       " 0.38765669345854525,\n",
       " 0.38356259250639679,\n",
       " 0.39578970813750031,\n",
       " 0.39848935794829127,\n",
       " 0.39461502170561547,\n",
       " 0.42165508365629906,\n",
       " 0.41809076023100605,\n",
       " 0.38152393054960959,\n",
       " 0.39592725086210956,\n",
       " 0.41060938358305682,\n",
       " 0.38476355838774429,\n",
       " 0.37984973621367202,\n",
       " 0.3815125322341798,\n",
       " 0.3895534906387208,\n",
       " 0.38724047946928719,\n",
       " 0.3996717424392579,\n",
       " 0.40023418903349617,\n",
       " 0.38001521778105474,\n",
       " 0.35166157245634766,\n",
       " 0.35316789722441405,\n",
       " 0.32684617328642573,\n",
       " 0.31520738697050776,\n",
       " 0.32759509754179683,\n",
       " 0.33510749149321284,\n",
       " 0.33607178211210931,\n",
       " 0.34358116626738272,\n",
       " 0.33098462009428697,\n",
       " 0.33285644626616195,\n",
       " 0.33483422183989242,\n",
       " 0.34173730564116195,\n",
       " 0.32023100757597639,\n",
       " 0.32946715641020491,\n",
       " 0.33259241580961896,\n",
       " 0.32944068241118141,\n",
       " 0.33953597354887671,\n",
       " 0.33423487949370095,\n",
       " 0.33839632892607396,\n",
       " 0.33224382114409151,\n",
       " 0.31707927799223601,\n",
       " 0.33863060855864224,\n",
       " 0.37977546787260708,\n",
       " 0.38586469745634733,\n",
       " 0.40262040805815397,\n",
       " 0.40544980716704071,\n",
       " 0.38135421848295864,\n",
       " 0.39073288440703091,\n",
       " 0.39256943225859342,\n",
       " 0.41470541858671839,\n",
       " 0.40224019336699179,\n",
       " 0.41308229923247031,\n",
       " 0.38961589527128865,\n",
       " 0.40607198047636672,\n",
       " 0.4131081056594722,\n",
       " 0.42402094745634716,\n",
       " 0.45642231082914991,\n",
       " 0.41312883472441359,\n",
       " 0.43485735797880809,\n",
       " 0.44619913578032172,\n",
       " 0.43179411411284124,\n",
       " 0.41006461811064399,\n",
       " 0.40618595981596622,\n",
       " 0.421561760902392,\n",
       " 0.41402607822416931,\n",
       " 0.43927559757231383,\n",
       " 0.43787691593168881,\n",
       " 0.45155017948149351,\n",
       " 0.44880871295927671,\n",
       " 0.4463779916763177,\n",
       " 0.44368941211699153,\n",
       " 0.42799525928496024,\n",
       " 0.40256313037871022,\n",
       " 0.38827809238432542,\n",
       " 0.39621844577788012,\n",
       " 0.4004359560012688,\n",
       " 0.37395005512236257,\n",
       " 0.40204816150663991,\n",
       " 0.41285677051542896,\n",
       " 0.41298903369902268,\n",
       " 0.39361002826689379,\n",
       " 0.3916831216812004,\n",
       " 0.4076137742996086,\n",
       " 0.42459330081938396,\n",
       " 0.42293313312529213,\n",
       " 0.43259978580473546,\n",
       " 0.43483735752104402,\n",
       " 0.44334020709990141,\n",
       " 0.43371582126616115,\n",
       " 0.43409769916533103,\n",
       " 0.41714198017119042,\n",
       " 0.43256199741362206,\n",
       " 0.43995430850981349,\n",
       " 0.42977192020414939,\n",
       " 0.42787300968168845,\n",
       " 0.42093960285185444,\n",
       " 0.40012025165556536,\n",
       " 0.39605574131010635,\n",
       " 0.40227022647856336,\n",
       " 0.3845813112258778,\n",
       " 0.39649869632719614,\n",
       " 0.39503128147123912,\n",
       " 0.37158243274687386,\n",
       " 0.3510317087173328,\n",
       " 0.3314692201614246,\n",
       " 0.30933481311796757,\n",
       " 0.30444647312162965,\n",
       " 0.29573085498808471,\n",
       " 0.30362809085844605,\n",
       " 0.32848618412016478,\n",
       " 0.30987885761259643,\n",
       " 0.28889849948881713,\n",
       " 0.29651841449736205,\n",
       " 0.29557059955595577,\n",
       " 0.29888206958769403,\n",
       " 0.29499701023100455,\n",
       " 0.29622992801664905,\n",
       " 0.30149178791044789,\n",
       " 0.31621112155912956,\n",
       " 0.25603494739531119,\n",
       " 0.25173817157744005,\n",
       " 0.24501593112944201,\n",
       " 0.236826291084276,\n",
       " 0.23675387287138536,\n",
       " 0.25064752292631698,\n",
       " 0.27654501056669784,\n",
       " 0.26570886707304547,\n",
       " 0.28931924915312357,\n",
       " 0.29737045383452004,\n",
       " 0.28377482318876807,\n",
       " 0.27955380344389502,\n",
       " 0.28170288181303565,\n",
       " 0.29470502185820169,\n",
       " 0.31333281421660009,\n",
       " 0.31287543964384618,\n",
       " 0.32494012546537937,\n",
       " 0.31924420261381686,\n",
       " 0.32801970005034026,\n",
       " 0.33398754978178558,\n",
       " 0.34374710559843596,\n",
       " 0.36411673068998868,\n",
       " 0.30463312625883632,\n",
       " 0.29646453571318199,\n",
       " 0.27149368000029134,\n",
       " 0.26763574695585773,\n",
       " 0.24468390369413895,\n",
       " 0.23461877155302566,\n",
       " 0.23996174335478346,\n",
       " 0.23646631336210766,\n",
       " 0.23742049884794747,\n",
       " 0.25482292270659002,\n",
       " 0.2626923608779767,\n",
       " 0.27347662067411926,\n",
       " 0.29613251590727352,\n",
       " 0.30206067752836724,\n",
       " 0.2993712282180645,\n",
       " 0.31460822772978325,\n",
       " 0.33521488666533011,\n",
       " 0.35146332645414846,\n",
       " 0.3637205858230449,\n",
       " 0.35606101703642379,\n",
       " 0.35701062488554486,\n",
       " 0.36672459888456832,\n",
       " 0.37206871891020304,\n",
       " 0.37942743015287878,\n",
       " 0.3730641679763651,\n",
       " 0.37138139820097443,\n",
       " 0.36456447696684358,\n",
       " 0.35772580432890411,\n",
       " 0.37270024967192167,\n",
       " 0.40136844348906031,\n",
       " 0.40937115573881616,\n",
       " 0.39914941120146263,\n",
       " 0.42472853565214619,\n",
       " 0.41780877399443134,\n",
       " 0.41165166187284929,\n",
       " 0.40431231403349382,\n",
       " 0.39783494663237073,\n",
       " 0.40228617572782971,\n",
       " 0.40295539569853278,\n",
       " 0.41231161594389409,\n",
       " 0.38635050678251714,\n",
       " 0.34744798374174563,\n",
       " 0.34474621295927488,\n",
       " 0.32525519657133539,\n",
       " 0.33729971790312246,\n",
       " 0.31725193119047596,\n",
       " 0.31838308811186267,\n",
       " 0.24410561084745835,\n",
       " 0.23380256748197981,\n",
       " 0.17841994571684308,\n",
       " 0.16581134510038797,\n",
       " 0.17060065555571022,\n",
       " 0.18095679187773167,\n",
       " 0.20352385044096408,\n",
       " 0.1983610391616672,\n",
       " 0.19176772785185273,\n",
       " 0.21385032749174529,\n",
       " 0.24179806995390349,\n",
       " 0.24840205478666716,\n",
       " 0.24224321842192106,\n",
       " 0.22160712528227261,\n",
       " 0.22092250156401089,\n",
       " 0.19056344318388393,\n",
       " 0.18179993915556361,\n",
       " 0.18991115283964563,\n",
       " 0.19128210544584678,\n",
       " 0.1842996339797823,\n",
       " 0.16610324573515339,\n",
       " 0.17109573078154011,\n",
       " 0.15194317722319051,\n",
       " 0.15522383213041704,\n",
       " 0.1710008478164522,\n",
       " 0.18626355457304397,\n",
       " 0.19003723239897169,\n",
       " 0.19756969547270214,\n",
       " 0.21121662616728221,\n",
       " 0.23210323429106147,\n",
       " 0.25858784580229194,\n",
       " 0.24560905170439151,\n",
       " 0.25283004093168643,\n",
       " 0.27674671268461609,\n",
       " 0.2558152818679657,\n",
       " 0.24939920139311217,\n",
       " 0.26806741046903987,\n",
       " 0.27320715808866874,\n",
       " 0.29417870807646168,\n",
       " 0.31471627902983079,\n",
       " 0.33095275974272143,\n",
       " 0.33931569004057294,\n",
       " 0.36914680194853189,\n",
       " 0.38064825534819008,\n",
       " 0.41888105106352208,\n",
       " 0.4292296953201139,\n",
       " 0.43555459308622718,\n",
       " 0.44224193668363926,\n",
       " 0.44168510532377597,\n",
       " 0.4395732088088834,\n",
       " 0.42785807132719389,\n",
       " 0.42232550144193998,\n",
       " 0.41810284519193996,\n",
       " 0.41307094669340483,\n",
       " 0.40292113971708643,\n",
       " 0.42659707355497706,\n",
       " 0.40667543888090479,\n",
       " 0.41306537342069971,\n",
       " 0.41137050342558251,\n",
       " 0.39519231128691062,\n",
       " 0.39008837223051412,\n",
       " 0.3826730089187465,\n",
       " 0.37694848346708637,\n",
       " 0.38222718524931293,\n",
       " 0.35323131656645157,\n",
       " 0.339690594673141,\n",
       " 0.34213873767851211,\n",
       " 0.34059238147734022,\n",
       " 0.35066145038603164,\n",
       " 0.3469282960891566,\n",
       " 0.36173776340483044,\n",
       " 0.37469723606108041,\n",
       " 0.36421182346342412,\n",
       " 0.35187499332426392,\n",
       " 0.35554998493192991,\n",
       " 0.35395084094999629,\n",
       " 0.35231364536283805,\n",
       " 0.3560036134719689,\n",
       " 0.34300861072538685,\n",
       " 0.31275203800199819,\n",
       " 0.35383859348295521,\n",
       " 0.35817513370512316,\n",
       " 0.33276499271391219,\n",
       " 0.34616160678861918,\n",
       " 0.36956580829618751,\n",
       " 0.23087259006498634,\n",
       " 0.22785965061186134,\n",
       " 0.21645457553861913,\n",
       " 0.20867296695707616,\n",
       " 0.20183061695097262,\n",
       " 0.20265750598905855,\n",
       " 0.19780892658232024,\n",
       " 0.19691962528227142,\n",
       " 0.19753978824613858,\n",
       " 0.18191018390653896,\n",
       " 0.18747943973539635,\n",
       " 0.19824143886564535,\n",
       " 0.21346505260465901,\n",
       " 0.21970535945890704,\n",
       " 0.22853031444547928,\n",
       " 0.21752566814420973,\n",
       " 0.23085105419157301,\n",
       " 0.24222360134123122,\n",
       " 0.25659540462492259,\n",
       " 0.25111617755888255,\n",
       " 0.22337480449674874,\n",
       " 0.22950503063200262,\n",
       " 0.22830789661405826,\n",
       " 0.22979631900785707,\n",
       " 0.23992208194731018,\n",
       " 0.24379277896879456,\n",
       " 0.26648531055448793,\n",
       " 0.25581800937650939,\n",
       " 0.26791541004179259,\n",
       " 0.30046103000639218,\n",
       " 0.30005070972440973,\n",
       " 0.30080844593046441,\n",
       " 0.33934877109525935,\n",
       " 0.19418101215360895,\n",
       " 0.19581735324857963,\n",
       " 0.193969113349898,\n",
       " 0.1653109807967974,\n",
       " 0.14466092967985403,\n",
       " 0.12172377681730519,\n",
       " 0.15002490139005908,\n",
       " 0.14888274669645554,\n",
       " 0.17740458202360398,\n",
       " 0.19507590389250046,\n",
       " 0.17010723781584028,\n",
       " 0.1836270895004106,\n",
       " 0.1930757513046098,\n",
       " 0.19788605785368205,\n",
       " 0.19112538814543006,\n",
       " 0.17820544338224645,\n",
       " 0.17571012020109408,\n",
       " 0.17461987590788117,\n",
       " 0.18793158435819851,\n",
       " 0.17022043895719752,\n",
       " 0.16187020397184593,\n",
       " 0.17089399814604025,\n",
       " 0.18052623653410177,\n",
       " 0.19201291179655294,\n",
       " 0.1847902479171584,\n",
       " 0.2054352979659865,\n",
       " 0.22460234355924821,\n",
       " 0.22365352725980972,\n",
       " 0.21695404720304701,\n",
       " 0.22779348659513682,\n",
       " 0.24975440502165047,\n",
       " 0.25833776760099614,\n",
       " 0.27290437793729982,\n",
       " 0.28732335758207517,\n",
       " 0.30114782428739739,\n",
       " 0.30985758876798819,\n",
       " 0.30443429470060535,\n",
       " 0.32084191799162098,\n",
       " 0.30333352947233388,\n",
       " 0.32425818157194325,\n",
       " 0.33466339206693835,\n",
       " 0.2986837244033641,\n",
       " 0.29726894474027815,\n",
       " 0.28792301464079084,\n",
       " 0.2840729513168162,\n",
       " 0.29496773433683571,\n",
       " 0.29015002727506811,\n",
       " 0.28774675464628391,\n",
       " 0.26573756122587372,\n",
       " 0.25052402973173304,\n",
       " 0.2482877645492379,\n",
       " 0.22966351604459923,\n",
       " 0.24242250537870566,\n",
       " 0.24093583393095175,\n",
       " 0.26609427547453079,\n",
       " 0.24477206516264113,\n",
       " 0.25390881443021923,\n",
       " 0.26728225803373484,\n",
       " 0.26994021892545844,\n",
       " 0.26488328266142031,\n",
       " 0.25931902408598084,\n",
       " 0.28221697330473083,\n",
       " 0.29187206745145933,\n",
       " 0.31290035343168393,\n",
       " 0.30844338703153745,\n",
       " 0.32367416858671322,\n",
       " 0.34150467777250421,\n",
       " 0.3375081243514837,\n",
       " 0.34758307552335871,\n",
       " 0.34215515804288993,\n",
       " 0.36938346004484301,\n",
       " 0.31419432735441333,\n",
       " 0.30983258533475999,\n",
       " 0.29046565532682539,\n",
       " 0.30684552669523357,\n",
       " 0.29882976818082924,\n",
       " 0.31123239421842686,\n",
       " 0.29719757556913484,\n",
       " 0.28641039752958403,\n",
       " 0.29678666400907622,\n",
       " 0.28677671337125882,\n",
       " 0.28448800182340722,\n",
       " 0.29556827068327046,\n",
       " 0.29440114879606344,\n",
       " 0.28015939426420305,\n",
       " 0.27490693378446668,\n",
       " 0.28705266284940811,\n",
       " 0.29699501323698135,\n",
       " 0.31253402996061419,\n",
       " 0.3239233541488466,\n",
       " 0.24126077938078017,\n",
       " 0.20423792934415905,\n",
       " 0.15661313915250863,\n",
       " 0.11375041484830939,\n",
       " 0.082598267555218541,\n",
       " 0.099911660194378665,\n",
       " 0.10039407634733277,\n",
       " 0.12030062007902222,\n",
       " 0.14358714962003782,\n",
       " 0.13818026256559443,\n",
       " 0.15135873699186395,\n",
       " 0.15777114009855339,\n",
       " 0.14589598941801141,\n",
       " 0.16157710933683464,\n",
       " 0.14974298954008169,\n",
       " 0.12705163860319202,\n",
       " 0.13567615222929064,\n",
       " 0.14788352489469589,\n",
       " 0.16765370464323104,\n",
       " 0.1796415205001646,\n",
       " 0.2076850538253599,\n",
       " 0.23585856533048682,\n",
       " 0.25254250431058933,\n",
       " 0.27269423389432956,\n",
       " 0.28549862766264011,\n",
       " 0.30543834018705418,\n",
       " 0.30228770160673191,\n",
       " 0.31109620952604344,\n",
       " 0.32070636081693699,\n",
       " 0.33165506839750336,\n",
       " 0.33270233821867035,\n",
       " 0.33906852054594083,\n",
       " 0.35687767696378747,\n",
       " 0.34710948657987634,\n",
       " 0.33015438175199546,\n",
       " 0.33310821437833821,\n",
       " 0.32545695781705891,\n",
       " 0.33801260852811849,\n",
       " 0.32915109729764974,\n",
       " 0.30902984714506188,\n",
       " 0.32667016887662925,\n",
       " 0.33270698070524252,\n",
       " 0.29633694744108235,\n",
       " 0.31512773990629228,\n",
       " 0.32121828746793779,\n",
       " 0.33245376110075026,\n",
       " 0.31740673923490553,\n",
       " 0.33144576358793287,\n",
       " 0.34303978824613596,\n",
       " 0.30995229625700021,\n",
       " 0.30281977176664376,\n",
       " 0.27269019031522773,\n",
       " 0.28371876621244452,\n",
       " 0.27604072856901191,\n",
       " 0.25689636325834292,\n",
       " 0.24241079998014464,\n",
       " 0.18233203601835266,\n",
       " 0.18783682346342098,\n",
       " 0.19423688983915338,\n",
       " 0.14647871875761037,\n",
       " 0.15899223041532518,\n",
       " 0.16245711231229781,\n",
       " 0.16743156147001262,\n",
       " 0.17951169109342568,\n",
       " 0.17681669902799599,\n",
       " 0.1984700841903495,\n",
       " 0.18149189853666298,\n",
       " 0.19334906101224889,\n",
       " 0.20029359722135531,\n",
       " 0.21798465061185823,\n",
       " 0.21201437664030059,\n",
       " 0.20890020275114043,\n",
       " 0.23549721431730253,\n",
       " 0.24715198421476345,\n",
       " 0.23702024173734643,\n",
       " 0.21819126605985617,\n",
       " 0.20637439441678976,\n",
       " 0.20645848560331317,\n",
       " 0.22694726276395771,\n",
       " 0.23683738994596454,\n",
       " 0.21575375080106707,\n",
       " 0.22759213161466568,\n",
       " 0.23451227855680434,\n",
       " 0.24519269466398205,\n",
       " 0.27339756679532973,\n",
       " 0.28272942066190687,\n",
       " 0.27446229267118416,\n",
       " 0.27360752391813242,\n",
       " 0.28879862117765392,\n",
       " 0.26114749431608164,\n",
       " 0.27115310192106212,\n",
       " 0.25975799274442635,\n",
       " 0.26169687747953374,\n",
       " 0.27024769496915774,\n",
       " 0.29091718959806395,\n",
       " 0.29495345973966547,\n",
       " 0.32773703289030021,\n",
       " 0.32012808704374257,\n",
       " 0.3204750204086107,\n",
       " 0.32782652950284896,\n",
       " 0.30582283687589584,\n",
       " 0.30406066036222396,\n",
       " 0.30593527507780011,\n",
       " 0.30257130146024636,\n",
       " 0.2862207612991135,\n",
       " 0.29577271938321992,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
