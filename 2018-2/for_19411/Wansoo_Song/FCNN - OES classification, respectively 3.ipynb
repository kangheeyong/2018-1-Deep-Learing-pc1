{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Power Pressure Gas1 Gas2\n",
    "# Training Data\n",
    "data = pd.read_excel('12th.xlsx') # LMML\n",
    "data = data.values\n",
    "RealOESdata = data[151:201,1:]\n",
    "data = pd.read_excel('14th.xlsx') # MLHM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('16th.xlsx') # HMML\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('17th.xlsx') # LMLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('21th.xlsx') # HMHM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('23th.xlsx') # MLML\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('24th.xlsx') # HMLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('25th.xlsx') # LMMH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "####\n",
    "data = pd.read_excel('1st.xlsx') # MMHH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('7th.xlsx') # MMLH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('15th.xlsx') # LHMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('22th.xlsx') # MHLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('26th.xlsx') # MMMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('3rd.xlsx') # LLMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('9th.xlsx') # MMLL\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('13th.xlsx') # HHMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('18th.xlsx') # MHHM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('19th.xlsx') # MHML\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('27th.xlsx') # MHMH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('4th.xlsx') # LMHM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('10th.xlsx') # MLLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('2nd.xlsx') # MLMH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('6th.xlsx') # HMMH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))\n",
    "data = pd.read_excel('8th.xlsx') # HMMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:201,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "data2 = pd.read_excel('12th.xlsx') # LLMM\n",
    "data2 = data2.values\n",
    "TestOESdata = data2[201:251,1:]\n",
    "data2 = pd.read_excel('14th.xlsx') # MMLL\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('16th.xlsx') # HHMM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('17th.xlsx') # MHHM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('21th.xlsx') # MHML\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('23th.xlsx') # MHMH\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('24th.xlsx') # LMHM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('25th.xlsx') # MLLM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('1st.xlsx') # MLMH\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('7th.xlsx') # HMMH\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('15th.xlsx') # HMMM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('22th.xlsx') # HMMH\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('26th.xlsx') # HMMM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "\n",
    "#####\n",
    "data2 = pd.read_excel('3rd.xlsx') # LLMM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('9th.xlsx') # MMLL\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('13th.xlsx') # HHMM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('18th.xlsx') # MHHM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('19th.xlsx') # MHML\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('27th.xlsx') # MHMH\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('4th.xlsx') # LMHM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('10th.xlsx') # MLLM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('2nd.xlsx') # MLMH\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('6th.xlsx') # HMMH\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))\n",
    "data2 = pd.read_excel('8th.xlsx') # HMMM\n",
    "data2 = data2.values\n",
    "TestOESdata = np.vstack((TestOESdata, data2[201:251,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RealOESdata_norm = (RealOESdata-np.min(RealOESdata))/(np.max(RealOESdata)-np.min(RealOESdata))\n",
    "\n",
    "# Low = 1 0 0\n",
    "# Medium = 0 1 0\n",
    "# High = 0 0 1\n",
    "\n",
    "####\n",
    "Y_ClassifiedResult_Power = np.array([1, 0, 0])\n",
    "for k in range(49):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 0, 1])))    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Power = np.vstack((Y_ClassifiedResult_Power, np.array([0, 0, 1])))\n",
    "\n",
    "####\n",
    "Y_ClassifiedResult_Press = np.array([0, 1, 0])\n",
    "for k in range(49):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 0, 1])))    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_Press = np.vstack((Y_ClassifiedResult_Press, np.array([0, 1, 0])))\n",
    "    \n",
    "####\n",
    "Y_ClassifiedResult_C3H6 = np.array([0, 1, 0])\n",
    "for k in range(49):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_C3H6 = np.vstack((Y_ClassifiedResult_C3H6, np.array([0, 1, 0])))\n",
    "    \n",
    "####\n",
    "Y_ClassifiedResult_N2 = np.array([1, 0, 0])\n",
    "for k in range(49):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))    \n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_ClassifiedResult_N2 = np.vstack((Y_ClassifiedResult_N2, np.array([0, 1, 0])))\n",
    "\n",
    "    \n",
    "Sequence = np.array([0])\n",
    "for k in range(23):\n",
    "    Sequence = np.hstack((Sequence, np.array([k+1])))\n",
    "    \n",
    "Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "Y_Test_Power = np.array([1, 0, 0])\n",
    "for k in range(49):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 0, 1])))    \n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_Power = np.vstack((Y_Test_Power, np.array([0, 0, 1])))\n",
    "\n",
    "####\n",
    "Y_Test_Press = np.array([0, 1, 0])\n",
    "for k in range(49):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 0, 1])))    \n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_Press = np.vstack((Y_Test_Press, np.array([0, 1, 0])))\n",
    "\n",
    "####\n",
    "Y_Test_C3H6 = np.array([0, 1, 0])\n",
    "for k in range(49):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))    \n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_C3H6 = np.vstack((Y_Test_C3H6, np.array([0, 1, 0])))\n",
    "    \n",
    "####\n",
    "Y_Test_N2 = np.array([1, 0, 0])\n",
    "for k in range(49):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([1, 0, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 0, 1])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for k in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([1, 0, 0]))) \n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))    \n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([1, 0, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 0, 1])))\n",
    "for i in range(50):\n",
    "    Y_Test_N2 = np.vstack((Y_Test_N2, np.array([0, 1, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input 죽이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimplifiedInput = np.identity(3648)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좀 전에 돌린거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000001\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "n_InputUnit = 3648\n",
    "n_1stHiddenUnit = 500\n",
    "n_2ndHiddenUnit = 200\n",
    "n_3rdHiddenUnit = 60\n",
    "n_OutputUnit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-899e1f9b1066>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Algorithm which decides standard for classifying data\n",
    "# Algorithm which can detect some kind of different phenomena\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_InputUnit, n_1stHiddenUnit],stddev=0.01))\n",
    "W2 = tf.Variable(tf.random_normal([n_1stHiddenUnit, n_2ndHiddenUnit],stddev=0.01))\n",
    "W3 = tf.Variable(tf.random_normal([n_2ndHiddenUnit, n_3rdHiddenUnit],stddev=0.01))\n",
    "W4 = tf.Variable(tf.random_normal([n_3rdHiddenUnit, n_OutputUnit],stddev=0.01))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([n_1stHiddenUnit], stddev=0.01))\n",
    "b2 = tf.Variable(tf.random_normal([n_2ndHiddenUnit], stddev=0.01))\n",
    "b3 = tf.Variable(tf.random_normal([n_3rdHiddenUnit], stddev=0.01))\n",
    "b4 = tf.Variable(tf.random_normal([n_OutputUnit], stddev=0.01))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_InputUnit]) # tensorflow의 placeholder라는 변수를 선언\n",
    "Y = tf.placeholder(tf.float32, [None, n_OutputUnit])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "L1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "L2 = tf.nn.elu(tf.matmul(L1_drop, W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "L3 = tf.nn.elu(tf.matmul(L2_drop, W3) + b3)\n",
    "logits = tf.matmul(L3, W4) + b4\n",
    "Hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "WL1 = tf.matmul(X, W1) + b1\n",
    "WL1_drop = tf.nn.dropout(WL1, keep_prob)\n",
    "\n",
    "WL2 = tf.matmul(WL1_drop, W2) + b2\n",
    "WL2_drop = tf.nn.dropout(WL2, keep_prob)\n",
    "\n",
    "WL3 = tf.matmul(WL2_drop, W3) + b3\n",
    "\n",
    "WL4 = tf.matmul(WL3, W4) + b4\n",
    "\n",
    "#tf.stop_gradient(Y)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "#cost = cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(, Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.5).minimize(cost)\n",
    "prediction = tf.argmax(Hypothesis, 1)\n",
    "correct_prediction = tf.equal(tf.argmax(Hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Minibatch Loss: 0.044304, Minibatch Accuracy: 1.000000\n",
      "Step 1000: Minibatch Loss: 1.185526, Minibatch Accuracy: 0.000000\n",
      "Step 2000: Minibatch Loss: 1.288128, Minibatch Accuracy: 0.000000\n",
      "Step 3000: Minibatch Loss: 0.923257, Minibatch Accuracy: 1.000000\n",
      "Step 4000: Minibatch Loss: 0.962931, Minibatch Accuracy: 1.000000\n",
      "Step 5000: Minibatch Loss: 0.974320, Minibatch Accuracy: 0.660000\n",
      "Step 6000: Minibatch Loss: 0.940058, Minibatch Accuracy: 0.980000\n",
      "Step 7000: Minibatch Loss: 0.873932, Minibatch Accuracy: 1.000000\n",
      "Step 8000: Minibatch Loss: 0.565712, Minibatch Accuracy: 1.000000\n",
      "Step 9000: Minibatch Loss: 0.458394, Minibatch Accuracy: 1.000000\n",
      "Step 10000: Minibatch Loss: 0.573646, Minibatch Accuracy: 1.000000\n",
      "Step 11000: Minibatch Loss: 1.507476, Minibatch Accuracy: 0.000000\n",
      "Step 12000: Minibatch Loss: 0.610551, Minibatch Accuracy: 1.000000\n",
      "Step 13000: Minibatch Loss: 0.468370, Minibatch Accuracy: 1.000000\n",
      "Step 14000: Minibatch Loss: 0.759421, Minibatch Accuracy: 1.000000\n",
      "Step 15000: Minibatch Loss: 0.092899, Minibatch Accuracy: 1.000000\n",
      "Step 16000: Minibatch Loss: 0.704926, Minibatch Accuracy: 1.000000\n",
      "Step 17000: Minibatch Loss: 0.605808, Minibatch Accuracy: 1.000000\n",
      "Step 18000: Minibatch Loss: 1.089215, Minibatch Accuracy: 0.000000\n",
      "Step 19000: Minibatch Loss: 0.671963, Minibatch Accuracy: 1.000000\n",
      "Step 20000: Minibatch Loss: 0.751154, Minibatch Accuracy: 1.000000\n",
      "Step 21000: Minibatch Loss: 1.053273, Minibatch Accuracy: 0.000000\n",
      "Step 22000: Minibatch Loss: 0.705168, Minibatch Accuracy: 1.000000\n",
      "Step 23000: Minibatch Loss: 0.347615, Minibatch Accuracy: 1.000000\n",
      "Step 24000: Minibatch Loss: 0.854332, Minibatch Accuracy: 0.000000\n",
      "Step 25000: Minibatch Loss: 0.636925, Minibatch Accuracy: 1.000000\n",
      "Step 26000: Minibatch Loss: 1.433762, Minibatch Accuracy: 0.000000\n",
      "Step 27000: Minibatch Loss: 0.731656, Minibatch Accuracy: 0.900000\n",
      "Step 28000: Minibatch Loss: 0.673273, Minibatch Accuracy: 1.000000\n",
      "Step 29000: Minibatch Loss: 0.698851, Minibatch Accuracy: 1.000000\n",
      "Step 30000: Minibatch Loss: 0.561608, Minibatch Accuracy: 1.000000\n",
      "Step 31000: Minibatch Loss: 0.461751, Minibatch Accuracy: 1.000000\n",
      "Step 32000: Minibatch Loss: 0.327631, Minibatch Accuracy: 1.000000\n",
      "Step 33000: Minibatch Loss: 0.077228, Minibatch Accuracy: 1.000000\n",
      "Step 34000: Minibatch Loss: 0.084006, Minibatch Accuracy: 1.000000\n",
      "Step 35000: Minibatch Loss: 0.055925, Minibatch Accuracy: 1.000000\n",
      "Step 36000: Minibatch Loss: 0.828785, Minibatch Accuracy: 0.000000\n",
      "Step 37000: Minibatch Loss: 0.084124, Minibatch Accuracy: 1.000000\n",
      "Step 38000: Minibatch Loss: 0.489958, Minibatch Accuracy: 1.000000\n",
      "Step 39000: Minibatch Loss: 0.523804, Minibatch Accuracy: 1.000000\n",
      "Step 40000: Minibatch Loss: 0.032625, Minibatch Accuracy: 1.000000\n",
      "Step 41000: Minibatch Loss: 0.000001, Minibatch Accuracy: 1.000000\n",
      "Step 42000: Minibatch Loss: 0.018334, Minibatch Accuracy: 1.000000\n",
      "Step 43000: Minibatch Loss: 0.762000, Minibatch Accuracy: 0.000000\n",
      "Step 44000: Minibatch Loss: 0.204535, Minibatch Accuracy: 1.000000\n",
      "Step 45000: Minibatch Loss: 0.061475, Minibatch Accuracy: 1.000000\n",
      "Step 46000: Minibatch Loss: 0.523004, Minibatch Accuracy: 1.000000\n",
      "Step 47000: Minibatch Loss: 0.446641, Minibatch Accuracy: 1.000000\n",
      "Step 48000: Minibatch Loss: 0.282044, Minibatch Accuracy: 1.000000\n",
      "Step 49000: Minibatch Loss: 0.623247, Minibatch Accuracy: 0.980000\n",
      "Step 50000: Minibatch Loss: 0.106368, Minibatch Accuracy: 1.000000\n",
      "Step 51000: Minibatch Loss: 0.036366, Minibatch Accuracy: 1.000000\n",
      "Step 52000: Minibatch Loss: 0.055543, Minibatch Accuracy: 1.000000\n",
      "Step 53000: Minibatch Loss: 1.229278, Minibatch Accuracy: 0.000000\n",
      "Step 54000: Minibatch Loss: 0.319314, Minibatch Accuracy: 1.000000\n",
      "Step 55000: Minibatch Loss: 0.002376, Minibatch Accuracy: 1.000000\n",
      "Step 56000: Minibatch Loss: 0.096645, Minibatch Accuracy: 1.000000\n",
      "Step 57000: Minibatch Loss: 0.010705, Minibatch Accuracy: 1.000000\n",
      "Step 58000: Minibatch Loss: 0.091120, Minibatch Accuracy: 1.000000\n",
      "Step 59000: Minibatch Loss: 0.423373, Minibatch Accuracy: 1.000000\n",
      "Step 60000: Minibatch Loss: 0.244266, Minibatch Accuracy: 1.000000\n",
      "Step 61000: Minibatch Loss: 0.031117, Minibatch Accuracy: 1.000000\n",
      "Step 62000: Minibatch Loss: 0.001816, Minibatch Accuracy: 1.000000\n",
      "Step 63000: Minibatch Loss: 0.009425, Minibatch Accuracy: 1.000000\n",
      "Step 64000: Minibatch Loss: 0.003510, Minibatch Accuracy: 1.000000\n",
      "Step 65000: Minibatch Loss: 0.000005, Minibatch Accuracy: 1.000000\n",
      "Step 66000: Minibatch Loss: 0.000894, Minibatch Accuracy: 1.000000\n",
      "Step 67000: Minibatch Loss: 0.073273, Minibatch Accuracy: 1.000000\n",
      "Step 68000: Minibatch Loss: 0.039551, Minibatch Accuracy: 1.000000\n",
      "Step 69000: Minibatch Loss: 0.003708, Minibatch Accuracy: 1.000000\n",
      "Step 70000: Minibatch Loss: 0.008289, Minibatch Accuracy: 1.000000\n",
      "Step 71000: Minibatch Loss: 0.029900, Minibatch Accuracy: 1.000000\n",
      "Step 72000: Minibatch Loss: 0.000732, Minibatch Accuracy: 1.000000\n",
      "Step 73000: Minibatch Loss: 0.053371, Minibatch Accuracy: 1.000000\n",
      "Step 74000: Minibatch Loss: 0.307537, Minibatch Accuracy: 1.000000\n",
      "Step 75000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 76000: Minibatch Loss: 0.002943, Minibatch Accuracy: 1.000000\n",
      "Step 77000: Minibatch Loss: 0.000362, Minibatch Accuracy: 1.000000\n",
      "Step 78000: Minibatch Loss: 0.099047, Minibatch Accuracy: 1.000000\n",
      "Step 79000: Minibatch Loss: 0.000009, Minibatch Accuracy: 1.000000\n",
      "Step 80000: Minibatch Loss: 0.032915, Minibatch Accuracy: 1.000000\n",
      "Step 81000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 82000: Minibatch Loss: 0.000362, Minibatch Accuracy: 1.000000\n",
      "Step 83000: Minibatch Loss: 0.000471, Minibatch Accuracy: 1.000000\n",
      "Step 84000: Minibatch Loss: 0.000453, Minibatch Accuracy: 1.000000\n",
      "Step 85000: Minibatch Loss: 0.000190, Minibatch Accuracy: 1.000000\n",
      "Step 86000: Minibatch Loss: 0.041118, Minibatch Accuracy: 1.000000\n",
      "Step 87000: Minibatch Loss: 0.000082, Minibatch Accuracy: 1.000000\n",
      "Step 88000: Minibatch Loss: 0.073670, Minibatch Accuracy: 1.000000\n",
      "Step 89000: Minibatch Loss: 0.027599, Minibatch Accuracy: 1.000000\n",
      "Step 90000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 91000: Minibatch Loss: 0.005207, Minibatch Accuracy: 1.000000\n",
      "Step 92000: Minibatch Loss: 0.025979, Minibatch Accuracy: 1.000000\n",
      "Step 93000: Minibatch Loss: 0.004515, Minibatch Accuracy: 1.000000\n",
      "Step 94000: Minibatch Loss: 0.020507, Minibatch Accuracy: 1.000000\n",
      "Step 95000: Minibatch Loss: 0.011420, Minibatch Accuracy: 1.000000\n",
      "Step 96000: Minibatch Loss: 0.035671, Minibatch Accuracy: 1.000000\n",
      "Step 97000: Minibatch Loss: 0.097022, Minibatch Accuracy: 1.000000\n",
      "Step 98000: Minibatch Loss: 0.000514, Minibatch Accuracy: 1.000000\n",
      "Step 99000: Minibatch Loss: 0.052244, Minibatch Accuracy: 1.000000\n",
      "Step 100000: Minibatch Loss: 0.047643, Minibatch Accuracy: 1.000000\n",
      "Step 101000: Minibatch Loss: 0.046841, Minibatch Accuracy: 1.000000\n",
      "Step 102000: Minibatch Loss: 0.161262, Minibatch Accuracy: 1.000000\n",
      "Step 103000: Minibatch Loss: 0.020326, Minibatch Accuracy: 1.000000\n",
      "Step 104000: Minibatch Loss: 0.107213, Minibatch Accuracy: 1.000000\n",
      "Step 105000: Minibatch Loss: 0.001563, Minibatch Accuracy: 1.000000\n",
      "Step 106000: Minibatch Loss: 0.047491, Minibatch Accuracy: 1.000000\n",
      "Step 107000: Minibatch Loss: 0.025694, Minibatch Accuracy: 1.000000\n",
      "Step 108000: Minibatch Loss: 0.006280, Minibatch Accuracy: 1.000000\n",
      "Step 109000: Minibatch Loss: 0.000751, Minibatch Accuracy: 1.000000\n",
      "Step 110000: Minibatch Loss: 0.000674, Minibatch Accuracy: 1.000000\n",
      "Step 111000: Minibatch Loss: 0.000364, Minibatch Accuracy: 1.000000\n",
      "Step 112000: Minibatch Loss: 0.023828, Minibatch Accuracy: 1.000000\n",
      "Step 113000: Minibatch Loss: 0.000098, Minibatch Accuracy: 1.000000\n",
      "Step 114000: Minibatch Loss: 0.010501, Minibatch Accuracy: 1.000000\n",
      "Step 115000: Minibatch Loss: 0.000301, Minibatch Accuracy: 1.000000\n",
      "Step 116000: Minibatch Loss: 0.001234, Minibatch Accuracy: 1.000000\n",
      "Step 117000: Minibatch Loss: 0.026058, Minibatch Accuracy: 1.000000\n",
      "Step 118000: Minibatch Loss: 0.025267, Minibatch Accuracy: 1.000000\n",
      "Step 119000: Minibatch Loss: 0.026173, Minibatch Accuracy: 1.000000\n",
      "Step 120000: Minibatch Loss: 0.009771, Minibatch Accuracy: 1.000000\n",
      "Step 121000: Minibatch Loss: 0.000024, Minibatch Accuracy: 1.000000\n",
      "Step 122000: Minibatch Loss: 0.087000, Minibatch Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 123000: Minibatch Loss: 0.001523, Minibatch Accuracy: 1.000000\n",
      "Step 124000: Minibatch Loss: 0.001814, Minibatch Accuracy: 1.000000\n",
      "Step 125000: Minibatch Loss: 0.000059, Minibatch Accuracy: 1.000000\n",
      "Step 126000: Minibatch Loss: 0.000017, Minibatch Accuracy: 1.000000\n",
      "Step 127000: Minibatch Loss: 0.024007, Minibatch Accuracy: 1.000000\n",
      "Step 128000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 129000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 130000: Minibatch Loss: 0.016407, Minibatch Accuracy: 1.000000\n",
      "Step 131000: Minibatch Loss: 0.000096, Minibatch Accuracy: 1.000000\n",
      "Step 132000: Minibatch Loss: 0.000782, Minibatch Accuracy: 1.000000\n",
      "Step 133000: Minibatch Loss: 0.000185, Minibatch Accuracy: 1.000000\n",
      "Step 134000: Minibatch Loss: 0.017275, Minibatch Accuracy: 1.000000\n",
      "Step 135000: Minibatch Loss: 0.016729, Minibatch Accuracy: 1.000000\n",
      "Step 136000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 137000: Minibatch Loss: 0.001666, Minibatch Accuracy: 1.000000\n",
      "Step 138000: Minibatch Loss: 0.004251, Minibatch Accuracy: 1.000000\n",
      "Step 139000: Minibatch Loss: 0.003211, Minibatch Accuracy: 1.000000\n",
      "Step 140000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 141000: Minibatch Loss: 0.041792, Minibatch Accuracy: 1.000000\n",
      "Step 142000: Minibatch Loss: 0.035419, Minibatch Accuracy: 1.000000\n",
      "Step 143000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 144000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 145000: Minibatch Loss: 0.000112, Minibatch Accuracy: 1.000000\n",
      "Step 146000: Minibatch Loss: 0.003934, Minibatch Accuracy: 1.000000\n",
      "Step 147000: Minibatch Loss: 0.016560, Minibatch Accuracy: 1.000000\n",
      "Step 148000: Minibatch Loss: 0.027209, Minibatch Accuracy: 1.000000\n",
      "Step 149000: Minibatch Loss: 0.000010, Minibatch Accuracy: 1.000000\n",
      "Step 150000: Minibatch Loss: 0.000078, Minibatch Accuracy: 1.000000\n",
      "Full batch Accuracy: 1.000000\n",
      "[[ 0.00475988 -0.0059677   0.01417542]\n",
      " [ 0.00417753 -0.0055723   0.01400844]\n",
      " [ 0.00352145 -0.00516831  0.01393493]\n",
      " ..., \n",
      " [ 0.00399347 -0.00574814  0.01441732]\n",
      " [ 0.00340768 -0.00522838  0.0144105 ]\n",
      " [ 0.00394374 -0.00515291  0.01379643]]\n",
      "[[ 0.00475356 -0.0059954   0.01421569]\n",
      " [ 0.00416833 -0.00559567  0.01404338]\n",
      " [ 0.00349516 -0.00518441  0.01397308]\n",
      " ..., \n",
      " [ 0.00398065 -0.00577325  0.01445824]\n",
      " [ 0.00338508 -0.0052501   0.0144539 ]\n",
      " [ 0.00392265 -0.00516985  0.01383045]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range (150001):\n",
    "        Order = Sequence[step % 24]\n",
    "        batch_X = np.reshape(RealOESdata[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_InputUnit))\n",
    "        \n",
    "        ### Y_ClassifiedResult_Power -> Power에 대한 one hot vector\n",
    "        batch_Y = np.reshape(Y_ClassifiedResult_Power[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_OutputUnit))\n",
    "        session.run(train, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 0.5})\n",
    "        MinibatchCost = session.run(cost, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "            print ('Step %i: Minibatch Loss: %f, Minibatch Accuracy: %f' % (step, MinibatchCost,\n",
    "                                                                            train_accuracy))\n",
    "            \n",
    "        if step % 24 == 0:\n",
    "            random.shuffle(Sequence)\n",
    "    \n",
    "    RealOESdata_Reshaped = np.reshape(RealOESdata, (1200, 3648))\n",
    "    \n",
    "    ###Y_ClassifiedResult_Power\n",
    "    train_accuracy = accuracy.eval(feed_dict = {X: RealOESdata_Reshaped, Y: Y_ClassifiedResult_Power, keep_prob: 1.0})        \n",
    "    print('Full batch Accuracy: %f' % (train_accuracy))\n",
    "    \n",
    "    PredictedSimplifiedInputData = session.run(logits, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputData)\n",
    "    np.savetxt('Power_Weight.txt', PredictedSimplifiedInputData, delimiter='\\t')\n",
    "    \n",
    "    PredictedSimplifiedInputDataLinear = session.run(WL4, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputDataLinear)\n",
    "    np.savetxt('Power_Weight2.txt', PredictedSimplifiedInputDataLinear, delimiter='\\t')\n",
    "    \n",
    "    \n",
    "    #for i in range(24):\n",
    "    #    Result_batch_X = np.reshape(RealOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                (batchsize, n_InputUnit))\n",
    "    #    PredictedBatchData = session.run(prediction, feed_dict = {X : Result_batch_X, keep_prob: 1.0})\n",
    "    #    \n",
    "    #    ### Y_ClassifiedResult_Power\n",
    "    #    Y_ClassifiedResult_Reshaped = np.reshape(Y_ClassifiedResult_Power[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                             (batchsize, n_OutputUnit))\n",
    "    #    \n",
    "    #    if i % 1 == 0:\n",
    "    #        print(\"Original Data\")\n",
    "    #        print(Y_ClassifiedResult_Reshaped[0,:])\n",
    "    #    \n",
    "    #    print(i)\n",
    "    #    print(\"Result of predicting EntireData\")\n",
    "    #    print(PredictedBatchData)\n",
    "    #        \n",
    "    #print(\"####################################################\")  \n",
    "    #\n",
    "    #for i in range(24):\n",
    "    #    #\n",
    "    #    Test_batch_X = np.reshape(TestOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                (batchsize, n_InputUnit))\n",
    "    #    \n",
    "    #    \n",
    "    #    PredictedTestBatchData = session.run(prediction, feed_dict = {X : Test_batch_X, keep_prob: 1.0})\n",
    "    #    \n",
    "    #    ###Y_Test_Power -> Test data의 power에 대한 one hot vector\n",
    "    #    #\n",
    "    #    Y_Test_Reshaped = np.reshape(Y_Test_Power[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                             (batchsize, n_OutputUnit))\n",
    "    #    \n",
    "    #    if i % 1 == 0:\n",
    "    #        print(\"Original Data\")\n",
    "    #        print(Y_Test_Reshaped[0,:])\n",
    "    #    \n",
    "    #    print(i)\n",
    "    #    print(\"Result of Predicted Test Data\")\n",
    "    #    print(PredictedTestBatchData)\n",
    "    ##\n",
    "    #TestOESdata_Reshaped = np.reshape(TestOESdata, (1200,3648))\n",
    "    #\n",
    "    ####Y_Test_Power\n",
    "    ##\n",
    "    #train_accuracy = accuracy.eval(feed_dict = {X: TestOESdata_Reshaped, Y: Y_Test_Power, keep_prob: 1.0})        \n",
    "    #print('Test Full batch Accuracy: %f' % (train_accuracy))          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3H6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000001\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "n_InputUnit = 3648\n",
    "n_1stHiddenUnit = 500\n",
    "n_2ndHiddenUnit = 200\n",
    "n_3rdHiddenUnit = 60\n",
    "n_OutputUnit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm which decides standard for classifying data\n",
    "# Algorithm which can detect some kind of different phenomena\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_InputUnit, n_1stHiddenUnit],stddev=0.01))\n",
    "W2 = tf.Variable(tf.random_normal([n_1stHiddenUnit, n_2ndHiddenUnit],stddev=0.01))\n",
    "W3 = tf.Variable(tf.random_normal([n_2ndHiddenUnit, n_3rdHiddenUnit],stddev=0.01))\n",
    "W4 = tf.Variable(tf.random_normal([n_3rdHiddenUnit, n_OutputUnit],stddev=0.01))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([n_1stHiddenUnit], stddev=0.01))\n",
    "b2 = tf.Variable(tf.random_normal([n_2ndHiddenUnit], stddev=0.01))\n",
    "b3 = tf.Variable(tf.random_normal([n_3rdHiddenUnit], stddev=0.01))\n",
    "b4 = tf.Variable(tf.random_normal([n_OutputUnit], stddev=0.01))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_InputUnit]) # tensorflow의 placeholder라는 변수를 선언\n",
    "Y = tf.placeholder(tf.float32, [None, n_OutputUnit])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "L1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "L2 = tf.nn.elu(tf.matmul(L1_drop, W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "L3 = tf.nn.elu(tf.matmul(L2_drop, W3) + b3)\n",
    "logits = tf.matmul(L3, W4) + b4\n",
    "Hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "WL1 = tf.matmul(X, W1) + b1\n",
    "WL1_drop = tf.nn.dropout(WL1, keep_prob)\n",
    "\n",
    "WL2 = tf.matmul(WL1_drop, W2) + b2\n",
    "WL2_drop = tf.nn.dropout(WL2, keep_prob)\n",
    "\n",
    "WL3 = tf.matmul(WL2_drop, W3) + b3\n",
    "\n",
    "WL4 = tf.matmul(WL3, W4) + b4\n",
    "\n",
    "#tf.stop_gradient(Y)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "#cost = cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(, Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.5).minimize(cost)\n",
    "prediction = tf.argmax(Hypothesis, 1)\n",
    "correct_prediction = tf.equal(tf.argmax(Hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Minibatch Loss: 0.335568, Minibatch Accuracy: 1.000000\n",
      "Step 1000: Minibatch Loss: 0.715777, Minibatch Accuracy: 1.000000\n",
      "Step 2000: Minibatch Loss: 0.982937, Minibatch Accuracy: 1.000000\n",
      "Step 3000: Minibatch Loss: 0.803550, Minibatch Accuracy: 1.000000\n",
      "Step 4000: Minibatch Loss: 0.885419, Minibatch Accuracy: 0.200000\n",
      "Step 5000: Minibatch Loss: 0.620227, Minibatch Accuracy: 1.000000\n",
      "Step 6000: Minibatch Loss: 0.604035, Minibatch Accuracy: 1.000000\n",
      "Step 7000: Minibatch Loss: 0.677219, Minibatch Accuracy: 1.000000\n",
      "Step 8000: Minibatch Loss: 0.719064, Minibatch Accuracy: 1.000000\n",
      "Step 9000: Minibatch Loss: 0.922783, Minibatch Accuracy: 0.400000\n",
      "Step 10000: Minibatch Loss: 0.802166, Minibatch Accuracy: 0.620000\n",
      "Step 11000: Minibatch Loss: 0.653310, Minibatch Accuracy: 1.000000\n",
      "Step 12000: Minibatch Loss: 0.761658, Minibatch Accuracy: 0.420000\n",
      "Step 13000: Minibatch Loss: 0.581066, Minibatch Accuracy: 1.000000\n",
      "Step 14000: Minibatch Loss: 0.557828, Minibatch Accuracy: 1.000000\n",
      "Step 15000: Minibatch Loss: 0.539886, Minibatch Accuracy: 1.000000\n",
      "Step 16000: Minibatch Loss: 0.552976, Minibatch Accuracy: 1.000000\n",
      "Step 17000: Minibatch Loss: 0.869706, Minibatch Accuracy: 0.000000\n",
      "Step 18000: Minibatch Loss: 0.453979, Minibatch Accuracy: 1.000000\n",
      "Step 19000: Minibatch Loss: 0.604648, Minibatch Accuracy: 1.000000\n",
      "Step 20000: Minibatch Loss: 0.647434, Minibatch Accuracy: 1.000000\n",
      "Step 21000: Minibatch Loss: 0.788516, Minibatch Accuracy: 1.000000\n",
      "Step 22000: Minibatch Loss: 0.855891, Minibatch Accuracy: 0.000000\n",
      "Step 23000: Minibatch Loss: 0.394844, Minibatch Accuracy: 1.000000\n",
      "Step 24000: Minibatch Loss: 0.319589, Minibatch Accuracy: 1.000000\n",
      "Step 25000: Minibatch Loss: 0.546019, Minibatch Accuracy: 1.000000\n",
      "Step 26000: Minibatch Loss: 0.644100, Minibatch Accuracy: 1.000000\n",
      "Step 27000: Minibatch Loss: 0.864516, Minibatch Accuracy: 0.000000\n",
      "Step 28000: Minibatch Loss: 0.041548, Minibatch Accuracy: 1.000000\n",
      "Step 29000: Minibatch Loss: 0.587999, Minibatch Accuracy: 1.000000\n",
      "Step 30000: Minibatch Loss: 0.188877, Minibatch Accuracy: 1.000000\n",
      "Step 31000: Minibatch Loss: 0.083588, Minibatch Accuracy: 1.000000\n",
      "Step 32000: Minibatch Loss: 0.588732, Minibatch Accuracy: 1.000000\n",
      "Step 33000: Minibatch Loss: 0.469541, Minibatch Accuracy: 1.000000\n",
      "Step 34000: Minibatch Loss: 0.893977, Minibatch Accuracy: 0.000000\n",
      "Step 35000: Minibatch Loss: 0.456656, Minibatch Accuracy: 1.000000\n",
      "Step 36000: Minibatch Loss: 0.401061, Minibatch Accuracy: 1.000000\n",
      "Step 37000: Minibatch Loss: 0.532793, Minibatch Accuracy: 1.000000\n",
      "Step 38000: Minibatch Loss: 0.847001, Minibatch Accuracy: 0.080000\n",
      "Step 39000: Minibatch Loss: 0.684518, Minibatch Accuracy: 0.700000\n",
      "Step 40000: Minibatch Loss: 0.113400, Minibatch Accuracy: 1.000000\n",
      "Step 41000: Minibatch Loss: 0.757634, Minibatch Accuracy: 0.000000\n",
      "Step 42000: Minibatch Loss: 0.033288, Minibatch Accuracy: 1.000000\n",
      "Step 43000: Minibatch Loss: 0.000019, Minibatch Accuracy: 1.000000\n",
      "Step 44000: Minibatch Loss: 0.000011, Minibatch Accuracy: 1.000000\n",
      "Step 45000: Minibatch Loss: 0.013522, Minibatch Accuracy: 1.000000\n",
      "Step 46000: Minibatch Loss: 0.461367, Minibatch Accuracy: 1.000000\n",
      "Step 47000: Minibatch Loss: 0.071688, Minibatch Accuracy: 1.000000\n",
      "Step 48000: Minibatch Loss: 0.755348, Minibatch Accuracy: 0.000000\n",
      "Step 49000: Minibatch Loss: 0.188939, Minibatch Accuracy: 1.000000\n",
      "Step 50000: Minibatch Loss: 0.088003, Minibatch Accuracy: 1.000000\n",
      "Step 51000: Minibatch Loss: 0.763254, Minibatch Accuracy: 0.300000\n",
      "Step 52000: Minibatch Loss: 0.024426, Minibatch Accuracy: 1.000000\n",
      "Step 53000: Minibatch Loss: 0.024722, Minibatch Accuracy: 1.000000\n",
      "Step 54000: Minibatch Loss: 0.502506, Minibatch Accuracy: 1.000000\n",
      "Step 55000: Minibatch Loss: 0.664759, Minibatch Accuracy: 1.000000\n",
      "Step 56000: Minibatch Loss: 0.765626, Minibatch Accuracy: 0.000000\n",
      "Step 57000: Minibatch Loss: 0.728868, Minibatch Accuracy: 0.180000\n",
      "Step 58000: Minibatch Loss: 0.228588, Minibatch Accuracy: 1.000000\n",
      "Step 59000: Minibatch Loss: 0.002706, Minibatch Accuracy: 1.000000\n",
      "Step 60000: Minibatch Loss: 0.363827, Minibatch Accuracy: 1.000000\n",
      "Step 61000: Minibatch Loss: 0.310722, Minibatch Accuracy: 1.000000\n",
      "Step 62000: Minibatch Loss: 0.000089, Minibatch Accuracy: 1.000000\n",
      "Step 63000: Minibatch Loss: 0.508431, Minibatch Accuracy: 1.000000\n",
      "Step 64000: Minibatch Loss: 0.577158, Minibatch Accuracy: 1.000000\n",
      "Step 65000: Minibatch Loss: 0.043090, Minibatch Accuracy: 1.000000\n",
      "Step 66000: Minibatch Loss: 0.350171, Minibatch Accuracy: 1.000000\n",
      "Step 67000: Minibatch Loss: 0.031218, Minibatch Accuracy: 1.000000\n",
      "Step 68000: Minibatch Loss: 0.015128, Minibatch Accuracy: 1.000000\n",
      "Step 69000: Minibatch Loss: 0.001746, Minibatch Accuracy: 1.000000\n",
      "Step 70000: Minibatch Loss: 0.651286, Minibatch Accuracy: 0.880000\n",
      "Step 71000: Minibatch Loss: 0.045380, Minibatch Accuracy: 1.000000\n",
      "Step 72000: Minibatch Loss: 0.000015, Minibatch Accuracy: 1.000000\n",
      "Step 73000: Minibatch Loss: 0.235004, Minibatch Accuracy: 1.000000\n",
      "Step 74000: Minibatch Loss: 0.041659, Minibatch Accuracy: 1.000000\n",
      "Step 75000: Minibatch Loss: 0.032802, Minibatch Accuracy: 1.000000\n",
      "Step 76000: Minibatch Loss: 0.276740, Minibatch Accuracy: 1.000000\n",
      "Step 77000: Minibatch Loss: 0.590522, Minibatch Accuracy: 1.000000\n",
      "Step 78000: Minibatch Loss: 0.311901, Minibatch Accuracy: 1.000000\n",
      "Step 79000: Minibatch Loss: 0.019285, Minibatch Accuracy: 1.000000\n",
      "Step 80000: Minibatch Loss: 0.004209, Minibatch Accuracy: 1.000000\n",
      "Step 81000: Minibatch Loss: 0.000485, Minibatch Accuracy: 1.000000\n",
      "Step 82000: Minibatch Loss: 0.017721, Minibatch Accuracy: 1.000000\n",
      "Step 83000: Minibatch Loss: 0.529222, Minibatch Accuracy: 1.000000\n",
      "Step 84000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 85000: Minibatch Loss: 0.577451, Minibatch Accuracy: 1.000000\n",
      "Step 86000: Minibatch Loss: 0.178028, Minibatch Accuracy: 1.000000\n",
      "Step 87000: Minibatch Loss: 0.338838, Minibatch Accuracy: 1.000000\n",
      "Step 88000: Minibatch Loss: 0.011087, Minibatch Accuracy: 1.000000\n",
      "Step 89000: Minibatch Loss: 0.000001, Minibatch Accuracy: 1.000000\n",
      "Step 90000: Minibatch Loss: 0.006431, Minibatch Accuracy: 1.000000\n",
      "Step 91000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 92000: Minibatch Loss: 0.043773, Minibatch Accuracy: 1.000000\n",
      "Step 93000: Minibatch Loss: 0.447607, Minibatch Accuracy: 1.000000\n",
      "Step 94000: Minibatch Loss: 0.000003, Minibatch Accuracy: 1.000000\n",
      "Step 95000: Minibatch Loss: 0.000003, Minibatch Accuracy: 1.000000\n",
      "Step 96000: Minibatch Loss: 0.637008, Minibatch Accuracy: 0.980000\n",
      "Step 97000: Minibatch Loss: 0.185764, Minibatch Accuracy: 1.000000\n",
      "Step 98000: Minibatch Loss: 0.000001, Minibatch Accuracy: 1.000000\n",
      "Step 99000: Minibatch Loss: 0.285608, Minibatch Accuracy: 1.000000\n",
      "Step 100000: Minibatch Loss: 0.578358, Minibatch Accuracy: 0.980000\n",
      "Step 101000: Minibatch Loss: 0.008310, Minibatch Accuracy: 1.000000\n",
      "Step 102000: Minibatch Loss: 0.001440, Minibatch Accuracy: 1.000000\n",
      "Step 103000: Minibatch Loss: 0.055709, Minibatch Accuracy: 1.000000\n",
      "Step 104000: Minibatch Loss: 0.102904, Minibatch Accuracy: 1.000000\n",
      "Step 105000: Minibatch Loss: 0.627324, Minibatch Accuracy: 0.980000\n",
      "Step 106000: Minibatch Loss: 0.571533, Minibatch Accuracy: 1.000000\n",
      "Step 107000: Minibatch Loss: 0.085796, Minibatch Accuracy: 1.000000\n",
      "Step 108000: Minibatch Loss: 0.119189, Minibatch Accuracy: 1.000000\n",
      "Step 109000: Minibatch Loss: 0.030638, Minibatch Accuracy: 1.000000\n",
      "Step 110000: Minibatch Loss: 0.022564, Minibatch Accuracy: 1.000000\n",
      "Step 111000: Minibatch Loss: 0.000016, Minibatch Accuracy: 1.000000\n",
      "Step 112000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 113000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 114000: Minibatch Loss: 0.202262, Minibatch Accuracy: 1.000000\n",
      "Step 115000: Minibatch Loss: 0.366408, Minibatch Accuracy: 1.000000\n",
      "Step 116000: Minibatch Loss: 0.464469, Minibatch Accuracy: 0.980000\n",
      "Step 117000: Minibatch Loss: 0.596021, Minibatch Accuracy: 0.980000\n",
      "Step 118000: Minibatch Loss: 0.188564, Minibatch Accuracy: 1.000000\n",
      "Step 119000: Minibatch Loss: 0.002434, Minibatch Accuracy: 1.000000\n",
      "Step 120000: Minibatch Loss: 0.437818, Minibatch Accuracy: 1.000000\n",
      "Step 121000: Minibatch Loss: 0.000169, Minibatch Accuracy: 1.000000\n",
      "Step 122000: Minibatch Loss: 0.000001, Minibatch Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 123000: Minibatch Loss: 0.619587, Minibatch Accuracy: 0.900000\n",
      "Step 124000: Minibatch Loss: 0.481971, Minibatch Accuracy: 0.980000\n",
      "Step 125000: Minibatch Loss: 0.083481, Minibatch Accuracy: 1.000000\n",
      "Step 126000: Minibatch Loss: 0.311671, Minibatch Accuracy: 1.000000\n",
      "Step 127000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 128000: Minibatch Loss: 0.102290, Minibatch Accuracy: 1.000000\n",
      "Step 129000: Minibatch Loss: 0.010401, Minibatch Accuracy: 1.000000\n",
      "Step 130000: Minibatch Loss: 0.155576, Minibatch Accuracy: 1.000000\n",
      "Step 131000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 132000: Minibatch Loss: 0.609817, Minibatch Accuracy: 0.900000\n",
      "Step 133000: Minibatch Loss: 0.007638, Minibatch Accuracy: 1.000000\n",
      "Step 134000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 135000: Minibatch Loss: 0.015168, Minibatch Accuracy: 1.000000\n",
      "Step 136000: Minibatch Loss: 0.000007, Minibatch Accuracy: 1.000000\n",
      "Step 137000: Minibatch Loss: 0.008684, Minibatch Accuracy: 1.000000\n",
      "Step 138000: Minibatch Loss: 0.006489, Minibatch Accuracy: 1.000000\n",
      "Step 139000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 140000: Minibatch Loss: 0.002047, Minibatch Accuracy: 1.000000\n",
      "Step 141000: Minibatch Loss: 0.005950, Minibatch Accuracy: 1.000000\n",
      "Step 142000: Minibatch Loss: 0.203100, Minibatch Accuracy: 1.000000\n",
      "Step 143000: Minibatch Loss: 0.196481, Minibatch Accuracy: 1.000000\n",
      "Step 144000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 145000: Minibatch Loss: 0.150544, Minibatch Accuracy: 1.000000\n",
      "Step 146000: Minibatch Loss: 0.000718, Minibatch Accuracy: 1.000000\n",
      "Step 147000: Minibatch Loss: 0.371230, Minibatch Accuracy: 0.980000\n",
      "Step 148000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 149000: Minibatch Loss: 0.095864, Minibatch Accuracy: 1.000000\n",
      "Step 150000: Minibatch Loss: 0.001368, Minibatch Accuracy: 1.000000\n",
      "Full batch Accuracy: 0.998333\n",
      "[[-0.00320613  0.00045935 -0.00828474]\n",
      " [-0.00263711  0.00027083 -0.00842275]\n",
      " [-0.0030892   0.00029903 -0.00819533]\n",
      " ..., \n",
      " [-0.00324149  0.00037544 -0.0080972 ]\n",
      " [-0.00287525  0.00027178 -0.00828941]\n",
      " [-0.00360248  0.00059002 -0.00816247]]\n",
      "[[-0.00322729  0.00044722 -0.00826813]\n",
      " [-0.002644    0.00025515 -0.00841019]\n",
      " [-0.0031075   0.00028327 -0.00817644]\n",
      " ..., \n",
      " [-0.00326588  0.00036277 -0.0080767 ]\n",
      " [-0.00289186  0.00025576 -0.00826991]\n",
      " [-0.00362633  0.00058033 -0.00814678]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range (150001):\n",
    "        Order = Sequence[step % 24]\n",
    "        batch_X = np.reshape(RealOESdata[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_InputUnit))\n",
    "        \n",
    "        ### Y_ClassifiedResult_Power -> Power에 대한 one hot vector\n",
    "        batch_Y = np.reshape(Y_ClassifiedResult_C3H6[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_OutputUnit))\n",
    "        session.run(train, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 0.5})\n",
    "        MinibatchCost = session.run(cost, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "            print ('Step %i: Minibatch Loss: %f, Minibatch Accuracy: %f' % (step, MinibatchCost,\n",
    "                                                                            train_accuracy))\n",
    "            \n",
    "        if step % 24 == 0:\n",
    "            random.shuffle(Sequence)\n",
    "    \n",
    "    RealOESdata_Reshaped = np.reshape(RealOESdata, (1200, 3648))\n",
    "    \n",
    "    ###Y_ClassifiedResult_Power\n",
    "    train_accuracy = accuracy.eval(feed_dict = {X: RealOESdata_Reshaped, Y: Y_ClassifiedResult_C3H6, keep_prob: 1.0})        \n",
    "    print('Full batch Accuracy: %f' % (train_accuracy))\n",
    "    \n",
    "    PredictedSimplifiedInputData = session.run(logits, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputData)\n",
    "    np.savetxt('C3H6_Weight.txt', PredictedSimplifiedInputData, delimiter='\\t')\n",
    "    \n",
    "    PredictedSimplifiedInputDataLinear = session.run(WL4, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputDataLinear)\n",
    "    np.savetxt('C3H6_Weight2.txt', PredictedSimplifiedInputDataLinear, delimiter='\\t')\n",
    "    \n",
    "#    for i in range(24):\n",
    "#        Result_batch_X = np.reshape(RealOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    "#                                    (batchsize, n_InputUnit))\n",
    "#        PredictedBatchData = session.run(prediction, feed_dict = {X : Result_batch_X, keep_prob: 1.0})\n",
    "#        \n",
    "#        ### Y_ClassifiedResult_Power\n",
    "#        Y_ClassifiedResult_Reshaped = np.reshape(Y_ClassifiedResult_C3H6[i*batchsize: (i+1)*batchsize,:],\n",
    "#                                                 (batchsize, n_OutputUnit))\n",
    "#        \n",
    "#        if i % 1 == 0:\n",
    "#            print(\"Original Data\")\n",
    "#            print(Y_ClassifiedResult_Reshaped[0,:])\n",
    "#        \n",
    "#        print(i)\n",
    "#        print(\"Result of predicting EntireData\")\n",
    "#        print(PredictedBatchData)\n",
    "#            \n",
    "#    print(\"####################################################\")  \n",
    "#    \n",
    "#    for i in range(24):\n",
    "#        #\n",
    "#        Test_batch_X = np.reshape(TestOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    "#                                    (batchsize, n_InputUnit))\n",
    "#        \n",
    "#        \n",
    "#        PredictedTestBatchData = session.run(prediction, feed_dict = {X : Test_batch_X, keep_prob: 1.0})\n",
    "#        \n",
    "#        ###Y_Test_Power -> Test data의 power에 대한 one hot vector\n",
    "#        #\n",
    "#        Y_Test_Reshaped = np.reshape(Y_Test_C3H6[i*batchsize: (i+1)*batchsize,:],\n",
    "#                                                 (batchsize, n_OutputUnit))\n",
    "#        \n",
    "#        if i % 1 == 0:\n",
    "#            print(\"Original Data\")\n",
    "#            print(Y_Test_Reshaped[0,:])\n",
    "#        \n",
    "#        print(i)\n",
    "#        print(\"Result of Predicted Test Data\")\n",
    "#        print(PredictedTestBatchData)\n",
    "#    #\n",
    "#    TestOESdata_Reshaped = np.reshape(TestOESdata, (1200,3648))\n",
    "#    \n",
    "#    ###Y_Test_Power\n",
    "#    #\n",
    "#    train_accuracy = accuracy.eval(feed_dict = {X: TestOESdata_Reshaped, Y: Y_Test_C3H6, keep_prob: 1.0})        \n",
    "#    print('Test Full batch Accuracy: %f' % (train_accuracy))          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EO99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000001\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "n_InputUnit = 3648\n",
    "n_1stHiddenUnit = 500\n",
    "n_2ndHiddenUnit = 200\n",
    "n_3rdHiddenUnit = 60\n",
    "n_OutputUnit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm which decides standard for classifying data\n",
    "# Algorithm which can detect some kind of different phenomena\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_InputUnit, n_1stHiddenUnit],stddev=0.01))\n",
    "W2 = tf.Variable(tf.random_normal([n_1stHiddenUnit, n_2ndHiddenUnit],stddev=0.01))\n",
    "W3 = tf.Variable(tf.random_normal([n_2ndHiddenUnit, n_3rdHiddenUnit],stddev=0.01))\n",
    "W4 = tf.Variable(tf.random_normal([n_3rdHiddenUnit, n_OutputUnit],stddev=0.01))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([n_1stHiddenUnit], stddev=0.01))\n",
    "b2 = tf.Variable(tf.random_normal([n_2ndHiddenUnit], stddev=0.01))\n",
    "b3 = tf.Variable(tf.random_normal([n_3rdHiddenUnit], stddev=0.01))\n",
    "b4 = tf.Variable(tf.random_normal([n_OutputUnit], stddev=0.01))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_InputUnit]) # tensorflow의 placeholder라는 변수를 선언\n",
    "Y = tf.placeholder(tf.float32, [None, n_OutputUnit])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "L1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "L2 = tf.nn.elu(tf.matmul(L1_drop, W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "L3 = tf.nn.elu(tf.matmul(L2_drop, W3) + b3)\n",
    "logits = tf.matmul(L3, W4) + b4\n",
    "Hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "WL1 = tf.matmul(X, W1) + b1\n",
    "WL1_drop = tf.nn.dropout(WL1, keep_prob)\n",
    "\n",
    "WL2 = tf.matmul(WL1_drop, W2) + b2\n",
    "WL2_drop = tf.nn.dropout(WL2, keep_prob)\n",
    "\n",
    "WL3 = tf.matmul(WL2_drop, W3) + b3\n",
    "\n",
    "WL4 = tf.matmul(WL3, W4) + b4\n",
    "\n",
    "#tf.stop_gradient(Y)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "#cost = cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(, Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.5).minimize(cost)\n",
    "prediction = tf.argmax(Hypothesis, 1)\n",
    "correct_prediction = tf.equal(tf.argmax(Hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Minibatch Loss: 1.968719, Minibatch Accuracy: 0.000000\n",
      "Step 1000: Minibatch Loss: 0.547718, Minibatch Accuracy: 1.000000\n",
      "Step 2000: Minibatch Loss: 0.733248, Minibatch Accuracy: 1.000000\n",
      "Step 3000: Minibatch Loss: 0.749760, Minibatch Accuracy: 1.000000\n",
      "Step 4000: Minibatch Loss: 0.617076, Minibatch Accuracy: 1.000000\n",
      "Step 5000: Minibatch Loss: 0.749758, Minibatch Accuracy: 0.360000\n",
      "Step 6000: Minibatch Loss: 1.848539, Minibatch Accuracy: 0.000000\n",
      "Step 7000: Minibatch Loss: 0.593623, Minibatch Accuracy: 1.000000\n",
      "Step 8000: Minibatch Loss: 0.649740, Minibatch Accuracy: 1.000000\n",
      "Step 9000: Minibatch Loss: 0.684291, Minibatch Accuracy: 1.000000\n",
      "Step 10000: Minibatch Loss: 0.625726, Minibatch Accuracy: 1.000000\n",
      "Step 11000: Minibatch Loss: 0.321760, Minibatch Accuracy: 1.000000\n",
      "Step 12000: Minibatch Loss: 0.192593, Minibatch Accuracy: 1.000000\n",
      "Step 13000: Minibatch Loss: 0.297028, Minibatch Accuracy: 1.000000\n",
      "Step 14000: Minibatch Loss: 0.271801, Minibatch Accuracy: 1.000000\n",
      "Step 15000: Minibatch Loss: 0.432759, Minibatch Accuracy: 1.000000\n",
      "Step 16000: Minibatch Loss: 0.578019, Minibatch Accuracy: 1.000000\n",
      "Step 17000: Minibatch Loss: 0.463201, Minibatch Accuracy: 1.000000\n",
      "Step 18000: Minibatch Loss: 0.996577, Minibatch Accuracy: 0.000000\n",
      "Step 19000: Minibatch Loss: 0.624549, Minibatch Accuracy: 1.000000\n",
      "Step 20000: Minibatch Loss: 0.172010, Minibatch Accuracy: 1.000000\n",
      "Step 21000: Minibatch Loss: 1.256796, Minibatch Accuracy: 0.000000\n",
      "Step 22000: Minibatch Loss: 0.302657, Minibatch Accuracy: 1.000000\n",
      "Step 23000: Minibatch Loss: 0.213740, Minibatch Accuracy: 1.000000\n",
      "Step 24000: Minibatch Loss: 0.433732, Minibatch Accuracy: 1.000000\n",
      "Step 25000: Minibatch Loss: 0.063483, Minibatch Accuracy: 1.000000\n",
      "Step 26000: Minibatch Loss: 0.105938, Minibatch Accuracy: 1.000000\n",
      "Step 27000: Minibatch Loss: 0.304342, Minibatch Accuracy: 1.000000\n",
      "Step 28000: Minibatch Loss: 0.139532, Minibatch Accuracy: 1.000000\n",
      "Step 29000: Minibatch Loss: 0.136090, Minibatch Accuracy: 1.000000\n",
      "Step 30000: Minibatch Loss: 0.129463, Minibatch Accuracy: 1.000000\n",
      "Step 31000: Minibatch Loss: 0.253484, Minibatch Accuracy: 1.000000\n",
      "Step 32000: Minibatch Loss: 0.140596, Minibatch Accuracy: 1.000000\n",
      "Step 33000: Minibatch Loss: 0.041128, Minibatch Accuracy: 1.000000\n",
      "Step 34000: Minibatch Loss: 0.266892, Minibatch Accuracy: 1.000000\n",
      "Step 35000: Minibatch Loss: 0.214106, Minibatch Accuracy: 1.000000\n",
      "Step 36000: Minibatch Loss: 0.103994, Minibatch Accuracy: 1.000000\n",
      "Step 37000: Minibatch Loss: 0.205155, Minibatch Accuracy: 1.000000\n",
      "Step 38000: Minibatch Loss: 0.010917, Minibatch Accuracy: 1.000000\n",
      "Step 39000: Minibatch Loss: 0.241819, Minibatch Accuracy: 1.000000\n",
      "Step 40000: Minibatch Loss: 0.850302, Minibatch Accuracy: 0.060000\n",
      "Step 41000: Minibatch Loss: 0.043964, Minibatch Accuracy: 1.000000\n",
      "Step 42000: Minibatch Loss: 0.096556, Minibatch Accuracy: 1.000000\n",
      "Step 43000: Minibatch Loss: 0.036952, Minibatch Accuracy: 1.000000\n",
      "Step 44000: Minibatch Loss: 0.000031, Minibatch Accuracy: 1.000000\n",
      "Step 45000: Minibatch Loss: 0.024314, Minibatch Accuracy: 1.000000\n",
      "Step 46000: Minibatch Loss: 0.053607, Minibatch Accuracy: 1.000000\n",
      "Step 47000: Minibatch Loss: 0.000042, Minibatch Accuracy: 1.000000\n",
      "Step 48000: Minibatch Loss: 0.215310, Minibatch Accuracy: 1.000000\n",
      "Step 49000: Minibatch Loss: 0.122738, Minibatch Accuracy: 1.000000\n",
      "Step 50000: Minibatch Loss: 0.008580, Minibatch Accuracy: 1.000000\n",
      "Step 51000: Minibatch Loss: 0.062721, Minibatch Accuracy: 1.000000\n",
      "Step 52000: Minibatch Loss: 0.505193, Minibatch Accuracy: 1.000000\n",
      "Step 53000: Minibatch Loss: 0.027595, Minibatch Accuracy: 1.000000\n",
      "Step 54000: Minibatch Loss: 0.185486, Minibatch Accuracy: 1.000000\n",
      "Step 55000: Minibatch Loss: 0.047912, Minibatch Accuracy: 1.000000\n",
      "Step 56000: Minibatch Loss: 0.010213, Minibatch Accuracy: 1.000000\n",
      "Step 57000: Minibatch Loss: 0.014334, Minibatch Accuracy: 1.000000\n",
      "Step 58000: Minibatch Loss: 0.062086, Minibatch Accuracy: 1.000000\n",
      "Step 59000: Minibatch Loss: 0.029130, Minibatch Accuracy: 1.000000\n",
      "Step 60000: Minibatch Loss: 0.034990, Minibatch Accuracy: 1.000000\n",
      "Step 61000: Minibatch Loss: 0.007937, Minibatch Accuracy: 1.000000\n",
      "Step 62000: Minibatch Loss: 0.008383, Minibatch Accuracy: 1.000000\n",
      "Step 63000: Minibatch Loss: 0.026022, Minibatch Accuracy: 1.000000\n",
      "Step 64000: Minibatch Loss: 0.015657, Minibatch Accuracy: 1.000000\n",
      "Step 65000: Minibatch Loss: 0.242908, Minibatch Accuracy: 1.000000\n",
      "Step 66000: Minibatch Loss: 0.010414, Minibatch Accuracy: 1.000000\n",
      "Step 67000: Minibatch Loss: 0.012777, Minibatch Accuracy: 1.000000\n",
      "Step 68000: Minibatch Loss: 0.013272, Minibatch Accuracy: 1.000000\n",
      "Step 69000: Minibatch Loss: 0.010828, Minibatch Accuracy: 1.000000\n",
      "Step 70000: Minibatch Loss: 0.021462, Minibatch Accuracy: 1.000000\n",
      "Step 71000: Minibatch Loss: 0.000001, Minibatch Accuracy: 1.000000\n",
      "Step 72000: Minibatch Loss: 0.005515, Minibatch Accuracy: 1.000000\n",
      "Step 73000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 74000: Minibatch Loss: 0.005652, Minibatch Accuracy: 1.000000\n",
      "Step 75000: Minibatch Loss: 0.001086, Minibatch Accuracy: 1.000000\n",
      "Step 76000: Minibatch Loss: 0.000163, Minibatch Accuracy: 1.000000\n",
      "Step 77000: Minibatch Loss: 0.011431, Minibatch Accuracy: 1.000000\n",
      "Step 78000: Minibatch Loss: 0.003138, Minibatch Accuracy: 1.000000\n",
      "Step 79000: Minibatch Loss: 0.001071, Minibatch Accuracy: 1.000000\n",
      "Step 80000: Minibatch Loss: 0.000197, Minibatch Accuracy: 1.000000\n",
      "Step 81000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 82000: Minibatch Loss: 0.003609, Minibatch Accuracy: 1.000000\n",
      "Step 83000: Minibatch Loss: 0.000001, Minibatch Accuracy: 1.000000\n",
      "Step 84000: Minibatch Loss: 0.008549, Minibatch Accuracy: 1.000000\n",
      "Step 85000: Minibatch Loss: 0.032620, Minibatch Accuracy: 1.000000\n",
      "Step 86000: Minibatch Loss: 0.000005, Minibatch Accuracy: 1.000000\n",
      "Step 87000: Minibatch Loss: 0.003151, Minibatch Accuracy: 1.000000\n",
      "Step 88000: Minibatch Loss: 0.002440, Minibatch Accuracy: 1.000000\n",
      "Step 89000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 90000: Minibatch Loss: 0.006649, Minibatch Accuracy: 1.000000\n",
      "Step 91000: Minibatch Loss: 0.002018, Minibatch Accuracy: 1.000000\n",
      "Step 92000: Minibatch Loss: 0.001032, Minibatch Accuracy: 1.000000\n",
      "Step 93000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 94000: Minibatch Loss: 0.000003, Minibatch Accuracy: 1.000000\n",
      "Step 95000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 96000: Minibatch Loss: 0.000022, Minibatch Accuracy: 1.000000\n",
      "Step 97000: Minibatch Loss: 0.189033, Minibatch Accuracy: 1.000000\n",
      "Step 98000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 99000: Minibatch Loss: 0.002789, Minibatch Accuracy: 1.000000\n",
      "Step 100000: Minibatch Loss: 0.000017, Minibatch Accuracy: 1.000000\n",
      "Step 101000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 102000: Minibatch Loss: 0.003106, Minibatch Accuracy: 1.000000\n",
      "Step 103000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 104000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 105000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 106000: Minibatch Loss: 0.121515, Minibatch Accuracy: 1.000000\n",
      "Step 107000: Minibatch Loss: 0.000007, Minibatch Accuracy: 1.000000\n",
      "Step 108000: Minibatch Loss: 0.003580, Minibatch Accuracy: 1.000000\n",
      "Step 109000: Minibatch Loss: 0.072801, Minibatch Accuracy: 1.000000\n",
      "Step 110000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 111000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 112000: Minibatch Loss: 0.002448, Minibatch Accuracy: 1.000000\n",
      "Step 113000: Minibatch Loss: 0.004708, Minibatch Accuracy: 1.000000\n",
      "Step 114000: Minibatch Loss: 0.071029, Minibatch Accuracy: 1.000000\n",
      "Step 115000: Minibatch Loss: 0.000015, Minibatch Accuracy: 1.000000\n",
      "Step 116000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 117000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 118000: Minibatch Loss: 0.000583, Minibatch Accuracy: 1.000000\n",
      "Step 119000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 120000: Minibatch Loss: 0.049174, Minibatch Accuracy: 1.000000\n",
      "Step 121000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 122000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 123000: Minibatch Loss: 0.011803, Minibatch Accuracy: 1.000000\n",
      "Step 124000: Minibatch Loss: 0.001487, Minibatch Accuracy: 1.000000\n",
      "Step 125000: Minibatch Loss: 0.001559, Minibatch Accuracy: 1.000000\n",
      "Step 126000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 127000: Minibatch Loss: 0.003447, Minibatch Accuracy: 1.000000\n",
      "Step 128000: Minibatch Loss: 0.000856, Minibatch Accuracy: 1.000000\n",
      "Step 129000: Minibatch Loss: 0.000485, Minibatch Accuracy: 1.000000\n",
      "Step 130000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 131000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 132000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 133000: Minibatch Loss: 0.002493, Minibatch Accuracy: 1.000000\n",
      "Step 134000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 135000: Minibatch Loss: 0.001280, Minibatch Accuracy: 1.000000\n",
      "Step 136000: Minibatch Loss: 0.000863, Minibatch Accuracy: 1.000000\n",
      "Step 137000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 138000: Minibatch Loss: 0.003055, Minibatch Accuracy: 1.000000\n",
      "Step 139000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 140000: Minibatch Loss: 0.001560, Minibatch Accuracy: 1.000000\n",
      "Step 141000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 142000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 143000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 144000: Minibatch Loss: 0.016284, Minibatch Accuracy: 1.000000\n",
      "Step 145000: Minibatch Loss: 0.005352, Minibatch Accuracy: 1.000000\n",
      "Step 146000: Minibatch Loss: 0.000105, Minibatch Accuracy: 1.000000\n",
      "Step 147000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 148000: Minibatch Loss: 0.000484, Minibatch Accuracy: 1.000000\n",
      "Step 149000: Minibatch Loss: 0.000450, Minibatch Accuracy: 1.000000\n",
      "Step 150000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Full batch Accuracy: 1.000000\n",
      "[[-0.01336224  0.00174975  0.00236854]\n",
      " [-0.01281829  0.00174864  0.00151949]\n",
      " [-0.01267876  0.00171039  0.00151869]\n",
      " ..., \n",
      " [-0.01250809  0.00158666  0.00142988]\n",
      " [-0.0130061   0.00168314  0.00205314]\n",
      " [-0.0130127   0.00179737  0.00192833]]\n",
      "[[-0.01336876  0.00175739  0.00236783]\n",
      " [-0.0128167   0.00175825  0.00150113]\n",
      " [-0.01267788  0.001718    0.00150933]\n",
      " ..., \n",
      " [-0.01250355  0.00159491  0.00141044]\n",
      " [-0.01300793  0.00169175  0.00204374]\n",
      " [-0.01301325  0.00180676  0.00191643]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range (150001):\n",
    "        Order = Sequence[step % 24]\n",
    "        batch_X = np.reshape(RealOESdata[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_InputUnit))\n",
    "        \n",
    "        ### Y_ClassifiedResult_Power -> Power에 대한 one hot vector\n",
    "        batch_Y = np.reshape(Y_ClassifiedResult_N2[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_OutputUnit))\n",
    "        session.run(train, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 0.5})\n",
    "        MinibatchCost = session.run(cost, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "            print ('Step %i: Minibatch Loss: %f, Minibatch Accuracy: %f' % (step, MinibatchCost,\n",
    "                                                                            train_accuracy))\n",
    "            \n",
    "        if step % 24 == 0:\n",
    "            random.shuffle(Sequence)\n",
    "    \n",
    "    RealOESdata_Reshaped = np.reshape(RealOESdata, (1200, 3648))\n",
    "    \n",
    "    ###Y_ClassifiedResult_Power\n",
    "    train_accuracy = accuracy.eval(feed_dict = {X: RealOESdata_Reshaped, Y: Y_ClassifiedResult_N2, keep_prob: 1.0})        \n",
    "    print('Full batch Accuracy: %f' % (train_accuracy))\n",
    "    \n",
    "    PredictedSimplifiedInputData = session.run(logits, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputData)\n",
    "    np.savetxt('N2_Weight.txt', PredictedSimplifiedInputData, delimiter='\\t')\n",
    "    \n",
    "    PredictedSimplifiedInputDataLinear = session.run(WL4, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputDataLinear)\n",
    "    np.savetxt('N2_Weight2.txt', PredictedSimplifiedInputDataLinear, delimiter='\\t')\n",
    "    \n",
    " #   for i in range(24):\n",
    " #       Result_batch_X = np.reshape(RealOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    " #                                   (batchsize, n_InputUnit))\n",
    " #       PredictedBatchData = session.run(prediction, feed_dict = {X : Result_batch_X, keep_prob: 1.0})\n",
    " #       \n",
    " #       ### Y_ClassifiedResult_Power\n",
    " #       Y_ClassifiedResult_Reshaped = np.reshape(Y_ClassifiedResult_N2[i*batchsize: (i+1)*batchsize,:],\n",
    " #                                                (batchsize, n_OutputUnit))\n",
    " #       \n",
    " #       if i % 1 == 0:\n",
    " #           print(\"Original Data\")\n",
    " #           print(Y_ClassifiedResult_Reshaped[0,:])\n",
    " #       \n",
    " #       print(i)\n",
    " #       print(\"Result of predicting EntireData\")\n",
    " #       print(PredictedBatchData)\n",
    " #           \n",
    " #   print(\"####################################################\")  \n",
    " #   \n",
    " #   for i in range(24):\n",
    " #       #\n",
    " #       Test_batch_X = np.reshape(TestOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    " #                                   (batchsize, n_InputUnit))\n",
    " #       \n",
    " #       \n",
    " #       PredictedTestBatchData = session.run(prediction, feed_dict = {X : Test_batch_X, keep_prob: 1.0})\n",
    " #       \n",
    " #       ###Y_Test_Power -> Test data의 power에 대한 one hot vector\n",
    " #       #\n",
    " #       Y_Test_Reshaped = np.reshape(Y_Test_N2[i*batchsize: (i+1)*batchsize,:],\n",
    " #                                                (batchsize, n_OutputUnit))\n",
    " #       \n",
    " #       if i % 1 == 0:\n",
    " #           print(\"Original Data\")\n",
    " #           print(Y_Test_Reshaped[0,:])\n",
    " #       \n",
    " #       print(i)\n",
    " #       print(\"Result of Predicted Test Data\")\n",
    " #       print(PredictedTestBatchData)\n",
    " #   #\n",
    " #   TestOESdata_Reshaped = np.reshape(TestOESdata, (1200,3648))\n",
    " #   \n",
    " #   ###Y_Test_Power\n",
    " #   #\n",
    " #   train_accuracy = accuracy.eval(feed_dict = {X: TestOESdata_Reshaped, Y: Y_Test_N2, keep_prob: 1.0})        \n",
    " #   print('Test Full batch Accuracy: %f' % (train_accuracy))          \n",
    " #   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좀 전에 돌린거2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000001\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "n_InputUnit = 3648\n",
    "n_1stHiddenUnit = 500\n",
    "n_2ndHiddenUnit = 200\n",
    "n_3rdHiddenUnit = 60\n",
    "n_OutputUnit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm which decides standard for classifying data\n",
    "# Algorithm which can detect some kind of different phenomena\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_InputUnit, n_1stHiddenUnit],stddev=0.01))\n",
    "W2 = tf.Variable(tf.random_normal([n_1stHiddenUnit, n_2ndHiddenUnit],stddev=0.01))\n",
    "W3 = tf.Variable(tf.random_normal([n_2ndHiddenUnit, n_3rdHiddenUnit],stddev=0.01))\n",
    "W4 = tf.Variable(tf.random_normal([n_3rdHiddenUnit, n_OutputUnit],stddev=0.01))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([n_1stHiddenUnit], stddev=0.01))\n",
    "b2 = tf.Variable(tf.random_normal([n_2ndHiddenUnit], stddev=0.01))\n",
    "b3 = tf.Variable(tf.random_normal([n_3rdHiddenUnit], stddev=0.01))\n",
    "b4 = tf.Variable(tf.random_normal([n_OutputUnit], stddev=0.01))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_InputUnit]) # tensorflow의 placeholder라는 변수를 선언\n",
    "Y = tf.placeholder(tf.float32, [None, n_OutputUnit])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "L1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "L2 = tf.nn.elu(tf.matmul(L1_drop, W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "L3 = tf.nn.elu(tf.matmul(L2_drop, W3) + b3)\n",
    "logits = tf.matmul(L3, W4) + b4\n",
    "Hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "WL1 = tf.matmul(X, W1) + b1\n",
    "WL1_drop = tf.nn.dropout(WL1, keep_prob)\n",
    "\n",
    "WL2 = tf.matmul(WL1_drop, W2) + b2\n",
    "WL2_drop = tf.nn.dropout(WL2, keep_prob)\n",
    "\n",
    "WL3 = tf.matmul(WL2_drop, W3) + b3\n",
    "\n",
    "WL4 = tf.matmul(WL3, W4) + b4\n",
    "\n",
    "#tf.stop_gradient(Y)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "#cost = cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(, Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.5).minimize(cost)\n",
    "prediction = tf.argmax(Hypothesis, 1)\n",
    "correct_prediction = tf.equal(tf.argmax(Hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Minibatch Loss: 1.200342, Minibatch Accuracy: 0.000000\n",
      "Step 1000: Minibatch Loss: 0.843270, Minibatch Accuracy: 1.000000\n",
      "Step 2000: Minibatch Loss: 1.349878, Minibatch Accuracy: 0.000000\n",
      "Step 3000: Minibatch Loss: 0.777171, Minibatch Accuracy: 1.000000\n",
      "Step 4000: Minibatch Loss: 0.874479, Minibatch Accuracy: 1.000000\n",
      "Step 5000: Minibatch Loss: 0.807099, Minibatch Accuracy: 1.000000\n",
      "Step 6000: Minibatch Loss: 0.863224, Minibatch Accuracy: 0.000000\n",
      "Step 7000: Minibatch Loss: 0.666214, Minibatch Accuracy: 1.000000\n",
      "Step 8000: Minibatch Loss: 0.582226, Minibatch Accuracy: 1.000000\n",
      "Step 9000: Minibatch Loss: 0.904552, Minibatch Accuracy: 1.000000\n",
      "Step 10000: Minibatch Loss: 0.689404, Minibatch Accuracy: 1.000000\n",
      "Step 11000: Minibatch Loss: 0.691547, Minibatch Accuracy: 1.000000\n",
      "Step 12000: Minibatch Loss: 1.003530, Minibatch Accuracy: 0.000000\n",
      "Step 13000: Minibatch Loss: 0.625697, Minibatch Accuracy: 1.000000\n",
      "Step 14000: Minibatch Loss: 1.072404, Minibatch Accuracy: 0.000000\n",
      "Step 15000: Minibatch Loss: 0.888370, Minibatch Accuracy: 0.000000\n",
      "Step 16000: Minibatch Loss: 0.679120, Minibatch Accuracy: 1.000000\n",
      "Step 17000: Minibatch Loss: 0.457011, Minibatch Accuracy: 1.000000\n",
      "Step 18000: Minibatch Loss: 0.487329, Minibatch Accuracy: 1.000000\n",
      "Step 19000: Minibatch Loss: 0.922832, Minibatch Accuracy: 0.000000\n",
      "Step 20000: Minibatch Loss: 0.183375, Minibatch Accuracy: 1.000000\n",
      "Step 21000: Minibatch Loss: 0.891858, Minibatch Accuracy: 0.000000\n",
      "Step 22000: Minibatch Loss: 0.719325, Minibatch Accuracy: 1.000000\n",
      "Step 23000: Minibatch Loss: 0.453951, Minibatch Accuracy: 1.000000\n",
      "Step 24000: Minibatch Loss: 0.446348, Minibatch Accuracy: 1.000000\n",
      "Step 25000: Minibatch Loss: 1.015600, Minibatch Accuracy: 0.000000\n",
      "Step 26000: Minibatch Loss: 0.814974, Minibatch Accuracy: 0.000000\n",
      "Step 27000: Minibatch Loss: 0.627736, Minibatch Accuracy: 1.000000\n",
      "Step 28000: Minibatch Loss: 1.163265, Minibatch Accuracy: 0.000000\n",
      "Step 29000: Minibatch Loss: 0.194040, Minibatch Accuracy: 1.000000\n",
      "Step 30000: Minibatch Loss: 1.047772, Minibatch Accuracy: 0.000000\n",
      "Step 31000: Minibatch Loss: 0.280716, Minibatch Accuracy: 1.000000\n",
      "Step 32000: Minibatch Loss: 0.187387, Minibatch Accuracy: 1.000000\n",
      "Step 33000: Minibatch Loss: 0.100936, Minibatch Accuracy: 1.000000\n",
      "Step 34000: Minibatch Loss: 0.500092, Minibatch Accuracy: 1.000000\n",
      "Step 35000: Minibatch Loss: 0.597640, Minibatch Accuracy: 1.000000\n",
      "Step 36000: Minibatch Loss: 0.422686, Minibatch Accuracy: 1.000000\n",
      "Step 37000: Minibatch Loss: 0.254940, Minibatch Accuracy: 1.000000\n",
      "Step 38000: Minibatch Loss: 0.452917, Minibatch Accuracy: 1.000000\n",
      "Step 39000: Minibatch Loss: 0.827726, Minibatch Accuracy: 0.060000\n",
      "Step 40000: Minibatch Loss: 0.201027, Minibatch Accuracy: 1.000000\n",
      "Step 41000: Minibatch Loss: 0.059070, Minibatch Accuracy: 1.000000\n",
      "Step 42000: Minibatch Loss: 0.339213, Minibatch Accuracy: 1.000000\n",
      "Step 43000: Minibatch Loss: 0.182976, Minibatch Accuracy: 1.000000\n",
      "Step 44000: Minibatch Loss: 0.685447, Minibatch Accuracy: 1.000000\n",
      "Step 45000: Minibatch Loss: 0.318811, Minibatch Accuracy: 1.000000\n",
      "Step 46000: Minibatch Loss: 0.685057, Minibatch Accuracy: 0.640000\n",
      "Step 47000: Minibatch Loss: 0.312077, Minibatch Accuracy: 1.000000\n",
      "Step 48000: Minibatch Loss: 0.465961, Minibatch Accuracy: 1.000000\n",
      "Step 49000: Minibatch Loss: 0.387587, Minibatch Accuracy: 1.000000\n",
      "Step 50000: Minibatch Loss: 0.008981, Minibatch Accuracy: 1.000000\n",
      "Step 51000: Minibatch Loss: 0.489974, Minibatch Accuracy: 1.000000\n",
      "Step 52000: Minibatch Loss: 0.018423, Minibatch Accuracy: 1.000000\n",
      "Step 53000: Minibatch Loss: 0.237131, Minibatch Accuracy: 1.000000\n",
      "Step 54000: Minibatch Loss: 0.514716, Minibatch Accuracy: 0.980000\n",
      "Step 55000: Minibatch Loss: 0.760826, Minibatch Accuracy: 0.000000\n",
      "Step 56000: Minibatch Loss: 0.712265, Minibatch Accuracy: 0.080000\n",
      "Step 57000: Minibatch Loss: 0.723077, Minibatch Accuracy: 0.000000\n",
      "Step 58000: Minibatch Loss: 0.583542, Minibatch Accuracy: 1.000000\n",
      "Step 59000: Minibatch Loss: 1.072638, Minibatch Accuracy: 0.000000\n",
      "Step 60000: Minibatch Loss: 1.082260, Minibatch Accuracy: 0.000000\n",
      "Step 61000: Minibatch Loss: 0.940410, Minibatch Accuracy: 0.000000\n",
      "Step 62000: Minibatch Loss: 0.255397, Minibatch Accuracy: 1.000000\n",
      "Step 63000: Minibatch Loss: 0.090338, Minibatch Accuracy: 1.000000\n",
      "Step 64000: Minibatch Loss: 0.118299, Minibatch Accuracy: 1.000000\n",
      "Step 65000: Minibatch Loss: 0.228189, Minibatch Accuracy: 1.000000\n",
      "Step 66000: Minibatch Loss: 0.216894, Minibatch Accuracy: 1.000000\n",
      "Step 67000: Minibatch Loss: 0.315192, Minibatch Accuracy: 1.000000\n",
      "Step 68000: Minibatch Loss: 0.057608, Minibatch Accuracy: 1.000000\n",
      "Step 69000: Minibatch Loss: 0.601509, Minibatch Accuracy: 1.000000\n",
      "Step 70000: Minibatch Loss: 0.205621, Minibatch Accuracy: 1.000000\n",
      "Step 71000: Minibatch Loss: 0.103853, Minibatch Accuracy: 1.000000\n",
      "Step 72000: Minibatch Loss: 0.275510, Minibatch Accuracy: 1.000000\n",
      "Step 73000: Minibatch Loss: 0.018539, Minibatch Accuracy: 1.000000\n",
      "Step 74000: Minibatch Loss: 0.603835, Minibatch Accuracy: 1.000000\n",
      "Step 75000: Minibatch Loss: 0.540486, Minibatch Accuracy: 1.000000\n",
      "Step 76000: Minibatch Loss: 0.000643, Minibatch Accuracy: 1.000000\n",
      "Step 77000: Minibatch Loss: 0.002977, Minibatch Accuracy: 1.000000\n",
      "Step 78000: Minibatch Loss: 0.000412, Minibatch Accuracy: 1.000000\n",
      "Step 79000: Minibatch Loss: 0.191656, Minibatch Accuracy: 1.000000\n",
      "Step 80000: Minibatch Loss: 1.251331, Minibatch Accuracy: 0.000000\n",
      "Step 81000: Minibatch Loss: 0.457203, Minibatch Accuracy: 1.000000\n",
      "Step 82000: Minibatch Loss: 0.402679, Minibatch Accuracy: 1.000000\n",
      "Step 83000: Minibatch Loss: 0.176760, Minibatch Accuracy: 1.000000\n",
      "Step 84000: Minibatch Loss: 0.451680, Minibatch Accuracy: 1.000000\n",
      "Step 85000: Minibatch Loss: 0.003133, Minibatch Accuracy: 1.000000\n",
      "Step 86000: Minibatch Loss: 0.339220, Minibatch Accuracy: 1.000000\n",
      "Step 87000: Minibatch Loss: 0.138276, Minibatch Accuracy: 1.000000\n",
      "Step 88000: Minibatch Loss: 0.167604, Minibatch Accuracy: 1.000000\n",
      "Step 89000: Minibatch Loss: 0.249438, Minibatch Accuracy: 1.000000\n",
      "Step 90000: Minibatch Loss: 0.044246, Minibatch Accuracy: 1.000000\n",
      "Step 91000: Minibatch Loss: 1.082628, Minibatch Accuracy: 0.000000\n",
      "Step 92000: Minibatch Loss: 0.084760, Minibatch Accuracy: 1.000000\n",
      "Step 93000: Minibatch Loss: 0.165708, Minibatch Accuracy: 1.000000\n",
      "Step 94000: Minibatch Loss: 0.027462, Minibatch Accuracy: 1.000000\n",
      "Step 95000: Minibatch Loss: 0.061906, Minibatch Accuracy: 1.000000\n",
      "Step 96000: Minibatch Loss: 0.121162, Minibatch Accuracy: 1.000000\n",
      "Step 97000: Minibatch Loss: 0.287502, Minibatch Accuracy: 1.000000\n",
      "Step 98000: Minibatch Loss: 0.000004, Minibatch Accuracy: 1.000000\n",
      "Step 99000: Minibatch Loss: 0.004854, Minibatch Accuracy: 1.000000\n",
      "Step 100000: Minibatch Loss: 0.000003, Minibatch Accuracy: 1.000000\n",
      "Step 101000: Minibatch Loss: 0.156294, Minibatch Accuracy: 1.000000\n",
      "Step 102000: Minibatch Loss: 0.017217, Minibatch Accuracy: 1.000000\n",
      "Step 103000: Minibatch Loss: 0.753862, Minibatch Accuracy: 0.180000\n",
      "Step 104000: Minibatch Loss: 0.165736, Minibatch Accuracy: 1.000000\n",
      "Step 105000: Minibatch Loss: 0.073156, Minibatch Accuracy: 1.000000\n",
      "Step 106000: Minibatch Loss: 0.011656, Minibatch Accuracy: 1.000000\n",
      "Step 107000: Minibatch Loss: 0.034922, Minibatch Accuracy: 1.000000\n",
      "Step 108000: Minibatch Loss: 0.149064, Minibatch Accuracy: 1.000000\n",
      "Step 109000: Minibatch Loss: 0.000375, Minibatch Accuracy: 1.000000\n",
      "Step 110000: Minibatch Loss: 0.024244, Minibatch Accuracy: 1.000000\n",
      "Step 111000: Minibatch Loss: 0.011527, Minibatch Accuracy: 1.000000\n",
      "Step 112000: Minibatch Loss: 0.000414, Minibatch Accuracy: 1.000000\n",
      "Step 113000: Minibatch Loss: 0.000002, Minibatch Accuracy: 1.000000\n",
      "Step 114000: Minibatch Loss: 0.000001, Minibatch Accuracy: 1.000000\n",
      "Step 115000: Minibatch Loss: 0.053797, Minibatch Accuracy: 1.000000\n",
      "Step 116000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 117000: Minibatch Loss: 0.002056, Minibatch Accuracy: 1.000000\n",
      "Step 118000: Minibatch Loss: 0.032347, Minibatch Accuracy: 1.000000\n",
      "Step 119000: Minibatch Loss: 0.001355, Minibatch Accuracy: 1.000000\n",
      "Step 120000: Minibatch Loss: 0.003367, Minibatch Accuracy: 1.000000\n",
      "Step 121000: Minibatch Loss: 0.000006, Minibatch Accuracy: 1.000000\n",
      "Step 122000: Minibatch Loss: 0.000831, Minibatch Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 123000: Minibatch Loss: 0.001002, Minibatch Accuracy: 1.000000\n",
      "Step 124000: Minibatch Loss: 0.000062, Minibatch Accuracy: 1.000000\n",
      "Step 125000: Minibatch Loss: 0.002618, Minibatch Accuracy: 1.000000\n",
      "Step 126000: Minibatch Loss: 0.011584, Minibatch Accuracy: 1.000000\n",
      "Step 127000: Minibatch Loss: 0.010888, Minibatch Accuracy: 1.000000\n",
      "Step 128000: Minibatch Loss: 0.079964, Minibatch Accuracy: 1.000000\n",
      "Step 129000: Minibatch Loss: 0.000166, Minibatch Accuracy: 1.000000\n",
      "Step 130000: Minibatch Loss: 0.049913, Minibatch Accuracy: 1.000000\n",
      "Step 131000: Minibatch Loss: 0.058852, Minibatch Accuracy: 1.000000\n",
      "Step 132000: Minibatch Loss: 0.002936, Minibatch Accuracy: 1.000000\n",
      "Step 133000: Minibatch Loss: 0.006374, Minibatch Accuracy: 1.000000\n",
      "Step 134000: Minibatch Loss: 0.002901, Minibatch Accuracy: 1.000000\n",
      "Step 135000: Minibatch Loss: 0.017869, Minibatch Accuracy: 1.000000\n",
      "Step 136000: Minibatch Loss: 0.000000, Minibatch Accuracy: 1.000000\n",
      "Step 137000: Minibatch Loss: 0.001330, Minibatch Accuracy: 1.000000\n",
      "Step 138000: Minibatch Loss: 0.262618, Minibatch Accuracy: 1.000000\n",
      "Step 139000: Minibatch Loss: 0.006026, Minibatch Accuracy: 1.000000\n",
      "Step 140000: Minibatch Loss: 0.000034, Minibatch Accuracy: 1.000000\n",
      "Step 141000: Minibatch Loss: 0.003399, Minibatch Accuracy: 1.000000\n",
      "Step 142000: Minibatch Loss: 0.052610, Minibatch Accuracy: 1.000000\n",
      "Step 143000: Minibatch Loss: 0.000993, Minibatch Accuracy: 1.000000\n",
      "Step 144000: Minibatch Loss: 0.000435, Minibatch Accuracy: 1.000000\n",
      "Step 145000: Minibatch Loss: 0.004575, Minibatch Accuracy: 1.000000\n",
      "Step 146000: Minibatch Loss: 0.027066, Minibatch Accuracy: 1.000000\n",
      "Step 147000: Minibatch Loss: 0.000084, Minibatch Accuracy: 1.000000\n",
      "Step 148000: Minibatch Loss: 0.036433, Minibatch Accuracy: 1.000000\n",
      "Step 149000: Minibatch Loss: 0.000300, Minibatch Accuracy: 1.000000\n",
      "Step 150000: Minibatch Loss: 0.028283, Minibatch Accuracy: 1.000000\n",
      "Full batch Accuracy: 1.000000\n",
      "[[-0.0029575  -0.0061618   0.01606   ]\n",
      " [-0.00217697 -0.00640061  0.01525953]\n",
      " [-0.00267373 -0.00639711  0.0160347 ]\n",
      " ..., \n",
      " [-0.00185005 -0.00623959  0.01490259]\n",
      " [-0.0022924  -0.00593893  0.01521326]\n",
      " [-0.00224067 -0.00598083  0.01503668]]\n",
      "[[-0.00296708 -0.0061673   0.01608402]\n",
      " [-0.00218204 -0.00641234  0.01527925]\n",
      " [-0.00268143 -0.00640554  0.01605938]\n",
      " ..., \n",
      " [-0.00184767 -0.00624695  0.01491225]\n",
      " [-0.00229363 -0.00594266  0.01522631]\n",
      " [-0.00224548 -0.00598687  0.01505365]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range (150001):\n",
    "        Order = Sequence[step % 24]\n",
    "        batch_X = np.reshape(RealOESdata[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_InputUnit))\n",
    "        \n",
    "        ### Y_ClassifiedResult_Power -> Power에 대한 one hot vector\n",
    "        batch_Y = np.reshape(Y_ClassifiedResult_Press[Order*batchsize: (Order+1)*batchsize,:],\n",
    "                             (batchsize, n_OutputUnit))\n",
    "        session.run(train, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 0.5})\n",
    "        MinibatchCost = session.run(cost, feed_dict = {X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={X: batch_X, Y: batch_Y, keep_prob: 1.0})\n",
    "            print ('Step %i: Minibatch Loss: %f, Minibatch Accuracy: %f' % (step, MinibatchCost,\n",
    "                                                                            train_accuracy))\n",
    "            \n",
    "        if step % 24 == 0:\n",
    "            random.shuffle(Sequence)\n",
    "    \n",
    "    RealOESdata_Reshaped = np.reshape(RealOESdata, (1200, 3648))\n",
    "    \n",
    "    ###Y_ClassifiedResult_Power\n",
    "    train_accuracy = accuracy.eval(feed_dict = {X: RealOESdata_Reshaped, Y: Y_ClassifiedResult_Press, keep_prob: 1.0})        \n",
    "    print('Full batch Accuracy: %f' % (train_accuracy))\n",
    "    \n",
    "    PredictedSimplifiedInputData = session.run(logits, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputData)\n",
    "    np.savetxt('Press_Weight.txt', PredictedSimplifiedInputData, delimiter='\\t')\n",
    "    \n",
    "    PredictedSimplifiedInputDataLinear = session.run(WL4, feed_dict = {X : SimplifiedInput,keep_prob: 1.0})\n",
    "    print(PredictedSimplifiedInputDataLinear)\n",
    "    np.savetxt('Press_Weight2.txt', PredictedSimplifiedInputDataLinear, delimiter='\\t')\n",
    "    \n",
    "    #for i in range(24):\n",
    "    #    Result_batch_X = np.reshape(RealOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                (batchsize, n_InputUnit))\n",
    "    #    PredictedBatchData = session.run(prediction, feed_dict = {X : Result_batch_X, keep_prob: 1.0})\n",
    "    #    \n",
    "    #    ### Y_ClassifiedResult_Power\n",
    "    #    Y_ClassifiedResult_Reshaped = np.reshape(Y_ClassifiedResult_Press[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                             (batchsize, n_OutputUnit))\n",
    "    #    \n",
    "    #    if i % 1 == 0:\n",
    "    #        print(\"Original Data\")\n",
    "    #        print(Y_ClassifiedResult_Reshaped[0,:])\n",
    "    #    \n",
    "    #    print(i)\n",
    "    #    print(\"Result of predicting EntireData\")\n",
    "    #    print(PredictedBatchData)\n",
    "    #        \n",
    "    #print(\"####################################################\")  \n",
    "    #\n",
    "    #for i in range(24):\n",
    "    #    #\n",
    "    #    Test_batch_X = np.reshape(TestOESdata[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                (batchsize, n_InputUnit))\n",
    "    #    \n",
    "    #    \n",
    "    #    PredictedTestBatchData = session.run(prediction, feed_dict = {X : Test_batch_X, keep_prob: 1.0})\n",
    "    #    \n",
    "    #    ###Y_Test_Power -> Test data의 power에 대한 one hot vector\n",
    "    #    #\n",
    "    #    Y_Test_Reshaped = np.reshape(Y_Test_Press[i*batchsize: (i+1)*batchsize,:],\n",
    "    #                                             (batchsize, n_OutputUnit))\n",
    "    #    \n",
    "    #    if i % 1 == 0:\n",
    "    #        print(\"Original Data\")\n",
    "    #        print(Y_Test_Reshaped[0,:])\n",
    "    #    \n",
    "    #    print(i)\n",
    "    #    print(\"Result of Predicted Test Data\")\n",
    "    #    print(PredictedTestBatchData)\n",
    "    ##\n",
    "    #TestOESdata_Reshaped = np.reshape(TestOESdata, (1200,3648))\n",
    "    #\n",
    "    ####Y_Test_Power\n",
    "    ##\n",
    "    #train_accuracy = accuracy.eval(feed_dict = {X: TestOESdata_Reshaped, Y: Y_Test_Press, keep_prob: 1.0})        \n",
    "    #print('Test Full batch Accuracy: %f' % (train_accuracy))          \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 503,
   "position": {
    "height": "40px",
    "left": "103.797px",
    "right": "20px",
    "top": "128px",
    "width": "707.594px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
