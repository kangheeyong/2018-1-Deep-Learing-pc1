{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Power Pressure Gas1 Gas2\n",
    "data = pd.read_excel('2nd.xlsx') # MLMH\n",
    "data = data.values\n",
    "RealOESdata = data[151:251,1:]\n",
    "data = pd.read_excel('4th.xlsx') # LMHM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('6th.xlsx') # HMMH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('8th.xlsx') # HMMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('10th.xlsx') # MLLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('12th.xlsx') # LMML\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('14th.xlsx') # MLHM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('16th.xlsx') # HMML\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('17th.xlsx') # LMLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('21th.xlsx') # HMHM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('23th.xlsx') # MLML\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('24th.xlsx') # HMLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('25th.xlsx') # LMMH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "####\n",
    "data = pd.read_excel('1st.xlsx') # MMHH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('7th.xlsx') # MMLH\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('15th.xlsx') # LHMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('22th.xlsx') # MHLM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "data = pd.read_excel('26th.xlsx') # MMMM\n",
    "data = data.values\n",
    "RealOESdata = np.vstack((RealOESdata, data[151:251,1:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN_classifier(x):\n",
    "  # 입력 이미지\n",
    "  x_image = x\n",
    "\n",
    "  # 첫번째 convolutional layer - 하나의 grayscale 이미지를 64개의 특징들(feature)으로 맵핑(maping)합니다.\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 64], stddev=5e-2))\n",
    "  b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "  h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "\n",
    "  # 첫번째 Pooling layer\n",
    "  h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  # 두번째 convolutional layer - 32개의 특징들(feature)을 64개의 특징들(feature)로 맵핑(maping)합니다.\n",
    "  W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
    "  b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "  h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "\n",
    "  # 두번째 pooling layer.\n",
    "  h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  # 세번째 convolutional layer\n",
    "  W_conv3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "  b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "  h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "\n",
    "  # 네번째 convolutional layer\n",
    "  W_conv4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "  b_conv4 = tf.Variable(tf.constant(0.1, shape=[128])) \n",
    "  h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4)\n",
    "\n",
    "  # 다섯번째 convolutional layer\n",
    "  W_conv5 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "  b_conv5 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "  h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5)\n",
    "\n",
    "  # Fully Connected Layer 1 - 2번의 downsampling 이후에, 우리의 32x32 이미지는 8x8x128 특징맵(feature map)이 됩니다.\n",
    "  # 이를 384개의 특징들로 맵핑(maping)합니다.\n",
    "  W_fc1 = tf.Variable(tf.truncated_normal(shape=[8 * 8 * 128, 384], stddev=5e-2))\n",
    "  b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "\n",
    "  h_conv5_flat = tf.reshape(h_conv5, [-1, 8*8*128])\n",
    "  h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout - 모델의 복잡도를 컨트롤합니다. 특징들의 co-adaptation을 방지합니다.\n",
    "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
    "\n",
    "  # Fully Connected Layer 2 - 384개의 특징들(feature)을 10개의 클래스-airplane, automobile, bird...-로 맵핑(maping)합니다.\n",
    "  W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
    "  b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "  logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "  y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "  return y_pred, logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
