{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum DCGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_DCGANs_for_FD'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     80
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def G(x,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "\n",
    "        conv1 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "    r5= tf.add(conv5, 0 ,name=name)#1*1*100\n",
    "  \n",
    "    return r5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_epoch = 100\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.001\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,name='G_sample') # G(z)\n",
    "E_z = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z, isTrain, reuse=True, name ='re_image')\n",
    "re_z = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "E_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z-z)**2, axis=[1,2,3])) , name = 're_z_loss')             \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "    \n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss, var_list=G_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -56.582, D_real_e : 48.751, D_fake_e : 33.499, G_e : 33.865, E_e : 5.256, new_measure : 60.468, k_curr : 0.731252\n",
      "D_e : 28.847, D_real_e : 38.333, D_fake_e : 26.425, G_e : 26.954, E_e : 4.318, new_measure : 39.740, k_curr : 0.392786\n",
      "D_e : 27.196, D_real_e : 33.044, D_fake_e : 22.697, G_e : 23.209, E_e : 3.825, new_measure : 34.311, k_curr : 0.172733\n",
      "D_e : 25.190, D_real_e : 28.195, D_fake_e : 19.455, G_e : 19.794, E_e : 3.535, new_measure : 29.551, k_curr : 0.012580\n",
      "D_e : 22.304, D_real_e : 24.196, D_fake_e : 16.731, G_e : 16.937, E_e : 3.360, new_measure : 25.718, k_curr : 0.013312\n",
      "D_e : 19.593, D_real_e : 20.989, D_fake_e : 14.518, G_e : 14.664, E_e : 3.250, new_measure : 22.276, k_curr : 0.090365\n",
      "D_e : 17.992, D_real_e : 18.137, D_fake_e : 12.667, G_e : 12.747, E_e : 3.196, new_measure : 19.586, k_curr : -0.052919\n",
      "D_e : 15.776, D_real_e : 15.790, D_fake_e : 11.003, G_e : 11.034, E_e : 3.180, new_measure : 16.959, k_curr : 0.002004\n",
      "D_e : 13.606, D_real_e : 13.968, D_fake_e : 9.683, G_e : 9.743, E_e : 3.166, new_measure : 14.991, k_curr : 0.096957\n",
      "D_e : 12.326, D_real_e : 12.129, D_fake_e : 8.491, G_e : 8.506, E_e : 3.166, new_measure : 13.127, k_curr : 0.053957\n",
      "D_e : 11.018, D_real_e : 11.128, D_fake_e : 7.744, G_e : 7.790, E_e : 3.166, new_measure : 11.867, k_curr : 0.053761\n",
      "D_e : 9.990, D_real_e : 10.087, D_fake_e : 7.010, G_e : 7.063, E_e : 3.167, new_measure : 10.689, k_curr : 0.049653\n",
      "D_e : 9.301, D_real_e : 9.475, D_fake_e : 6.584, G_e : 6.645, E_e : 3.168, new_measure : 10.048, k_curr : 0.013955\n",
      "D_e : 8.756, D_real_e : 8.837, D_fake_e : 6.122, G_e : 6.189, E_e : 3.178, new_measure : 9.386, k_curr : 0.004871\n",
      "D_e : 8.231, D_real_e : 8.381, D_fake_e : 5.805, G_e : 5.868, E_e : 3.185, new_measure : 8.786, k_curr : 0.000825\n",
      "D_e : 7.799, D_real_e : 7.826, D_fake_e : 5.433, G_e : 5.498, E_e : 3.199, new_measure : 8.258, k_curr : -0.055482\n",
      "D_e : 7.411, D_real_e : 7.565, D_fake_e : 5.194, G_e : 5.272, E_e : 3.213, new_measure : 7.894, k_curr : 0.009159\n",
      "D_e : 6.830, D_real_e : 6.906, D_fake_e : 4.761, G_e : 4.834, E_e : 3.225, new_measure : 7.139, k_curr : 0.011072\n",
      "D_e : 6.254, D_real_e : 6.348, D_fake_e : 4.377, G_e : 4.450, E_e : 3.243, new_measure : 6.564, k_curr : -0.006481\n",
      "D_e : 5.956, D_real_e : 6.028, D_fake_e : 4.134, G_e : 4.203, E_e : 3.264, new_measure : 6.247, k_curr : 0.040141\n",
      "D_e : 5.777, D_real_e : 5.878, D_fake_e : 4.059, G_e : 4.124, E_e : 3.289, new_measure : 6.079, k_curr : 0.012761\n",
      "D_e : 5.637, D_real_e : 5.732, D_fake_e : 3.922, G_e : 4.009, E_e : 3.315, new_measure : 5.941, k_curr : 0.020878\n",
      "D_e : 5.520, D_real_e : 5.623, D_fake_e : 3.865, G_e : 3.934, E_e : 3.345, new_measure : 5.838, k_curr : 0.026263\n",
      "D_e : 5.396, D_real_e : 5.527, D_fake_e : 3.795, G_e : 3.868, E_e : 3.373, new_measure : 5.734, k_curr : 0.028794\n",
      "D_e : 5.319, D_real_e : 5.425, D_fake_e : 3.729, G_e : 3.807, E_e : 3.405, new_measure : 5.648, k_curr : 0.003250\n",
      "D_e : 5.248, D_real_e : 5.361, D_fake_e : 3.653, G_e : 3.746, E_e : 3.436, new_measure : 5.569, k_curr : 0.021368\n",
      "D_e : 5.097, D_real_e : 5.236, D_fake_e : 3.586, G_e : 3.665, E_e : 3.465, new_measure : 5.426, k_curr : 0.022453\n",
      "D_e : 5.080, D_real_e : 5.193, D_fake_e : 3.550, G_e : 3.628, E_e : 3.495, new_measure : 5.413, k_curr : 0.041431\n",
      "D_e : 5.008, D_real_e : 5.152, D_fake_e : 3.537, G_e : 3.617, E_e : 3.527, new_measure : 5.351, k_curr : 0.010730\n",
      "D_e : 4.970, D_real_e : 5.085, D_fake_e : 3.461, G_e : 3.556, E_e : 3.556, new_measure : 5.284, k_curr : 0.022655\n",
      "D_e : 4.896, D_real_e : 5.023, D_fake_e : 3.420, G_e : 3.515, E_e : 3.585, new_measure : 5.216, k_curr : 0.024847\n",
      "D_e : 4.840, D_real_e : 4.954, D_fake_e : 3.372, G_e : 3.458, E_e : 3.615, new_measure : 5.145, k_curr : 0.050479\n",
      "D_e : 4.782, D_real_e : 4.913, D_fake_e : 3.362, G_e : 3.448, E_e : 3.642, new_measure : 5.112, k_curr : 0.027430\n",
      "D_e : 4.732, D_real_e : 4.863, D_fake_e : 3.341, G_e : 3.412, E_e : 3.669, new_measure : 5.027, k_curr : 0.007046\n",
      "D_e : 4.727, D_real_e : 4.812, D_fake_e : 3.290, G_e : 3.361, E_e : 3.697, new_measure : 4.975, k_curr : 0.028299\n",
      "D_e : 4.642, D_real_e : 4.778, D_fake_e : 3.268, G_e : 3.348, E_e : 3.721, new_measure : 4.942, k_curr : 0.020047\n",
      "D_e : 4.614, D_real_e : 4.751, D_fake_e : 3.230, G_e : 3.316, E_e : 3.748, new_measure : 4.913, k_curr : 0.046008\n",
      "D_e : 4.578, D_real_e : 4.699, D_fake_e : 3.201, G_e : 3.289, E_e : 3.772, new_measure : 4.878, k_curr : 0.047812\n",
      "D_e : 4.526, D_real_e : 4.643, D_fake_e : 3.183, G_e : 3.258, E_e : 3.795, new_measure : 4.817, k_curr : 0.027403\n",
      "D_e : 4.519, D_real_e : 4.634, D_fake_e : 3.151, G_e : 3.238, E_e : 3.818, new_measure : 4.803, k_curr : 0.043572\n",
      "D_e : 4.470, D_real_e : 4.586, D_fake_e : 3.122, G_e : 3.211, E_e : 3.839, new_measure : 4.754, k_curr : 0.042502\n",
      "D_e : 4.453, D_real_e : 4.560, D_fake_e : 3.105, G_e : 3.187, E_e : 3.860, new_measure : 4.717, k_curr : 0.057633\n",
      "D_e : 4.392, D_real_e : 4.526, D_fake_e : 3.091, G_e : 3.180, E_e : 3.881, new_measure : 4.698, k_curr : 0.025472\n",
      "D_e : 4.390, D_real_e : 4.506, D_fake_e : 3.063, G_e : 3.145, E_e : 3.900, new_measure : 4.665, k_curr : 0.050322\n",
      "D_e : 4.351, D_real_e : 4.470, D_fake_e : 3.055, G_e : 3.134, E_e : 3.919, new_measure : 4.643, k_curr : 0.035377\n",
      "D_e : 4.319, D_real_e : 4.430, D_fake_e : 3.021, G_e : 3.100, E_e : 3.939, new_measure : 4.587, k_curr : 0.036841\n",
      "D_e : 4.308, D_real_e : 4.425, D_fake_e : 3.016, G_e : 3.097, E_e : 3.959, new_measure : 4.597, k_curr : 0.039988\n",
      "D_e : 4.286, D_real_e : 4.410, D_fake_e : 3.000, G_e : 3.091, E_e : 3.977, new_measure : 4.566, k_curr : 0.028193\n",
      "D_e : 4.258, D_real_e : 4.371, D_fake_e : 2.978, G_e : 3.060, E_e : 3.994, new_measure : 4.524, k_curr : 0.026117\n",
      "D_e : 4.229, D_real_e : 4.346, D_fake_e : 2.951, G_e : 3.036, E_e : 4.012, new_measure : 4.507, k_curr : 0.043769\n",
      "D_e : 4.211, D_real_e : 4.314, D_fake_e : 2.932, G_e : 3.018, E_e : 4.028, new_measure : 4.474, k_curr : 0.049324\n",
      "D_e : 4.186, D_real_e : 4.321, D_fake_e : 2.938, G_e : 3.024, E_e : 4.043, new_measure : 4.470, k_curr : 0.051565\n",
      "D_e : 4.143, D_real_e : 4.265, D_fake_e : 2.915, G_e : 2.993, E_e : 4.058, new_measure : 4.408, k_curr : 0.030614\n",
      "D_e : 4.160, D_real_e : 4.252, D_fake_e : 2.891, G_e : 2.973, E_e : 4.074, new_measure : 4.391, k_curr : 0.039109\n",
      "D_e : 4.128, D_real_e : 4.244, D_fake_e : 2.879, G_e : 2.966, E_e : 4.090, new_measure : 4.399, k_curr : 0.051369\n",
      "D_e : 4.103, D_real_e : 4.225, D_fake_e : 2.875, G_e : 2.966, E_e : 4.104, new_measure : 4.381, k_curr : 0.027407\n",
      "D_e : 4.082, D_real_e : 4.196, D_fake_e : 2.846, G_e : 2.933, E_e : 4.117, new_measure : 4.345, k_curr : 0.041150\n",
      "D_e : 4.058, D_real_e : 4.179, D_fake_e : 2.841, G_e : 2.923, E_e : 4.130, new_measure : 4.336, k_curr : 0.046014\n",
      "D_e : 4.046, D_real_e : 4.166, D_fake_e : 2.828, G_e : 2.914, E_e : 4.143, new_measure : 4.320, k_curr : 0.053704\n",
      "D_e : 4.015, D_real_e : 4.127, D_fake_e : 2.811, G_e : 2.891, E_e : 4.157, new_measure : 4.269, k_curr : 0.049358\n",
      "D_e : 3.995, D_real_e : 4.135, D_fake_e : 2.806, G_e : 2.893, E_e : 4.169, new_measure : 4.283, k_curr : 0.052369\n",
      "D_e : 4.012, D_real_e : 4.111, D_fake_e : 2.801, G_e : 2.882, E_e : 4.180, new_measure : 4.250, k_curr : 0.040413\n",
      "D_e : 3.957, D_real_e : 4.091, D_fake_e : 2.780, G_e : 2.859, E_e : 4.192, new_measure : 4.228, k_curr : 0.052070\n",
      "D_e : 3.972, D_real_e : 4.088, D_fake_e : 2.781, G_e : 2.866, E_e : 4.203, new_measure : 4.225, k_curr : 0.039546\n",
      "D_e : 3.940, D_real_e : 4.066, D_fake_e : 2.762, G_e : 2.837, E_e : 4.214, new_measure : 4.204, k_curr : 0.064658\n",
      "D_e : 3.940, D_real_e : 4.047, D_fake_e : 2.758, G_e : 2.842, E_e : 4.224, new_measure : 4.193, k_curr : 0.037529\n",
      "D_e : 3.910, D_real_e : 4.033, D_fake_e : 2.738, G_e : 2.817, E_e : 4.234, new_measure : 4.178, k_curr : 0.055062\n",
      "D_e : 3.917, D_real_e : 4.024, D_fake_e : 2.746, G_e : 2.820, E_e : 4.244, new_measure : 4.170, k_curr : 0.046187\n",
      "D_e : 3.869, D_real_e : 3.986, D_fake_e : 2.708, G_e : 2.785, E_e : 4.255, new_measure : 4.121, k_curr : 0.060960\n",
      "D_e : 3.866, D_real_e : 3.989, D_fake_e : 2.718, G_e : 2.795, E_e : 4.264, new_measure : 4.117, k_curr : 0.055279\n",
      "D_e : 3.849, D_real_e : 3.972, D_fake_e : 2.711, G_e : 2.790, E_e : 4.273, new_measure : 4.107, k_curr : 0.029187\n",
      "D_e : 3.856, D_real_e : 3.961, D_fake_e : 2.688, G_e : 2.767, E_e : 4.282, new_measure : 4.091, k_curr : 0.044836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 3.833, D_real_e : 3.928, D_fake_e : 2.677, G_e : 2.751, E_e : 4.291, new_measure : 4.069, k_curr : 0.041172\n",
      "D_e : 3.804, D_real_e : 3.937, D_fake_e : 2.675, G_e : 2.751, E_e : 4.300, new_measure : 4.073, k_curr : 0.054822\n",
      "D_e : 3.799, D_real_e : 3.906, D_fake_e : 2.661, G_e : 2.736, E_e : 4.309, new_measure : 4.041, k_curr : 0.048634\n",
      "D_e : 3.795, D_real_e : 3.906, D_fake_e : 2.648, G_e : 2.728, E_e : 4.316, new_measure : 4.051, k_curr : 0.066296\n",
      "D_e : 3.771, D_real_e : 3.898, D_fake_e : 2.656, G_e : 2.735, E_e : 4.324, new_measure : 4.033, k_curr : 0.047661\n",
      "D_e : 3.766, D_real_e : 3.877, D_fake_e : 2.636, G_e : 2.712, E_e : 4.331, new_measure : 4.015, k_curr : 0.052343\n",
      "D_e : 3.755, D_real_e : 3.875, D_fake_e : 2.637, G_e : 2.715, E_e : 4.339, new_measure : 4.006, k_curr : 0.044974\n",
      "D_e : 3.742, D_real_e : 3.841, D_fake_e : 2.621, G_e : 2.691, E_e : 4.346, new_measure : 3.976, k_curr : 0.038563\n",
      "D_e : 3.731, D_real_e : 3.841, D_fake_e : 2.616, G_e : 2.686, E_e : 4.353, new_measure : 3.972, k_curr : 0.045595\n",
      "D_e : 3.709, D_real_e : 3.823, D_fake_e : 2.612, G_e : 2.680, E_e : 4.360, new_measure : 3.959, k_curr : 0.036539\n",
      "D_e : 3.744, D_real_e : 3.823, D_fake_e : 2.601, G_e : 2.682, E_e : 4.366, new_measure : 3.960, k_curr : 0.019142\n",
      "D_e : 3.696, D_real_e : 3.795, D_fake_e : 2.572, G_e : 2.643, E_e : 4.373, new_measure : 3.937, k_curr : 0.057468\n",
      "D_e : 3.715, D_real_e : 3.824, D_fake_e : 2.606, G_e : 2.686, E_e : 4.379, new_measure : 3.968, k_curr : 0.031890\n",
      "D_e : 3.688, D_real_e : 3.784, D_fake_e : 2.580, G_e : 2.652, E_e : 4.385, new_measure : 3.907, k_curr : 0.023902\n",
      "D_e : 3.661, D_real_e : 3.783, D_fake_e : 2.567, G_e : 2.640, E_e : 4.391, new_measure : 3.904, k_curr : 0.045324\n",
      "D_e : 3.686, D_real_e : 3.767, D_fake_e : 2.570, G_e : 2.641, E_e : 4.397, new_measure : 3.901, k_curr : 0.033847\n",
      "D_e : 3.652, D_real_e : 3.747, D_fake_e : 2.553, G_e : 2.620, E_e : 4.404, new_measure : 3.880, k_curr : 0.042136\n",
      "D_e : 3.619, D_real_e : 3.738, D_fake_e : 2.547, G_e : 2.613, E_e : 4.410, new_measure : 3.858, k_curr : 0.053816\n",
      "D_e : 3.622, D_real_e : 3.720, D_fake_e : 2.544, G_e : 2.612, E_e : 4.415, new_measure : 3.840, k_curr : 0.030660\n",
      "D_e : 3.639, D_real_e : 3.724, D_fake_e : 2.538, G_e : 2.606, E_e : 4.421, new_measure : 3.851, k_curr : 0.033134\n",
      "D_e : 3.614, D_real_e : 3.706, D_fake_e : 2.525, G_e : 2.592, E_e : 4.426, new_measure : 3.826, k_curr : 0.039411\n",
      "D_e : 3.572, D_real_e : 3.694, D_fake_e : 2.514, G_e : 2.583, E_e : 4.432, new_measure : 3.814, k_curr : 0.046896\n",
      "D_e : 3.584, D_real_e : 3.686, D_fake_e : 2.521, G_e : 2.586, E_e : 4.437, new_measure : 3.809, k_curr : 0.031315\n",
      "D_e : 3.580, D_real_e : 3.683, D_fake_e : 2.506, G_e : 2.574, E_e : 4.442, new_measure : 3.810, k_curr : 0.042400\n",
      "D_e : 3.544, D_real_e : 3.653, D_fake_e : 2.491, G_e : 2.558, E_e : 4.448, new_measure : 3.785, k_curr : 0.039238\n",
      "D_e : 3.540, D_real_e : 3.655, D_fake_e : 2.490, G_e : 2.552, E_e : 4.452, new_measure : 3.775, k_curr : 0.057714\n",
      "D_e : 3.565, D_real_e : 3.651, D_fake_e : 2.487, G_e : 2.559, E_e : 4.457, new_measure : 3.781, k_curr : 0.048700\n",
      "D_e : 3.526, D_real_e : 3.644, D_fake_e : 2.483, G_e : 2.551, E_e : 4.462, new_measure : 3.772, k_curr : 0.049478\n",
      "total time :  14619.475571393967\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5115057320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            \n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e = sess.run([G_optim, G_loss], {u : u_, z : z_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            _ , E_e = sess.run([E_optim, E_loss], {u : u_, z : z_, isTrain : True})\n",
    "            E_error.append(E_e)\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, E_e : %.3f, new_measure : %.3f, k_curr : %3f'%(np.mean(D_error), np.mean(D_real_error),\n",
    "            np.mean(D_fake_error), np.mean(G_error),np.mean(E_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        r = sess.run([re_image],feed_dict={u : test_normal_data[0:16],isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/origin_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        r = sess.run([re_image],feed_dict={u : test_anomalous_data[0:16],isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/anomlous_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "\n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_DCGANs_for_FD/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 're_z:0' shape=(?, 1, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20418821334838866,\n",
       " 0.41657548522949212,\n",
       " 0.63495497894287101,\n",
       " 0.85688383865356432,\n",
       " 1.0822341842651366,\n",
       " 1.3105659408569335,\n",
       " 1.5410815200805663,\n",
       " 1.774405490875244,\n",
       " 2.0082518997192382,\n",
       " 2.243751438140869,\n",
       " 2.4813708801269527,\n",
       " 2.7184026527404779,\n",
       " 2.9545988807678216,\n",
       " 3.1913778533935542,\n",
       " 3.427158195495605,\n",
       " 3.662103481292724,\n",
       " 3.8943979301452631,\n",
       " 4.1270841407775869,\n",
       " 4.3564274749755851,\n",
       " 4.5840513610839837,\n",
       " 4.8113302116394037,\n",
       " 5.036336277008056,\n",
       " 5.2575814743041986,\n",
       " 5.477877780914306,\n",
       " 5.695012603759765,\n",
       " 5.9102202377319326,\n",
       " 6.1223028335571277,\n",
       " 6.3327925071716296,\n",
       " 6.5405541687011706,\n",
       " 6.7460858421325671,\n",
       " 6.948739028930663,\n",
       " 7.1502753295898422,\n",
       " 7.3492940177917463,\n",
       " 7.5440102806091289,\n",
       " 7.7334418296813947,\n",
       " 7.9202415313720689,\n",
       " 8.1009004364013659,\n",
       " 8.2719703445434565,\n",
       " 8.4337904090881342,\n",
       " 8.586725189208984,\n",
       " 8.7258839187622073,\n",
       " 8.8421368141174312,\n",
       " 8.9423484268188478,\n",
       " 9.0201709251403805,\n",
       " 9.0818690567016596,\n",
       " 9.1406732711791978,\n",
       " 9.1874560470581041,\n",
       " 9.2198155593872055,\n",
       " 9.2455931015014627,\n",
       " 9.3114149856567359,\n",
       " 9.3726372032165504,\n",
       " 9.4753679046630843,\n",
       " 9.5807317199707018,\n",
       " 9.6792591514587389,\n",
       " 9.7699057426452622,\n",
       " 9.8279220619201642,\n",
       " 9.8664191513061503,\n",
       " 9.891658851623534,\n",
       " 9.8996595916748031,\n",
       " 9.9024259529113756,\n",
       " 9.8850524406433085,\n",
       " 9.8901615867614723,\n",
       " 9.9155646781921369,\n",
       " 9.9210947723388649,\n",
       " 9.92752141189575,\n",
       " 9.9339790992736798,\n",
       " 9.9380985145568825,\n",
       " 9.9442337226867661,\n",
       " 9.9282491798400869,\n",
       " 9.911284923553465,\n",
       " 9.9021786842346167,\n",
       " 9.8839864540100066,\n",
       " 9.8501895332336389,\n",
       " 9.7903535423278765,\n",
       " 9.7411029357910106,\n",
       " 9.6910626640319766,\n",
       " 9.6306181831359812,\n",
       " 9.5685731353759707,\n",
       " 9.4908322448730402,\n",
       " 9.416720394134515,\n",
       " 9.3344023933410583,\n",
       " 9.2241733970642024,\n",
       " 9.1443144226074153,\n",
       " 9.0649921379089289,\n",
       " 8.961825012207024,\n",
       " 8.8761627311706466,\n",
       " 8.7690394897460866,\n",
       " 8.6559181098937916,\n",
       " 8.54239770126342,\n",
       " 8.4362246437072681,\n",
       " 8.3264057350158627,\n",
       " 8.2061943283080989,\n",
       " 8.0818633460998477,\n",
       " 7.9532522010803168,\n",
       " 7.8271343078613222,\n",
       " 7.704357105255121,\n",
       " 7.5716123580932555,\n",
       " 7.4566668739318782,\n",
       " 7.3118969192504819,\n",
       " 7.1699589271545348,\n",
       " 7.0275238723754816,\n",
       " 6.8988085365295344,\n",
       " 6.7647976837158135,\n",
       " 6.6293998146057058,\n",
       " 6.4805969886779717,\n",
       " 6.3434105911254814,\n",
       " 6.2015474052429127,\n",
       " 6.0524332160949639,\n",
       " 5.9159874916076589,\n",
       " 5.7550633621215752,\n",
       " 5.6175856056213309,\n",
       " 5.4566946296691823,\n",
       " 5.3117588424682545,\n",
       " 5.1790628662109306,\n",
       " 5.0261431388854909,\n",
       " 4.8813367843627855,\n",
       " 4.7333171157836835,\n",
       " 4.5936102523803628,\n",
       " 4.4274946098327552,\n",
       " 4.2803649291992105,\n",
       " 4.1076599578857342,\n",
       " 3.9458328132629314,\n",
       " 3.7675393486022868,\n",
       " 3.601994083404533,\n",
       " 3.4394880485534589,\n",
       " 3.2649053001403732,\n",
       " 3.121181221008293,\n",
       " 2.9667019157409591,\n",
       " 2.7983254203796308,\n",
       " 2.6361308479309002,\n",
       " 2.4576506347656171,\n",
       " 2.2790140724182049,\n",
       " 2.1014618339538496,\n",
       " 1.927264087676994,\n",
       " 1.7537160911559979,\n",
       " 1.5763248710632245,\n",
       " 1.4129160575866619,\n",
       " 1.2360551223754803,\n",
       " 1.0623457221984782,\n",
       " 0.89105618286132005,\n",
       " 0.72220571136473799,\n",
       " 0.53475381469725747,\n",
       " 0.36260216522215977,\n",
       " 0.19984670639037261,\n",
       " 0.032626766204825713,\n",
       " -0.14834608078003758,\n",
       " -0.31974268341065282,\n",
       " -0.49605044937134624,\n",
       " -0.67490471649170758,\n",
       " -0.84912420272827993,\n",
       " -1.0161123085022057,\n",
       " -1.1717975120544519,\n",
       " -1.3305548896789636,\n",
       " -1.5068723754882898,\n",
       " -1.6726846466064538,\n",
       " -1.8459172592163171,\n",
       " -2.0083527717590419,\n",
       " -2.1631573295593349,\n",
       " -2.3062683563232511,\n",
       " -2.4598379287719814,\n",
       " -2.5840773887634363,\n",
       " -2.718962291717538,\n",
       " -2.8537300758361903,\n",
       " -2.9825354690551844,\n",
       " -3.0611118431091393,\n",
       " -3.1720744438171473,\n",
       " -3.275888938903817,\n",
       " -3.3887133293151939,\n",
       " -3.4692095413208093,\n",
       " -3.5702136535644615,\n",
       " -3.636929351806649,\n",
       " -3.6940299110412682,\n",
       " -3.683191242218026,\n",
       " -3.7515580940246664,\n",
       " -3.7946958541870202,\n",
       " -3.774684753417977,\n",
       " -3.7626694221496666,\n",
       " -3.7632555465698325,\n",
       " -3.741790138244637,\n",
       " -3.7425574340820393,\n",
       " -3.756772327423104,\n",
       " -3.7379169654846276,\n",
       " -3.6870974388122644,\n",
       " -3.6309960746765224,\n",
       " -3.5865881958007901,\n",
       " -3.5058313064575284,\n",
       " -3.4225810737609952,\n",
       " -3.350933349609384,\n",
       " -3.2502475090026945,\n",
       " -3.1236204414367768,\n",
       " -3.0266601753234954,\n",
       " -2.9091808280944913,\n",
       " -2.7805900382995694,\n",
       " -2.6417879333496184,\n",
       " -2.4834801559448332,\n",
       " -2.333660343170175,\n",
       " -2.1756563739776702,\n",
       " -2.0070753135681243,\n",
       " -1.9409722023010345,\n",
       " -1.7965513057708831,\n",
       " -1.7307838382721039,\n",
       " -1.6381763973236176,\n",
       " -1.5184404850006197,\n",
       " -1.396976652145395,\n",
       " -1.3113284130096527,\n",
       " -1.1820578441619964,\n",
       " -1.0222191905975433,\n",
       " -0.86224706840516052,\n",
       " -0.70367963218689877,\n",
       " -0.54487726020813909,\n",
       " -0.37416874885560003,\n",
       " -0.20505273628235784,\n",
       " -0.040477041244516065,\n",
       " 0.13276807975768118,\n",
       " 0.31975850486754442,\n",
       " 0.50793316841124558,\n",
       " 0.65708546638487841,\n",
       " 0.83409980010985396,\n",
       " 1.0011070709228422,\n",
       " 1.1516801414489652,\n",
       " 1.3233267765045071,\n",
       " 1.472533418655386,\n",
       " 1.6172856693267728,\n",
       " 1.7459210376739407,\n",
       " 1.6867914485931301,\n",
       " 1.6905364284515285,\n",
       " 1.6391296901702785,\n",
       " 1.5987682132720851,\n",
       " 1.5408365383148097,\n",
       " 1.4733560276031397,\n",
       " 1.4304904193878076,\n",
       " 1.3567602100372218,\n",
       " 1.2826788883209133,\n",
       " 1.2020915050506495,\n",
       " 1.1126696872711086,\n",
       " 1.0264461307525539,\n",
       " 0.9318556194305323,\n",
       " 0.84603326606749518,\n",
       " 0.72873553276061043,\n",
       " 0.63969460868834482,\n",
       " 0.56058530998229006,\n",
       " 0.43981299018858888,\n",
       " 0.3753405857086084,\n",
       " 0.29893025016783692,\n",
       " 0.20974228477477047,\n",
       " 0.11060358619688958,\n",
       " 0.041354463577260664,\n",
       " -0.085358716964731574,\n",
       " -0.19344844627381363,\n",
       " -0.2700769481659035,\n",
       " -0.36079453849793475,\n",
       " -0.39644328880311053,\n",
       " -0.40793923759461442,\n",
       " -0.41133875846863788,\n",
       " -0.46289515113831564,\n",
       " -0.50532133674622581,\n",
       " -0.50595585823060085,\n",
       " -0.50190633201600121,\n",
       " -0.49160880851746608,\n",
       " -0.45255659675599147,\n",
       " -0.41446186256409695,\n",
       " -0.36157951545716338,\n",
       " -0.32080002784730011,\n",
       " -0.24544713401795443,\n",
       " -0.19040881156922396,\n",
       " -0.10344345283509312,\n",
       " -0.052715883255015007,\n",
       " 0.0073538036346334212,\n",
       " 0.013321619033803314,\n",
       " 0.078355768203725168,\n",
       " 0.15575746345518998,\n",
       " 0.22400527381895949,\n",
       " 0.32768002128600049,\n",
       " 0.4228380794525044,\n",
       " 0.5000814456939594,\n",
       " 0.53385532951353942,\n",
       " 0.58850466346739683,\n",
       " 0.66424447441100032,\n",
       " 0.70363650321959403,\n",
       " 0.73125247764586354,\n",
       " 0.73653311729430104,\n",
       " 0.76001486015318775,\n",
       " 0.73533989143370526,\n",
       " 0.77333466148375407,\n",
       " 0.78192587089537513,\n",
       " 0.75065377616881257,\n",
       " 0.74145007514952543,\n",
       " 0.72057356834410546,\n",
       " 0.66109305000304097,\n",
       " 0.63210244178770891,\n",
       " 0.60307095146178114,\n",
       " 0.53082293891905652,\n",
       " 0.45220491600035528,\n",
       " 0.40849683570860723,\n",
       " 0.38494884681700564,\n",
       " 0.3390911846160779,\n",
       " 0.27211148262022827,\n",
       " 0.23772625160216188,\n",
       " 0.19257024955748411,\n",
       " 0.16619164466856806,\n",
       " 0.15840151023863641,\n",
       " 0.12892117500304068,\n",
       " 0.11884411048888051,\n",
       " 0.078948011398304305,\n",
       " 0.049454240798939071,\n",
       " 0.044251455306995695,\n",
       " 0.035084634780872641,\n",
       " 0.059156251907337448,\n",
       " 0.042253942489612825,\n",
       " 0.041138929367054218,\n",
       " 0.060396940231312007,\n",
       " 0.046117578506458479,\n",
       " 0.055277975082386202,\n",
       " 0.054609460830677213,\n",
       " 0.075092737197864703,\n",
       " 0.077623949050892024,\n",
       " 0.10172430992125335,\n",
       " 0.12870924949644866,\n",
       " 0.14035936546324551,\n",
       " 0.15656159019469082,\n",
       " 0.16829232597349938,\n",
       " 0.20207097434996424,\n",
       " 0.21252085304259119,\n",
       " 0.23315015602110681,\n",
       " 0.24706408500670252,\n",
       " 0.28176426887511069,\n",
       " 0.30555599784849935,\n",
       " 0.24384520912169266,\n",
       " 0.26572029685972975,\n",
       " 0.27061343955992506,\n",
       " 0.26536262702940749,\n",
       " 0.27724617958067699,\n",
       " 0.28578838920592109,\n",
       " 0.30361891365050114,\n",
       " 0.34173552513121402,\n",
       " 0.35642376518248353,\n",
       " 0.3822715473174933,\n",
       " 0.40086911582945617,\n",
       " 0.42464526557921201,\n",
       " 0.45655877876280576,\n",
       " 0.48670493507384088,\n",
       " 0.50461373710631163,\n",
       " 0.53391056632994449,\n",
       " 0.54087654304503241,\n",
       " 0.52008542060850893,\n",
       " 0.53071125602721003,\n",
       " 0.54228034782408507,\n",
       " 0.54571497917174128,\n",
       " 0.54371291160582336,\n",
       " 0.54390423011778621,\n",
       " 0.54096996498106742,\n",
       " 0.54587179374693651,\n",
       " 0.55894476509093061,\n",
       " 0.55908403587340127,\n",
       " 0.52741402244566693,\n",
       " 0.50247498512266886,\n",
       " 0.47221235084532509,\n",
       " 0.46576752281187778,\n",
       " 0.4498015727996707,\n",
       " 0.44403481483458279,\n",
       " 0.43141234397886985,\n",
       " 0.43693764305113547,\n",
       " 0.43007708168028586,\n",
       " 0.42094714164732688,\n",
       " 0.40880895042418236,\n",
       " 0.39069744300841086,\n",
       " 0.38482696723936788,\n",
       " 0.37054437446593036,\n",
       " 0.35719394874571553,\n",
       " 0.37005478096007094,\n",
       " 0.37849080467222912,\n",
       " 0.37331799125670179,\n",
       " 0.3474089527130006,\n",
       " 0.34071371650694587,\n",
       " 0.33086960792540288,\n",
       " 0.34636717796324468,\n",
       " 0.33732053565977788,\n",
       " 0.33121369361876224,\n",
       " 0.31059028816221929,\n",
       " 0.29350596427916265,\n",
       " 0.2871694774627564,\n",
       " 0.30260016059874273,\n",
       " 0.30290090370177009,\n",
       " 0.31001550865172123,\n",
       " 0.31364407539366457,\n",
       " 0.27728497123717039,\n",
       " 0.25701901054381099,\n",
       " 0.26951033592222895,\n",
       " 0.28111884880064691,\n",
       " 0.29274489784239494,\n",
       " 0.30632314109801018,\n",
       " 0.31638083839415276,\n",
       " 0.31205836677550042,\n",
       " 0.32423980140684805,\n",
       " 0.33532082939146718,\n",
       " 0.34034116172789297,\n",
       " 0.3373694667816039,\n",
       " 0.3301878833770629,\n",
       " 0.3543648853301879,\n",
       " 0.39015476417540274,\n",
       " 0.40104646873472893,\n",
       " 0.40090417671202383,\n",
       " 0.41164914131163322,\n",
       " 0.41313864326475819,\n",
       " 0.42498419761656481,\n",
       " 0.42545611000059796,\n",
       " 0.41470207786558821,\n",
       " 0.43017816352843002,\n",
       " 0.45072306251524641,\n",
       " 0.44599064064024641,\n",
       " 0.42608934593199443,\n",
       " 0.41530835533140847,\n",
       " 0.42651356697081277,\n",
       " 0.43653430747984595,\n",
       " 0.45046431541441628,\n",
       " 0.45154717445372294,\n",
       " 0.46386128425596906,\n",
       " 0.48717360496519757,\n",
       " 0.47976767921446511,\n",
       " 0.48357250785826394,\n",
       " 0.45844105339049052,\n",
       " 0.47616743278502172,\n",
       " 0.47644542503355686,\n",
       " 0.46392522239683809,\n",
       " 0.45238368034361542,\n",
       " 0.4373140354156369,\n",
       " 0.4437979755401486,\n",
       " 0.44522561073301969,\n",
       " 0.45398430442808801,\n",
       " 0.44437577247618371,\n",
       " 0.43259975242613485,\n",
       " 0.42204420661925007,\n",
       " 0.4105804920196407,\n",
       " 0.38946024894713094,\n",
       " 0.37498481941221884,\n",
       " 0.38744246101378133,\n",
       " 0.40257954216002156,\n",
       " 0.38962222862242391,\n",
       " 0.37630412483214071,\n",
       " 0.38121214103697465,\n",
       " 0.3744022159576289,\n",
       " 0.37526938438414259,\n",
       " 0.37935752296446484,\n",
       " 0.37692777824400581,\n",
       " 0.36917472648619326,\n",
       " 0.36852781105040222,\n",
       " 0.3666903018951288,\n",
       " 0.38386536598204285,\n",
       " 0.38356756401060732,\n",
       " 0.37408259773253116,\n",
       " 0.37793813133238469,\n",
       " 0.35053939628599795,\n",
       " 0.33780664634703311,\n",
       " 0.3373400020599237,\n",
       " 0.29487120246885923,\n",
       " 0.30311584663389829,\n",
       " 0.27914999961851739,\n",
       " 0.26196646308897636,\n",
       " 0.26811606788633963,\n",
       " 0.25793949317930837,\n",
       " 0.27035561943052905,\n",
       " 0.26467320060728683,\n",
       " 0.24803393363951337,\n",
       " 0.22999481773375163,\n",
       " 0.2523618755340446,\n",
       " 0.2423038463592399,\n",
       " 0.237861364364611,\n",
       " 0.23766779899595866,\n",
       " 0.26674048805235512,\n",
       " 0.27749031639097815,\n",
       " 0.30287395286558749,\n",
       " 0.31180257606505041,\n",
       " 0.32698545265196449,\n",
       " 0.34235095024107581,\n",
       " 0.35425968360899573,\n",
       " 0.36070012474058755,\n",
       " 0.35788293266295085,\n",
       " 0.34911325645445473,\n",
       " 0.34630440711973792,\n",
       " 0.36914088249205235,\n",
       " 0.33297532844542144,\n",
       " 0.3285625820159781,\n",
       " 0.32866609764097809,\n",
       " 0.32607878684996244,\n",
       " 0.33240567588804837,\n",
       " 0.33374609565733548,\n",
       " 0.35809357643126127,\n",
       " 0.3707881641387808,\n",
       " 0.36527800941465971,\n",
       " 0.36364413642881988,\n",
       " 0.38353912162779447,\n",
       " 0.39808889198301906,\n",
       " 0.41574488639830226,\n",
       " 0.42519462013243309,\n",
       " 0.42920784568785303,\n",
       " 0.44374894142149557,\n",
       " 0.44541988563536272,\n",
       " 0.44322170448301895,\n",
       " 0.44490995216368301,\n",
       " 0.4057180233001576,\n",
       " 0.39357415962217907,\n",
       " 0.39649581336973766,\n",
       " 0.40498214912413216,\n",
       " 0.41303110694883916,\n",
       " 0.41266060447691533,\n",
       " 0.40275921821592897,\n",
       " 0.41035220909117309,\n",
       " 0.40230420875547973,\n",
       " 0.40946118736265746,\n",
       " 0.40803664970396603,\n",
       " 0.3840137348174914,\n",
       " 0.37680944633482538,\n",
       " 0.36680564689634881,\n",
       " 0.33980807304380972,\n",
       " 0.33151367759703237,\n",
       " 0.31945191001890738,\n",
       " 0.31491628456114368,\n",
       " 0.30113366127012803,\n",
       " 0.31903203010557724,\n",
       " 0.30723514366148541,\n",
       " 0.31368670845030378,\n",
       " 0.32120874977110453,\n",
       " 0.30740098762510842,\n",
       " 0.2888740558624131,\n",
       " 0.28611636161802834,\n",
       " 0.28647028923033302,\n",
       " 0.28927263832090916,\n",
       " 0.2610979251861435,\n",
       " 0.25539870262144621,\n",
       " 0.26592490196226654,\n",
       " 0.2604443912505966,\n",
       " 0.24540579414366298,\n",
       " 0.25271377372740322,\n",
       " 0.24870996284483488,\n",
       " 0.24733268928526456,\n",
       " 0.24367562294004969,\n",
       " 0.22570819282530358,\n",
       " 0.23831902122496176,\n",
       " 0.26263799095152424,\n",
       " 0.27850330543516677,\n",
       " 0.29942289161680735,\n",
       " 0.3190335330962995,\n",
       " 0.32259580039976626,\n",
       " 0.32713339424131899,\n",
       " 0.31768810081480531,\n",
       " 0.32193860816954162,\n",
       " 0.31183018302916071,\n",
       " 0.31469643592833063,\n",
       " 0.3294439983367779,\n",
       " 0.33558637809752007,\n",
       " 0.34394455146788139,\n",
       " 0.35361559486387745,\n",
       " 0.37668868064878952,\n",
       " 0.37337457084654341,\n",
       " 0.38116803550718792,\n",
       " 0.37329303932188518,\n",
       " 0.36204572868345747,\n",
       " 0.36615447425840864,\n",
       " 0.3812895870208598,\n",
       " 0.39278635978697307,\n",
       " 0.36506686592100623,\n",
       " 0.36959369087217808,\n",
       " 0.34754429435728551,\n",
       " 0.33254335212706088,\n",
       " 0.3260952396392679,\n",
       " 0.32203529167173855,\n",
       " 0.33851952934263696,\n",
       " 0.35976202201841817,\n",
       " 0.36590535163877946,\n",
       " 0.39035396385191418,\n",
       " 0.38915333366392585,\n",
       " 0.39225973701475592,\n",
       " 0.39494609642027351,\n",
       " 0.40586480140684578,\n",
       " 0.38735429954527351,\n",
       " 0.37553418922422854,\n",
       " 0.37568655204771484,\n",
       " 0.38148819923399413,\n",
       " 0.3870576267242285,\n",
       " 0.409492692947373,\n",
       " 0.40256023597715818,\n",
       " 0.37100504112242183,\n",
       " 0.38271203422544919,\n",
       " 0.38240645790098632,\n",
       " 0.3723738422393652,\n",
       " 0.36731698417662106,\n",
       " 0.37852117729185542,\n",
       " 0.39640087699888665,\n",
       " 0.37444845771788082,\n",
       " 0.37227496147154293,\n",
       " 0.37610460472105467,\n",
       " 0.37445688056944332,\n",
       " 0.39142980003355465,\n",
       " 0.39527959632872067,\n",
       " 0.39791402626036132,\n",
       " 0.38394338035582026,\n",
       " 0.39080849647520499,\n",
       " 0.38357042121885732,\n",
       " 0.37256892967222643,\n",
       " 0.36633930015562488,\n",
       " 0.3562687931060643,\n",
       " 0.35593767356871076,\n",
       " 0.35789086341856424,\n",
       " 0.35250831794737281,\n",
       " 0.3171101894378513,\n",
       " 0.30810066413877901,\n",
       " 0.30872946739195284,\n",
       " 0.31525717735289033,\n",
       " 0.30537732887266567,\n",
       " 0.30466208076475548,\n",
       " 0.31531521415708946,\n",
       " 0.27497199440000936,\n",
       " 0.26324738883970661,\n",
       " 0.25800153160093708,\n",
       " 0.25385049629209916,\n",
       " 0.23315270805357374,\n",
       " 0.23179112434385693,\n",
       " 0.23704840278623973,\n",
       " 0.23151090431211863,\n",
       " 0.22704807853697212,\n",
       " 0.24971012687681587,\n",
       " 0.25289456367491159,\n",
       " 0.26019714546202094,\n",
       " 0.23992719078062444,\n",
       " 0.22657703208921817,\n",
       " 0.2245243587493744,\n",
       " 0.23185748863218689,\n",
       " 0.23004039573667909,\n",
       " 0.2392779407501068,\n",
       " 0.23091963768003843,\n",
       " 0.21466258430479426,\n",
       " 0.22253450202940361,\n",
       " 0.21977034568785087,\n",
       " 0.20831307029722584,\n",
       " 0.21448081016538989,\n",
       " 0.19323288536070238,\n",
       " 0.19988049888609297,\n",
       " 0.19965194892881755,\n",
       " 0.20950580787657144,\n",
       " 0.232304170608505,\n",
       " 0.25835535621641514,\n",
       " 0.2284520969390714,\n",
       " 0.2265302410125577,\n",
       " 0.24339101600645416,\n",
       " 0.22883317756651275,\n",
       " 0.21825945472715724,\n",
       " 0.2026917858123623,\n",
       " 0.20180942726133691,\n",
       " 0.21249940681455953,\n",
       " 0.20886455345152241,\n",
       " 0.21163680076597552,\n",
       " 0.20109628105162003,\n",
       " 0.17282077217100481,\n",
       " 0.15936603355406143,\n",
       " 0.17302511405943249,\n",
       " 0.19319222450254769,\n",
       " 0.21123417091368049,\n",
       " 0.23835054969786015,\n",
       " 0.25830772972105354,\n",
       " 0.27894600868223518,\n",
       " 0.2936609897613367,\n",
       " 0.30428023719786013,\n",
       " 0.30227266883848514,\n",
       " 0.30211149787901248,\n",
       " 0.32277385139463743,\n",
       " 0.30970323753355344,\n",
       " 0.30519272804258663,\n",
       " 0.29618468284605343,\n",
       " 0.31726558113096553,\n",
       " 0.30050264930723508,\n",
       " 0.29347765541075072,\n",
       " 0.29878527641294794,\n",
       " 0.32580447578428584,\n",
       " 0.33120194816587761,\n",
       " 0.33985214042661976,\n",
       " 0.34114240074156116,\n",
       " 0.33853238677976916,\n",
       " 0.34190059280393909,\n",
       " 0.30135687637327502,\n",
       " 0.27449010848997424,\n",
       " 0.26133191680906603,\n",
       " 0.27199393081663437,\n",
       " 0.27808578109739607,\n",
       " 0.25156591796873395,\n",
       " 0.2478399810790855,\n",
       " 0.23875048828123391,\n",
       " 0.23977437210081398,\n",
       " 0.24865026092527687,\n",
       " 0.24983188629148778,\n",
       " 0.22717961120603852,\n",
       " 0.17073209381101895,\n",
       " 0.14612094497679043,\n",
       " 0.14335458374021817,\n",
       " 0.15586944580076503,\n",
       " 0.15650035476682947,\n",
       " 0.17075685501097007,\n",
       " 0.16479654312132164,\n",
       " 0.11009798049925132,\n",
       " 0.12515849685667318,\n",
       " 0.13973447036741535,\n",
       " 0.15897349929807938,\n",
       " 0.16792506408689772,\n",
       " 0.18613538742063793,\n",
       " 0.18599928474424629,\n",
       " 0.18690252876280097,\n",
       " 0.1851675930023029,\n",
       " 0.075932382583601729,\n",
       " 0.080730745315535299,\n",
       " 0.084621381759627065,\n",
       " 0.086195051193220809,\n",
       " 0.13582474327085753,\n",
       " 0.15943184852598447,\n",
       " 0.16552084159849423,\n",
       " 0.16932692146299616,\n",
       " 0.18082910346983208,\n",
       " 0.18686540794370901,\n",
       " 0.19326740264890918,\n",
       " 0.2077430534362627,\n",
       " 0.21207963562010057,\n",
       " 0.2255251312255693,\n",
       " 0.2353715724944902,\n",
       " 0.2134116077422929,\n",
       " 0.20556231307981729,\n",
       " 0.20627372360227819,\n",
       " 0.22699573135374304,\n",
       " 0.23655764007566685,\n",
       " 0.26631953048704377,\n",
       " 0.26317217826841582,\n",
       " 0.26106580162046661,\n",
       " 0.26309711647032008,\n",
       " 0.2679272747039626,\n",
       " 0.26954631423948505,\n",
       " 0.17846415710447527,\n",
       " 0.16039017486570573,\n",
       " 0.1667849216461012,\n",
       " 0.14743635749815198,\n",
       " 0.15015681648252696,\n",
       " 0.14246891975401133,\n",
       " 0.13226209068296638,\n",
       " 0.13148177909849371,\n",
       " 0.13970898628233158,\n",
       " 0.15674611854551515,\n",
       " 0.17372241020200929,\n",
       " 0.17473260307310304,\n",
       " 0.19418696022031984,\n",
       " 0.21701839637754639,\n",
       " 0.23339901351927,\n",
       " 0.24213445854185298,\n",
       " 0.27060540580747799,\n",
       " 0.28410453033445549,\n",
       " 0.28496525573728748,\n",
       " 0.27759207725523183,\n",
       " 0.29838306236265366,\n",
       " 0.28310222434995835,\n",
       " 0.2814365673065013,\n",
       " 0.20878695487974347,\n",
       " 0.20709893226621809,\n",
       " 0.19009205818174543,\n",
       " 0.22822118568418681,\n",
       " 0.23407699775694071,\n",
       " 0.21020247459409888,\n",
       " 0.19646580314634496,\n",
       " 0.18880168342588594,\n",
       " 0.19314543724058317,\n",
       " 0.19970668792722868,\n",
       " 0.18646204757688686,\n",
       " 0.18389362525938197,\n",
       " 0.18108855819700403,\n",
       " 0.18376049423216026,\n",
       " 0.19348404121397175,\n",
       " 0.17700707435606156,\n",
       " 0.16728400802610549,\n",
       " 0.17826073074339061,\n",
       " 0.1307484474181953,\n",
       " 0.13492416572569038,\n",
       " 0.14375485038755559,\n",
       " 0.16061546134946963,\n",
       " 0.1677935562133612,\n",
       " 0.17644651985166687,\n",
       " 0.19204534530637876,\n",
       " 0.2155718498229803,\n",
       " 0.19269932365415704,\n",
       " 0.18702429962156425,\n",
       " 0.20454743194578298,\n",
       " 0.20939738655088549,\n",
       " 0.22013553810117845,\n",
       " 0.23714265823362471,\n",
       " 0.24641179466245772,\n",
       " 0.2372326183318913,\n",
       " 0.25585089302061198,\n",
       " 0.26925084495542639,\n",
       " 0.26406351280210605,\n",
       " 0.27118360137937653,\n",
       " 0.29252862167356597,\n",
       " 0.31180220031736477,\n",
       " 0.30911609268186674,\n",
       " 0.32263477706907373,\n",
       " 0.32331148910520652,\n",
       " 0.31271326065061666,\n",
       " 0.28891059112547018,\n",
       " 0.29735675621030905,\n",
       " 0.30174668693540668,\n",
       " 0.31256080245969869,\n",
       " 0.31122076797483539,\n",
       " 0.32804248428342908,\n",
       " 0.33705954360960094,\n",
       " 0.33483404922483534,\n",
       " 0.33142244529722303,\n",
       " 0.32552611351011362,\n",
       " 0.31400975608823861,\n",
       " 0.3149767551421937,\n",
       " 0.32274756050108044,\n",
       " 0.30288463783262337,\n",
       " 0.31800319480894174,\n",
       " 0.30032006263731087,\n",
       " 0.29282245063779916,\n",
       " 0.30761560440061653,\n",
       " 0.28892120170591434,\n",
       " 0.29350753211973268,\n",
       " 0.28148692893980104,\n",
       " 0.29412032318113401,\n",
       " 0.30677856445310664,\n",
       " 0.29778836441038203,\n",
       " 0.28415214729307248,\n",
       " 0.28052582168577267,\n",
       " 0.27884559440610956,\n",
       " 0.26745904731748654,\n",
       " 0.26352906990049435,\n",
       " 0.2686390247344787,\n",
       " 0.26340517234800409,\n",
       " 0.26603491592405387,\n",
       " 0.22986690330503531,\n",
       " 0.229024240493756,\n",
       " 0.19913163375852649,\n",
       " 0.13805182456968368,\n",
       " 0.12772949028013286,\n",
       " 0.11575053405759866,\n",
       " 0.14608739089963968,\n",
       " 0.16527982711790137,\n",
       " 0.17273339462278417,\n",
       " 0.15832889366148045,\n",
       " 0.16717741394041111,\n",
       " 0.16865235710142185,\n",
       " 0.17063301086423921,\n",
       " 0.19415021514890715,\n",
       " 0.20868257904050869,\n",
       " 0.20184778594968839,\n",
       " 0.19437184524534265,\n",
       " 0.19567817878721278,\n",
       " 0.21776191520689048,\n",
       " 0.23491536331174886,\n",
       " 0.24025524330137288,\n",
       " 0.26041770744321857,\n",
       " 0.26533867645261799,\n",
       " 0.27528391838071858,\n",
       " 0.286308475494366,\n",
       " 0.22214952468870192,\n",
       " 0.1926982135772517,\n",
       " 0.17607769584653879,\n",
       " 0.18179241561887763,\n",
       " 0.18702031707761785,\n",
       " 0.19062470817564028,\n",
       " 0.19887974548337953,\n",
       " 0.14474731063840879,\n",
       " 0.14401725006101618,\n",
       " 0.1396468048095513,\n",
       " 0.11260421562192924,\n",
       " 0.097843141555767121,\n",
       " 0.11329441642759328,\n",
       " 0.12280667686460496,\n",
       " 0.12696517181394576,\n",
       " 0.11145902252195355,\n",
       " 0.12864524459836957,\n",
       " 0.13587984275815959,\n",
       " 0.14489601707456584,\n",
       " 0.1322692317962455,\n",
       " 0.1400644607543754,\n",
       " 0.13807671737668986,\n",
       " 0.09994311904905312,\n",
       " 0.1126919269561576,\n",
       " 0.13416572189329137,\n",
       " 0.11751832962034214,\n",
       " 0.12322806358335484,\n",
       " 0.10961054420469271,\n",
       " 0.13202780532834993,\n",
       " 0.11658363342283234,\n",
       " 0.12448689270017607,\n",
       " 0.13076012611387236,\n",
       " 0.14669271850584012,\n",
       " 0.16336117744443873,\n",
       " 0.17538094139097191,\n",
       " 0.17334887313840841,\n",
       " 0.16742427062986348,\n",
       " 0.18415315628049822,\n",
       " 0.16312624931333514,\n",
       " 0.13377445793149917,\n",
       " 0.12463624954221693,\n",
       " 0.11975662994382823,\n",
       " 0.096237365722636803,\n",
       " 0.080218715667705137,\n",
       " 0.087501125335673891,\n",
       " 0.090712646484355522,\n",
       " 0.10643535804746584,\n",
       " 0.12191108894346193,\n",
       " 0.13739535903928712,\n",
       " 0.14440126609800291,\n",
       " 0.14497499656675292,\n",
       " 0.16343716812131834,\n",
       " 0.18230022239683102,\n",
       " 0.18357448577878901,\n",
       " 0.20647848320005366,\n",
       " 0.21426930427549309,\n",
       " 0.2364859638213915,\n",
       " 0.23930129051206531,\n",
       " 0.23815463829038558,\n",
       " 0.24531589126584941,\n",
       " 0.24904453659055642,\n",
       " 0.26661553001401833,\n",
       " 0.2618652381896775,\n",
       " 0.27569751167295387,\n",
       " 0.27417387771604468,\n",
       " 0.28157684516904757,\n",
       " 0.28988146209714816,\n",
       " 0.32688631439207005,\n",
       " 0.33733506393430635,\n",
       " 0.32480257415769503,\n",
       " 0.3486486492156784,\n",
       " 0.34423983955381315,\n",
       " 0.31555202865598603,\n",
       " 0.3162396965026657,\n",
       " 0.30640462112424771,\n",
       " 0.30475196456907194,\n",
       " 0.27478559303281708,\n",
       " 0.27999716186521451,\n",
       " 0.26272940444944298,\n",
       " 0.26285749053953084,\n",
       " 0.27227967453000934,\n",
       " 0.26745846176145466,\n",
       " 0.26651789665220171,\n",
       " 0.28105410194394975,\n",
       " 0.29394123077390577,\n",
       " 0.29382133674619582,\n",
       " 0.2979126682281294,\n",
       " 0.28005250549314403,\n",
       " 0.25992439651487254,\n",
       " 0.26489302635190864,\n",
       " 0.24334249687192816,\n",
       " 0.24185797500608341,\n",
       " 0.20962048149106874,\n",
       " 0.21521290969846618,\n",
       " 0.22282911300657163,\n",
       " 0.22355880737302672,\n",
       " 0.22882231521604426,\n",
       " 0.22990816497800715,\n",
       " 0.22250271224973567,\n",
       " 0.22188974761960872,\n",
       " 0.24350564193723565,\n",
       " 0.23911355781553154,\n",
       " 0.24500284004209405,\n",
       " 0.24526527404783136,\n",
       " 0.22446060371396903,\n",
       " 0.22778316116330985,\n",
       " 0.21619814682004812,\n",
       " 0.22843115615842702,\n",
       " 0.24990837478635669,\n",
       " 0.21653374481199145,\n",
       " 0.25658974075315355,\n",
       " 0.225199258804301,\n",
       " 0.22312974357602949,\n",
       " 0.22462963104246014,\n",
       " 0.24382831573484295,\n",
       " 0.2535730400085246,\n",
       " 0.24626270294187419,\n",
       " 0.24192154502866617,\n",
       " 0.23553677177427162,\n",
       " 0.22062181091306557,\n",
       " 0.22138083648679602,\n",
       " 0.21552748298642979,\n",
       " 0.22077422904966221,\n",
       " 0.21126388359067783,\n",
       " 0.201652076721171,\n",
       " 0.20081788635251863,\n",
       " 0.19924393463132722,\n",
       " 0.18613295173642974,\n",
       " 0.18468696212766508,\n",
       " 0.17818058776853421,\n",
       " 0.18196453475950097,\n",
       " 0.178253820419291,\n",
       " 0.14381747817991108,\n",
       " 0.12863070297239154,\n",
       " 0.11903853034971085,\n",
       " 0.10611111640928114,\n",
       " 0.10606769943235242,\n",
       " 0.057790025711038945,\n",
       " 0.061662939071634636,\n",
       " 0.054578359603861197,\n",
       " 0.07230205535886608,\n",
       " 0.084950105667093614,\n",
       " 0.090096555709818213,\n",
       " 0.10286134719846565,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
