{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum BiGANs add Energy Loss example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_BiGANs_add_Energy_loss'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     80
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def G(x,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "\n",
    "        conv1 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "    r5= tf.add(conv5, 0 ,name=name)#1*1*100\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def D_enc(x,z,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        z_flat = tf.reshape(z, (-1, z_size))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[z_flat.get_shape()[1], 64*64*2],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[64*64*2],initializer=b_init)\n",
    "        fc1 = tf.nn.elu(tf.matmul(z_flat,w1) + b1)\n",
    "          \n",
    "        r0 = tf.reshape(fc1, (-1, 64,64,2)) # 64*64*2\n",
    "        \n",
    "        x_cat = tf.concat([x, r0],3)\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(x_cat,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "     \n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_epoch = 100\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "m = 10\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.001\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,isTrain,name='G_sample') # G(z)\n",
    "E_z = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z, isTrain, reuse=True, name ='re_image')\n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, E_z, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample,z, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')                         \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                      \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "\n",
    "temp1 = tf.maximum(0.0, m - tf.sqrt(tf.reduce_sum((D_real - u)**2, axis=[1,2,3])))\n",
    "E_loss = tf.reduce_mean(temp1,  name = 'E_loss' )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss, var_list=G_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -83.492, D_real_e : 50.586, D_fake_e : 34.503, G_e : 35.169, E_e : 0.000, new_measure : 63.877, k_curr : 0.676337\n",
      "D_e : 31.511, D_real_e : 37.095, D_fake_e : 25.650, G_e : 26.109, E_e : 0.000, new_measure : 39.166, k_curr : 0.278010\n",
      "D_e : 28.121, D_real_e : 31.556, D_fake_e : 21.749, G_e : 22.128, E_e : 0.000, new_measure : 32.955, k_curr : 0.169794\n",
      "D_e : 27.157, D_real_e : 27.663, D_fake_e : 19.403, G_e : 19.453, E_e : 0.000, new_measure : 29.087, k_curr : -0.079271\n",
      "D_e : 23.396, D_real_e : 24.463, D_fake_e : 17.055, G_e : 17.116, E_e : 0.005, new_measure : 25.988, k_curr : -0.057276\n",
      "D_e : 20.278, D_real_e : 21.047, D_fake_e : 14.672, G_e : 14.712, E_e : 0.018, new_measure : 22.123, k_curr : -0.000239\n",
      "D_e : 18.069, D_real_e : 18.223, D_fake_e : 12.711, G_e : 12.708, E_e : 0.065, new_measure : 19.397, k_curr : 0.134522\n",
      "D_e : 15.696, D_real_e : 16.151, D_fake_e : 11.298, G_e : 11.356, E_e : 0.238, new_measure : 17.170, k_curr : -0.005487\n",
      "D_e : 14.459, D_real_e : 14.641, D_fake_e : 10.224, G_e : 10.256, E_e : 0.381, new_measure : 15.488, k_curr : -0.025895\n",
      "D_e : 12.788, D_real_e : 13.007, D_fake_e : 9.042, G_e : 9.074, E_e : 0.810, new_measure : 13.506, k_curr : 0.059630\n",
      "D_e : 11.859, D_real_e : 11.948, D_fake_e : 8.324, G_e : 8.353, E_e : 1.115, new_measure : 12.475, k_curr : 0.090606\n",
      "D_e : 10.858, D_real_e : 11.026, D_fake_e : 7.693, G_e : 7.740, E_e : 1.428, new_measure : 11.471, k_curr : 0.029373\n",
      "D_e : 10.217, D_real_e : 10.314, D_fake_e : 7.172, G_e : 7.230, E_e : 1.743, new_measure : 10.704, k_curr : 0.000584\n",
      "D_e : 9.743, D_real_e : 9.799, D_fake_e : 6.807, G_e : 6.858, E_e : 2.005, new_measure : 10.167, k_curr : 0.003282\n",
      "D_e : 9.155, D_real_e : 9.222, D_fake_e : 6.416, G_e : 6.458, E_e : 2.200, new_measure : 9.557, k_curr : -0.004437\n",
      "D_e : 8.361, D_real_e : 8.379, D_fake_e : 5.828, G_e : 5.870, E_e : 2.368, new_measure : 8.623, k_curr : -0.016951\n",
      "D_e : 7.776, D_real_e : 7.809, D_fake_e : 5.407, G_e : 5.460, E_e : 2.720, new_measure : 8.090, k_curr : 0.002368\n",
      "D_e : 7.470, D_real_e : 7.490, D_fake_e : 5.193, G_e : 5.241, E_e : 2.999, new_measure : 7.773, k_curr : 0.008804\n",
      "D_e : 7.206, D_real_e : 7.228, D_fake_e : 5.003, G_e : 5.058, E_e : 3.217, new_measure : 7.501, k_curr : 0.013689\n",
      "D_e : 6.970, D_real_e : 6.991, D_fake_e : 4.846, G_e : 4.899, E_e : 3.429, new_measure : 7.278, k_curr : 0.000727\n",
      "D_e : 6.801, D_real_e : 6.820, D_fake_e : 4.725, G_e : 4.778, E_e : 3.585, new_measure : 7.080, k_curr : -0.008657\n",
      "D_e : 6.648, D_real_e : 6.662, D_fake_e : 4.610, G_e : 4.662, E_e : 3.737, new_measure : 6.927, k_curr : -0.005839\n",
      "D_e : 6.481, D_real_e : 6.505, D_fake_e : 4.504, G_e : 4.554, E_e : 3.878, new_measure : 6.783, k_curr : -0.009193\n",
      "D_e : 6.354, D_real_e : 6.372, D_fake_e : 4.388, G_e : 4.448, E_e : 4.002, new_measure : 6.648, k_curr : 0.025340\n",
      "D_e : 6.238, D_real_e : 6.259, D_fake_e : 4.331, G_e : 4.390, E_e : 4.112, new_measure : 6.543, k_curr : -0.001023\n",
      "D_e : 6.156, D_real_e : 6.167, D_fake_e : 4.267, G_e : 4.319, E_e : 4.209, new_measure : 6.441, k_curr : -0.006303\n",
      "D_e : 6.059, D_real_e : 6.068, D_fake_e : 4.191, G_e : 4.245, E_e : 4.300, new_measure : 6.368, k_curr : 0.001128\n",
      "D_e : 5.943, D_real_e : 5.960, D_fake_e : 4.112, G_e : 4.167, E_e : 4.396, new_measure : 6.209, k_curr : 0.013999\n",
      "D_e : 5.882, D_real_e : 5.901, D_fake_e : 4.075, G_e : 4.131, E_e : 4.457, new_measure : 6.177, k_curr : 0.012708\n",
      "D_e : 5.786, D_real_e : 5.802, D_fake_e : 4.003, G_e : 4.066, E_e : 4.547, new_measure : 6.069, k_curr : -0.000539\n",
      "D_e : 5.717, D_real_e : 5.734, D_fake_e : 3.937, G_e : 4.009, E_e : 4.615, new_measure : 5.962, k_curr : 0.014641\n",
      "D_e : 5.657, D_real_e : 5.668, D_fake_e : 3.910, G_e : 3.976, E_e : 4.676, new_measure : 5.908, k_curr : -0.008212\n",
      "D_e : 5.567, D_real_e : 5.582, D_fake_e : 3.830, G_e : 3.900, E_e : 4.748, new_measure : 5.828, k_curr : 0.014825\n",
      "D_e : 5.519, D_real_e : 5.535, D_fake_e : 3.806, G_e : 3.874, E_e : 4.800, new_measure : 5.787, k_curr : 0.016464\n",
      "D_e : 5.477, D_real_e : 5.492, D_fake_e : 3.779, G_e : 3.847, E_e : 4.849, new_measure : 5.738, k_curr : 0.008240\n",
      "D_e : 5.421, D_real_e : 5.436, D_fake_e : 3.742, G_e : 3.808, E_e : 4.902, new_measure : 5.661, k_curr : 0.000260\n",
      "D_e : 5.370, D_real_e : 5.382, D_fake_e : 3.693, G_e : 3.769, E_e : 4.950, new_measure : 5.615, k_curr : -0.002490\n",
      "D_e : 5.301, D_real_e : 5.309, D_fake_e : 3.659, G_e : 3.721, E_e : 5.016, new_measure : 5.546, k_curr : -0.016246\n",
      "D_e : 5.250, D_real_e : 5.263, D_fake_e : 3.605, G_e : 3.676, E_e : 5.063, new_measure : 5.511, k_curr : 0.007055\n",
      "D_e : 5.218, D_real_e : 5.231, D_fake_e : 3.586, G_e : 3.663, E_e : 5.096, new_measure : 5.469, k_curr : 0.004505\n",
      "D_e : 5.173, D_real_e : 5.186, D_fake_e : 3.561, G_e : 3.625, E_e : 5.136, new_measure : 5.418, k_curr : 0.018456\n",
      "D_e : 5.138, D_real_e : 5.146, D_fake_e : 3.533, G_e : 3.609, E_e : 5.177, new_measure : 5.388, k_curr : 0.000517\n",
      "D_e : 5.063, D_real_e : 5.074, D_fake_e : 3.480, G_e : 3.558, E_e : 5.233, new_measure : 5.295, k_curr : -0.015041\n",
      "D_e : 5.052, D_real_e : 5.058, D_fake_e : 3.461, G_e : 3.532, E_e : 5.260, new_measure : 5.290, k_curr : 0.008878\n",
      "D_e : 5.007, D_real_e : 5.014, D_fake_e : 3.441, G_e : 3.510, E_e : 5.299, new_measure : 5.221, k_curr : 0.007659\n",
      "D_e : 4.978, D_real_e : 4.989, D_fake_e : 3.422, G_e : 3.493, E_e : 5.325, new_measure : 5.214, k_curr : 0.006775\n",
      "D_e : 4.947, D_real_e : 4.953, D_fake_e : 3.402, G_e : 3.469, E_e : 5.361, new_measure : 5.170, k_curr : 0.001171\n",
      "D_e : 4.911, D_real_e : 4.915, D_fake_e : 3.382, G_e : 3.441, E_e : 5.394, new_measure : 5.122, k_curr : 0.000468\n",
      "D_e : 4.883, D_real_e : 4.893, D_fake_e : 3.351, G_e : 3.425, E_e : 5.420, new_measure : 5.102, k_curr : 0.001632\n",
      "D_e : 4.844, D_real_e : 4.848, D_fake_e : 3.327, G_e : 3.393, E_e : 5.457, new_measure : 5.076, k_curr : 0.001692\n",
      "D_e : 4.822, D_real_e : 4.829, D_fake_e : 3.313, G_e : 3.380, E_e : 5.477, new_measure : 5.056, k_curr : 0.000849\n",
      "D_e : 4.788, D_real_e : 4.792, D_fake_e : 3.295, G_e : 3.357, E_e : 5.508, new_measure : 5.012, k_curr : -0.005942\n",
      "D_e : 4.754, D_real_e : 4.761, D_fake_e : 3.265, G_e : 3.332, E_e : 5.539, new_measure : 4.964, k_curr : -0.003104\n",
      "D_e : 4.742, D_real_e : 4.746, D_fake_e : 3.256, G_e : 3.323, E_e : 5.562, new_measure : 4.977, k_curr : -0.005786\n",
      "D_e : 4.701, D_real_e : 4.706, D_fake_e : 3.230, G_e : 3.289, E_e : 5.591, new_measure : 4.910, k_curr : 0.010217\n",
      "D_e : 4.691, D_real_e : 4.692, D_fake_e : 3.215, G_e : 3.287, E_e : 5.611, new_measure : 4.935, k_curr : 0.003741\n",
      "D_e : 4.644, D_real_e : 4.647, D_fake_e : 3.198, G_e : 3.251, E_e : 5.644, new_measure : 4.860, k_curr : 0.008379\n",
      "D_e : 4.645, D_real_e : 4.647, D_fake_e : 3.188, G_e : 3.255, E_e : 5.656, new_measure : 4.890, k_curr : 0.001377\n",
      "D_e : 4.593, D_real_e : 4.599, D_fake_e : 3.165, G_e : 3.225, E_e : 5.694, new_measure : 4.790, k_curr : -0.013940\n",
      "D_e : 4.560, D_real_e : 4.561, D_fake_e : 3.117, G_e : 3.183, E_e : 5.729, new_measure : 4.759, k_curr : 0.012932\n",
      "D_e : 4.576, D_real_e : 4.580, D_fake_e : 3.138, G_e : 3.209, E_e : 5.723, new_measure : 4.792, k_curr : 0.002826\n",
      "D_e : 4.551, D_real_e : 4.554, D_fake_e : 3.140, G_e : 3.193, E_e : 5.738, new_measure : 4.742, k_curr : -0.011788\n",
      "D_e : 4.516, D_real_e : 4.520, D_fake_e : 3.106, G_e : 3.162, E_e : 5.764, new_measure : 4.726, k_curr : -0.007777\n",
      "D_e : 4.504, D_real_e : 4.508, D_fake_e : 3.088, G_e : 3.153, E_e : 5.783, new_measure : 4.714, k_curr : -0.000534\n",
      "D_e : 4.482, D_real_e : 4.482, D_fake_e : 3.080, G_e : 3.135, E_e : 5.804, new_measure : 4.707, k_curr : 0.005384\n",
      "D_e : 4.455, D_real_e : 4.460, D_fake_e : 3.052, G_e : 3.119, E_e : 5.826, new_measure : 4.662, k_curr : 0.013309\n",
      "D_e : 4.422, D_real_e : 4.423, D_fake_e : 3.044, G_e : 3.097, E_e : 5.855, new_measure : 4.644, k_curr : 0.010348\n",
      "D_e : 4.407, D_real_e : 4.412, D_fake_e : 3.025, G_e : 3.091, E_e : 5.870, new_measure : 4.621, k_curr : 0.002156\n",
      "D_e : 4.403, D_real_e : 4.404, D_fake_e : 3.001, G_e : 3.075, E_e : 5.885, new_measure : 4.626, k_curr : 0.024560\n",
      "D_e : 4.371, D_real_e : 4.373, D_fake_e : 3.005, G_e : 3.074, E_e : 5.909, new_measure : 4.593, k_curr : -0.011672\n",
      "D_e : 4.351, D_real_e : 4.351, D_fake_e : 2.970, G_e : 3.040, E_e : 5.928, new_measure : 4.589, k_curr : 0.004501\n",
      "D_e : 4.334, D_real_e : 4.335, D_fake_e : 2.979, G_e : 3.037, E_e : 5.944, new_measure : 4.573, k_curr : -0.002754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 4.314, D_real_e : 4.313, D_fake_e : 2.957, G_e : 3.013, E_e : 5.959, new_measure : 4.520, k_curr : 0.013776\n",
      "D_e : 4.305, D_real_e : 4.307, D_fake_e : 2.955, G_e : 3.015, E_e : 5.971, new_measure : 4.525, k_curr : 0.013743\n",
      "D_e : 4.290, D_real_e : 4.292, D_fake_e : 2.950, G_e : 3.006, E_e : 5.985, new_measure : 4.503, k_curr : 0.009063\n",
      "D_e : 4.267, D_real_e : 4.269, D_fake_e : 2.936, G_e : 2.989, E_e : 6.006, new_measure : 4.443, k_curr : 0.004862\n",
      "D_e : 4.262, D_real_e : 4.265, D_fake_e : 2.921, G_e : 2.984, E_e : 6.014, new_measure : 4.464, k_curr : 0.009472\n",
      "D_e : 4.225, D_real_e : 4.230, D_fake_e : 2.905, G_e : 2.961, E_e : 6.034, new_measure : 4.415, k_curr : 0.009373\n",
      "D_e : 4.240, D_real_e : 4.264, D_fake_e : 2.923, G_e : 2.990, E_e : 6.004, new_measure : 4.460, k_curr : -0.004076\n",
      "D_e : 4.210, D_real_e : 4.211, D_fake_e : 2.886, G_e : 2.941, E_e : 6.068, new_measure : 4.407, k_curr : 0.015024\n",
      "D_e : 4.177, D_real_e : 4.190, D_fake_e : 2.871, G_e : 2.939, E_e : 6.084, new_measure : 4.377, k_curr : -0.001145\n",
      "D_e : 4.170, D_real_e : 4.177, D_fake_e : 2.872, G_e : 2.924, E_e : 6.093, new_measure : 4.364, k_curr : -0.002828\n",
      "D_e : 4.148, D_real_e : 4.154, D_fake_e : 2.859, G_e : 2.911, E_e : 6.111, new_measure : 4.352, k_curr : -0.011620\n",
      "D_e : 4.129, D_real_e : 4.135, D_fake_e : 2.833, G_e : 2.891, E_e : 6.129, new_measure : 4.323, k_curr : -0.001005\n",
      "D_e : 4.129, D_real_e : 4.137, D_fake_e : 2.830, G_e : 2.894, E_e : 6.134, new_measure : 4.328, k_curr : 0.003593\n",
      "D_e : 4.111, D_real_e : 4.114, D_fake_e : 2.820, G_e : 2.877, E_e : 6.153, new_measure : 4.309, k_curr : 0.010494\n",
      "D_e : 4.104, D_real_e : 4.131, D_fake_e : 2.819, G_e : 2.893, E_e : 6.131, new_measure : 4.307, k_curr : 0.008488\n",
      "D_e : 4.086, D_real_e : 4.123, D_fake_e : 2.810, G_e : 2.886, E_e : 6.132, new_measure : 4.286, k_curr : 0.007600\n",
      "D_e : 4.072, D_real_e : 4.103, D_fake_e : 2.806, G_e : 2.872, E_e : 6.147, new_measure : 4.280, k_curr : 0.007332\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "    \n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "\n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            \n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, k : k_curr, isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e = sess.run([G_optim, G_loss], {u : u_, z : z_, k : k_curr,  isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            _ , E_e = sess.run([E_optim, E_loss], {u : u_, z : z_,  k : k_curr, isTrain : True}) \n",
    "            E_error.append(E_e)\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, E_e : %.3f, new_measure : %.3f, k_curr : %3f'%(np.mean(D_error), np.mean(D_real_error),\n",
    "            np.mean(D_fake_error), np.mean(G_error),np.mean(E_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        r = sess.run([re_image],feed_dict={u : test_normal_data[0:16],isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/origin_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []\n",
    "        E_error = []\n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "\n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
