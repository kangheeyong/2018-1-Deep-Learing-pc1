{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum DCGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_DCGANs_for_FD_v1'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     80
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def G(x,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "\n",
    "        conv1 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "    r5= tf.add(conv5, 0 ,name=name)#1*1*100\n",
    "  \n",
    "    return r5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_epoch = 100\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.001\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,name='G_sample') # G(z)\n",
    "E_z = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z, isTrain, reuse=True, name ='re_image')\n",
    "re_z = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "E_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z-z)**2, axis=[1,2,3])) , name = 're_z_loss')             \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "    \n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss, var_list=G_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -104.101, D_real_e : 50.571, D_fake_e : 35.085, G_e : 35.394, new_measure : 63.442, k_curr : 0.017039\n",
      "D_e : 31.323, D_real_e : 37.464, D_fake_e : 25.573, G_e : 26.099, new_measure : 39.677, k_curr : 0.369066\n",
      "D_e : 26.056, D_real_e : 33.058, D_fake_e : 22.630, G_e : 23.157, new_measure : 34.022, k_curr : 0.321842\n",
      "D_e : 24.219, D_real_e : 28.640, D_fake_e : 19.701, G_e : 20.113, new_measure : 29.648, k_curr : 0.141343\n",
      "D_e : 22.336, D_real_e : 24.770, D_fake_e : 17.046, G_e : 17.329, new_measure : 25.836, k_curr : 0.170310\n",
      "D_e : 19.837, D_real_e : 21.816, D_fake_e : 15.102, G_e : 15.299, new_measure : 22.759, k_curr : 0.091815\n",
      "D_e : 18.021, D_real_e : 18.909, D_fake_e : 13.116, G_e : 13.225, new_measure : 19.913, k_curr : 0.123356\n",
      "D_e : 15.883, D_real_e : 16.259, D_fake_e : 11.325, G_e : 11.448, new_measure : 17.284, k_curr : -0.062515\n",
      "D_e : 13.633, D_real_e : 13.981, D_fake_e : 9.623, G_e : 9.722, new_measure : 14.888, k_curr : 0.118192\n",
      "D_e : 11.989, D_real_e : 12.320, D_fake_e : 8.528, G_e : 8.632, new_measure : 13.074, k_curr : 0.096503\n",
      "D_e : 10.927, D_real_e : 11.083, D_fake_e : 7.741, G_e : 7.798, new_measure : 11.808, k_curr : -0.014148\n",
      "D_e : 9.882, D_real_e : 9.987, D_fake_e : 6.923, G_e : 6.986, new_measure : 10.612, k_curr : 0.001081\n",
      "D_e : 8.992, D_real_e : 9.154, D_fake_e : 6.344, G_e : 6.398, new_measure : 9.572, k_curr : 0.028525\n",
      "D_e : 8.224, D_real_e : 8.438, D_fake_e : 5.829, G_e : 5.906, new_measure : 8.822, k_curr : 0.029545\n",
      "D_e : 7.590, D_real_e : 7.802, D_fake_e : 5.380, G_e : 5.453, new_measure : 8.100, k_curr : 0.053515\n",
      "D_e : 6.771, D_real_e : 6.933, D_fake_e : 4.794, G_e : 4.858, new_measure : 7.177, k_curr : 0.039787\n",
      "D_e : 6.366, D_real_e : 6.506, D_fake_e : 4.479, G_e : 4.554, new_measure : 6.735, k_curr : 0.040314\n",
      "D_e : 6.085, D_real_e : 6.242, D_fake_e : 4.301, G_e : 4.381, new_measure : 6.486, k_curr : 0.007799\n",
      "D_e : 5.918, D_real_e : 6.039, D_fake_e : 4.158, G_e : 4.225, new_measure : 6.265, k_curr : 0.014950\n",
      "D_e : 5.780, D_real_e : 5.865, D_fake_e : 4.030, G_e : 4.100, new_measure : 6.072, k_curr : 0.032063\n",
      "D_e : 5.605, D_real_e : 5.730, D_fake_e : 3.930, G_e : 4.014, new_measure : 5.937, k_curr : 0.024403\n",
      "D_e : 5.480, D_real_e : 5.595, D_fake_e : 3.838, G_e : 3.913, new_measure : 5.805, k_curr : 0.033391\n",
      "D_e : 5.349, D_real_e : 5.513, D_fake_e : 3.771, G_e : 3.855, new_measure : 5.728, k_curr : 0.045228\n",
      "D_e : 5.269, D_real_e : 5.411, D_fake_e : 3.708, G_e : 3.795, new_measure : 5.595, k_curr : 0.025194\n",
      "D_e : 5.151, D_real_e : 5.317, D_fake_e : 3.645, G_e : 3.722, new_measure : 5.517, k_curr : 0.026565\n",
      "D_e : 5.109, D_real_e : 5.225, D_fake_e : 3.579, G_e : 3.653, new_measure : 5.415, k_curr : 0.040875\n",
      "D_e : 5.034, D_real_e : 5.170, D_fake_e : 3.542, G_e : 3.625, new_measure : 5.355, k_curr : 0.026164\n",
      "D_e : 4.964, D_real_e : 5.086, D_fake_e : 3.477, G_e : 3.561, new_measure : 5.270, k_curr : 0.023513\n",
      "D_e : 4.906, D_real_e : 5.029, D_fake_e : 3.439, G_e : 3.517, new_measure : 5.204, k_curr : 0.032553\n",
      "D_e : 4.848, D_real_e : 5.011, D_fake_e : 3.421, G_e : 3.510, new_measure : 5.193, k_curr : 0.026549\n",
      "D_e : 4.831, D_real_e : 4.938, D_fake_e : 3.373, G_e : 3.450, new_measure : 5.120, k_curr : 0.045345\n",
      "D_e : 4.744, D_real_e : 4.881, D_fake_e : 3.345, G_e : 3.422, new_measure : 5.064, k_curr : 0.030467\n",
      "D_e : 4.710, D_real_e : 4.840, D_fake_e : 3.307, G_e : 3.385, new_measure : 5.023, k_curr : 0.038473\n",
      "D_e : 4.673, D_real_e : 4.793, D_fake_e : 3.274, G_e : 3.353, new_measure : 4.980, k_curr : 0.044483\n",
      "D_e : 4.613, D_real_e : 4.734, D_fake_e : 3.240, G_e : 3.313, new_measure : 4.906, k_curr : 0.047061\n",
      "D_e : 4.621, D_real_e : 4.691, D_fake_e : 3.213, G_e : 3.290, new_measure : 4.878, k_curr : 0.030095\n",
      "D_e : 4.539, D_real_e : 4.682, D_fake_e : 3.184, G_e : 3.267, new_measure : 4.854, k_curr : 0.059515\n",
      "D_e : 4.536, D_real_e : 4.653, D_fake_e : 3.195, G_e : 3.270, new_measure : 4.816, k_curr : 0.023361\n",
      "D_e : 4.473, D_real_e : 4.605, D_fake_e : 3.137, G_e : 3.218, new_measure : 4.771, k_curr : 0.039410\n",
      "D_e : 4.431, D_real_e : 4.578, D_fake_e : 3.123, G_e : 3.206, new_measure : 4.741, k_curr : 0.035215\n",
      "D_e : 4.429, D_real_e : 4.553, D_fake_e : 3.099, G_e : 3.185, new_measure : 4.716, k_curr : 0.039604\n",
      "D_e : 4.389, D_real_e : 4.508, D_fake_e : 3.080, G_e : 3.161, new_measure : 4.673, k_curr : 0.023359\n",
      "D_e : 4.346, D_real_e : 4.478, D_fake_e : 3.045, G_e : 3.129, new_measure : 4.633, k_curr : 0.038853\n",
      "D_e : 4.360, D_real_e : 4.455, D_fake_e : 3.038, G_e : 3.115, new_measure : 4.629, k_curr : 0.048273\n",
      "D_e : 4.287, D_real_e : 4.428, D_fake_e : 3.024, G_e : 3.105, new_measure : 4.575, k_curr : 0.032932\n",
      "D_e : 4.253, D_real_e : 4.396, D_fake_e : 2.987, G_e : 3.074, new_measure : 4.550, k_curr : 0.042901\n",
      "D_e : 4.248, D_real_e : 4.380, D_fake_e : 2.988, G_e : 3.069, new_measure : 4.537, k_curr : 0.035092\n",
      "D_e : 4.247, D_real_e : 4.347, D_fake_e : 2.962, G_e : 3.035, new_measure : 4.507, k_curr : 0.057922\n",
      "D_e : 4.187, D_real_e : 4.327, D_fake_e : 2.961, G_e : 3.035, new_measure : 4.486, k_curr : 0.040020\n",
      "D_e : 4.215, D_real_e : 4.308, D_fake_e : 2.940, G_e : 3.016, new_measure : 4.466, k_curr : 0.037993\n",
      "D_e : 4.149, D_real_e : 4.287, D_fake_e : 2.919, G_e : 2.995, new_measure : 4.424, k_curr : 0.054645\n",
      "D_e : 4.111, D_real_e : 4.253, D_fake_e : 2.911, G_e : 2.983, new_measure : 4.406, k_curr : 0.036864\n",
      "D_e : 4.125, D_real_e : 4.242, D_fake_e : 2.893, G_e : 2.970, new_measure : 4.391, k_curr : 0.034587\n",
      "D_e : 4.111, D_real_e : 4.241, D_fake_e : 2.883, G_e : 2.965, new_measure : 4.379, k_curr : 0.044018\n",
      "D_e : 4.090, D_real_e : 4.199, D_fake_e : 2.868, G_e : 2.942, new_measure : 4.364, k_curr : 0.037235\n",
      "D_e : 4.049, D_real_e : 4.194, D_fake_e : 2.851, G_e : 2.926, new_measure : 4.338, k_curr : 0.063484\n",
      "D_e : 4.048, D_real_e : 4.174, D_fake_e : 2.853, G_e : 2.925, new_measure : 4.318, k_curr : 0.053560\n",
      "D_e : 4.027, D_real_e : 4.144, D_fake_e : 2.833, G_e : 2.904, new_measure : 4.295, k_curr : 0.046473\n",
      "D_e : 4.017, D_real_e : 4.123, D_fake_e : 2.800, G_e : 2.885, new_measure : 4.264, k_curr : 0.048462\n",
      "D_e : 3.992, D_real_e : 4.109, D_fake_e : 2.801, G_e : 2.877, new_measure : 4.254, k_curr : 0.047706\n",
      "D_e : 3.954, D_real_e : 4.095, D_fake_e : 2.790, G_e : 2.867, new_measure : 4.235, k_curr : 0.047163\n",
      "D_e : 3.957, D_real_e : 4.081, D_fake_e : 2.793, G_e : 2.860, new_measure : 4.222, k_curr : 0.037074\n",
      "D_e : 3.942, D_real_e : 4.050, D_fake_e : 2.765, G_e : 2.834, new_measure : 4.184, k_curr : 0.041648\n",
      "D_e : 3.925, D_real_e : 4.044, D_fake_e : 2.752, G_e : 2.830, new_measure : 4.192, k_curr : 0.045321\n",
      "D_e : 3.914, D_real_e : 4.035, D_fake_e : 2.742, G_e : 2.823, new_measure : 4.176, k_curr : 0.049278\n",
      "D_e : 3.892, D_real_e : 4.016, D_fake_e : 2.733, G_e : 2.813, new_measure : 4.173, k_curr : 0.045106\n",
      "D_e : 3.899, D_real_e : 4.019, D_fake_e : 2.726, G_e : 2.803, new_measure : 4.157, k_curr : 0.073451\n",
      "D_e : 3.883, D_real_e : 3.992, D_fake_e : 2.728, G_e : 2.808, new_measure : 4.127, k_curr : 0.034895\n",
      "D_e : 3.838, D_real_e : 3.969, D_fake_e : 2.696, G_e : 2.766, new_measure : 4.104, k_curr : 0.069705\n",
      "D_e : 3.852, D_real_e : 3.969, D_fake_e : 2.710, G_e : 2.788, new_measure : 4.109, k_curr : 0.042324\n",
      "D_e : 3.831, D_real_e : 3.951, D_fake_e : 2.688, G_e : 2.760, new_measure : 4.082, k_curr : 0.058431\n",
      "D_e : 3.813, D_real_e : 3.923, D_fake_e : 2.682, G_e : 2.752, new_measure : 4.052, k_curr : 0.041400\n",
      "D_e : 3.813, D_real_e : 3.926, D_fake_e : 2.672, G_e : 2.751, new_measure : 4.057, k_curr : 0.035016\n",
      "D_e : 3.796, D_real_e : 3.909, D_fake_e : 2.653, G_e : 2.732, new_measure : 4.040, k_curr : 0.048561\n",
      "D_e : 3.774, D_real_e : 3.885, D_fake_e : 2.644, G_e : 2.718, new_measure : 4.015, k_curr : 0.051487\n",
      "D_e : 3.764, D_real_e : 3.885, D_fake_e : 2.655, G_e : 2.725, new_measure : 4.022, k_curr : 0.037218\n",
      "D_e : 3.780, D_real_e : 3.873, D_fake_e : 2.635, G_e : 2.706, new_measure : 3.995, k_curr : 0.050707\n",
      "D_e : 3.759, D_real_e : 3.861, D_fake_e : 2.628, G_e : 2.705, new_measure : 3.989, k_curr : 0.042192\n",
      "D_e : 3.723, D_real_e : 3.830, D_fake_e : 2.612, G_e : 2.682, new_measure : 3.955, k_curr : 0.040377\n",
      "D_e : 3.699, D_real_e : 3.818, D_fake_e : 2.601, G_e : 2.665, new_measure : 3.939, k_curr : 0.061848\n",
      "D_e : 3.716, D_real_e : 3.831, D_fake_e : 2.607, G_e : 2.684, new_measure : 3.954, k_curr : 0.054014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 3.708, D_real_e : 3.821, D_fake_e : 2.602, G_e : 2.678, new_measure : 3.956, k_curr : 0.044188\n",
      "D_e : 3.675, D_real_e : 3.805, D_fake_e : 2.593, G_e : 2.663, new_measure : 3.926, k_curr : 0.044682\n",
      "D_e : 3.674, D_real_e : 3.784, D_fake_e : 2.590, G_e : 2.655, new_measure : 3.912, k_curr : 0.026100\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            \n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e = sess.run([G_optim, G_loss], {u : u_, z : z_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "                 \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, new_measure : %.3f, k_curr : %3f'%(np.mean(D_error), np.mean(D_real_error),\n",
    "            np.mean(D_fake_error), np.mean(G_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "    for i in range(1000000) :\n",
    "        z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "        \n",
    "        _ , E_e = sess.run([E_optim, E_loss], {u : u_, z : z_,isTrain : True})\n",
    "        \n",
    "    r = sess.run([re_image],feed_dict={u : test_normal_data[0:16],isTrain : False})        \n",
    "    mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/origin_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "    r = sess.run([re_image],feed_dict={u : test_anomalous_data[0:16],isTrain : False})        \n",
    "    mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/anomlous_{}.png'.format(str(epoch).zfill(3)))   \n",
    "        \n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
