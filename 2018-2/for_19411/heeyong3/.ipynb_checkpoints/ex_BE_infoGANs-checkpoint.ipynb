{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum DCGANs example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_DCGANs'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     80
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def G(x,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "\n",
    "        conv1 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_epoch = 100\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.001\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,name='G_sample') # G(z)\n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "    \n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss, var_list=G_vars, name='G_optim')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -432.286835, D_real_e : 62.150255, D_fake_e : 42.473082, G_e : 42.906395, nre_measure : 73.812303, k_curr : 1.677592\n",
      "D_e : 30.114376, D_real_e : 47.849579, D_fake_e : 33.378682, G_e : 33.856579, nre_measure : 50.413163, k_curr : 0.664343\n",
      "D_e : 26.811472, D_real_e : 38.674641, D_fake_e : 26.543383, G_e : 27.201057, nre_measure : 39.616884, k_curr : 0.303679\n",
      "D_e : 26.064808, D_real_e : 32.509228, D_fake_e : 22.278909, G_e : 22.781961, nre_measure : 33.589449, k_curr : 0.232275\n",
      "D_e : 23.831614, D_real_e : 27.967891, D_fake_e : 19.265283, G_e : 19.651110, nre_measure : 29.217746, k_curr : 0.026235\n",
      "D_e : 21.428505, D_real_e : 24.237637, D_fake_e : 16.620976, G_e : 16.904099, nre_measure : 25.401349, k_curr : 0.200526\n",
      "D_e : 19.499104, D_real_e : 20.927143, D_fake_e : 14.549443, G_e : 14.729532, nre_measure : 22.285002, k_curr : -0.024966\n",
      "D_e : 17.638784, D_real_e : 18.346062, D_fake_e : 12.673973, G_e : 12.811505, nre_measure : 19.627904, k_curr : 0.061102\n",
      "D_e : 15.581170, D_real_e : 16.009656, D_fake_e : 11.022538, G_e : 11.158907, nre_measure : 17.250271, k_curr : 0.195090\n",
      "D_e : 13.815989, D_real_e : 14.237977, D_fake_e : 9.890053, G_e : 9.985900, nre_measure : 15.331168, k_curr : 0.141004\n",
      "D_e : 12.288892, D_real_e : 12.518930, D_fake_e : 8.688355, G_e : 8.773153, nre_measure : 13.451129, k_curr : 0.113281\n",
      "D_e : 11.080950, D_real_e : 11.313852, D_fake_e : 7.835430, G_e : 7.934005, nre_measure : 12.254666, k_curr : 0.073216\n",
      "D_e : 10.310847, D_real_e : 10.224675, D_fake_e : 7.153289, G_e : 7.200315, nre_measure : 11.038713, k_curr : -0.047304\n",
      "D_e : 9.206212, D_real_e : 9.575509, D_fake_e : 6.620315, G_e : 6.698533, nre_measure : 10.164717, k_curr : -0.035196\n",
      "D_e : 8.600276, D_real_e : 8.862367, D_fake_e : 6.079402, G_e : 6.158548, nre_measure : 9.360785, k_curr : 0.091107\n",
      "D_e : 8.092880, D_real_e : 8.260204, D_fake_e : 5.688847, G_e : 5.769271, nre_measure : 8.754805, k_curr : 0.127147\n",
      "D_e : 7.638176, D_real_e : 7.769541, D_fake_e : 5.398359, G_e : 5.462880, nre_measure : 8.200293, k_curr : 0.059382\n",
      "D_e : 7.233134, D_real_e : 7.461769, D_fake_e : 5.130219, G_e : 5.222762, nre_measure : 7.801423, k_curr : 0.060717\n",
      "D_e : 6.678563, D_real_e : 6.870204, D_fake_e : 4.710085, G_e : 4.802316, nre_measure : 7.140108, k_curr : 0.079833\n",
      "D_e : 6.173425, D_real_e : 6.387696, D_fake_e : 4.393164, G_e : 4.481534, nre_measure : 6.625229, k_curr : 0.051421\n",
      "D_e : 5.894107, D_real_e : 6.036700, D_fake_e : 4.153844, G_e : 4.231149, nre_measure : 6.270226, k_curr : 0.036136\n",
      "D_e : 5.712229, D_real_e : 5.896378, D_fake_e : 4.042064, G_e : 4.131088, nre_measure : 6.098687, k_curr : 0.025990\n",
      "D_e : 5.558855, D_real_e : 5.697656, D_fake_e : 3.909432, G_e : 3.985049, nre_measure : 5.890896, k_curr : 0.035258\n",
      "D_e : 5.435113, D_real_e : 5.602600, D_fake_e : 3.822553, G_e : 3.914414, nre_measure : 5.814539, k_curr : 0.055993\n",
      "D_e : 5.334450, D_real_e : 5.452973, D_fake_e : 3.749079, G_e : 3.821911, nre_measure : 5.662135, k_curr : 0.042468\n",
      "D_e : 5.230403, D_real_e : 5.380640, D_fake_e : 3.669728, G_e : 3.758717, nre_measure : 5.586662, k_curr : 0.064114\n",
      "D_e : 5.167504, D_real_e : 5.292862, D_fake_e : 3.610980, G_e : 3.699748, nre_measure : 5.490449, k_curr : 0.078830\n",
      "D_e : 5.028605, D_real_e : 5.220481, D_fake_e : 3.584324, G_e : 3.674659, nre_measure : 5.395736, k_curr : 0.021927\n",
      "D_e : 5.037411, D_real_e : 5.174503, D_fake_e : 3.525317, G_e : 3.616413, nre_measure : 5.368872, k_curr : 0.037994\n",
      "D_e : 4.920450, D_real_e : 5.078331, D_fake_e : 3.462350, G_e : 3.552289, nre_measure : 5.264793, k_curr : 0.045114\n",
      "D_e : 4.883649, D_real_e : 5.037574, D_fake_e : 3.432816, G_e : 3.523636, nre_measure : 5.233425, k_curr : 0.052578\n",
      "D_e : 4.824151, D_real_e : 4.967328, D_fake_e : 3.395176, G_e : 3.478801, nre_measure : 5.148851, k_curr : 0.047898\n",
      "D_e : 4.815368, D_real_e : 4.942049, D_fake_e : 3.365695, G_e : 3.460706, nre_measure : 5.129208, k_curr : 0.044336\n",
      "D_e : 4.715302, D_real_e : 4.877877, D_fake_e : 3.327482, G_e : 3.418114, nre_measure : 5.041142, k_curr : 0.034258\n",
      "D_e : 4.705146, D_real_e : 4.839071, D_fake_e : 3.300229, G_e : 3.388420, nre_measure : 5.005071, k_curr : 0.031260\n",
      "D_e : 4.631701, D_real_e : 4.792402, D_fake_e : 3.261762, G_e : 3.348499, nre_measure : 4.971532, k_curr : 0.048569\n",
      "D_e : 4.588333, D_real_e : 4.722235, D_fake_e : 3.221886, G_e : 3.304833, nre_measure : 4.885111, k_curr : 0.050618\n",
      "D_e : 4.576778, D_real_e : 4.715468, D_fake_e : 3.213524, G_e : 3.301025, nre_measure : 4.886793, k_curr : 0.050066\n",
      "D_e : 4.532872, D_real_e : 4.665754, D_fake_e : 3.177600, G_e : 3.262926, nre_measure : 4.838814, k_curr : 0.058751\n",
      "D_e : 4.493452, D_real_e : 4.646295, D_fake_e : 3.168441, G_e : 3.254809, nre_measure : 4.809500, k_curr : 0.052024\n",
      "D_e : 4.444464, D_real_e : 4.603590, D_fake_e : 3.123619, G_e : 3.218060, nre_measure : 4.759378, k_curr : 0.064492\n",
      "D_e : 4.426430, D_real_e : 4.554281, D_fake_e : 3.113421, G_e : 3.196468, nre_measure : 4.715717, k_curr : 0.040772\n",
      "D_e : 4.392256, D_real_e : 4.523440, D_fake_e : 3.082080, G_e : 3.167286, nre_measure : 4.698027, k_curr : 0.038313\n",
      "D_e : 4.357221, D_real_e : 4.500965, D_fake_e : 3.053445, G_e : 3.142656, nre_measure : 4.657112, k_curr : 0.060768\n",
      "D_e : 4.309858, D_real_e : 4.484223, D_fake_e : 3.049249, G_e : 3.134529, nre_measure : 4.639148, k_curr : 0.073163\n",
      "D_e : 4.320987, D_real_e : 4.446025, D_fake_e : 3.051536, G_e : 3.126682, nre_measure : 4.607326, k_curr : 0.032662\n",
      "D_e : 4.294481, D_real_e : 4.409954, D_fake_e : 3.000205, G_e : 3.082706, nre_measure : 4.571935, k_curr : 0.044595\n",
      "D_e : 4.229996, D_real_e : 4.383345, D_fake_e : 2.991277, G_e : 3.073528, nre_measure : 4.555472, k_curr : 0.030075\n",
      "D_e : 4.233768, D_real_e : 4.368642, D_fake_e : 2.963840, G_e : 3.049975, nre_measure : 4.521025, k_curr : 0.052682\n",
      "D_e : 4.216379, D_real_e : 4.356034, D_fake_e : 2.969606, G_e : 3.053057, nre_measure : 4.508256, k_curr : 0.041948\n",
      "D_e : 4.197597, D_real_e : 4.315019, D_fake_e : 2.933764, G_e : 3.023548, nre_measure : 4.471915, k_curr : 0.033450\n",
      "D_e : 4.155633, D_real_e : 4.312553, D_fake_e : 2.933028, G_e : 3.018231, nre_measure : 4.456761, k_curr : 0.035008\n",
      "D_e : 4.152692, D_real_e : 4.267851, D_fake_e : 2.910535, G_e : 2.991641, nre_measure : 4.426207, k_curr : 0.023402\n",
      "D_e : 4.130091, D_real_e : 4.246067, D_fake_e : 2.885415, G_e : 2.968598, nre_measure : 4.393321, k_curr : 0.033620\n",
      "D_e : 4.110398, D_real_e : 4.242872, D_fake_e : 2.871191, G_e : 2.958605, nre_measure : 4.394564, k_curr : 0.065555\n",
      "D_e : 4.076375, D_real_e : 4.208150, D_fake_e : 2.865060, G_e : 2.950796, nre_measure : 4.370240, k_curr : 0.051301\n",
      "D_e : 4.088862, D_real_e : 4.210688, D_fake_e : 2.867446, G_e : 2.952188, nre_measure : 4.372173, k_curr : 0.038123\n",
      "D_e : 4.021136, D_real_e : 4.154513, D_fake_e : 2.824538, G_e : 2.904503, nre_measure : 4.296979, k_curr : 0.048361\n",
      "D_e : 4.032596, D_real_e : 4.137559, D_fake_e : 2.817873, G_e : 2.898290, nre_measure : 4.279158, k_curr : 0.042763\n",
      "D_e : 4.018905, D_real_e : 4.145455, D_fake_e : 2.823000, G_e : 2.904391, nre_measure : 4.296241, k_curr : 0.035562\n",
      "D_e : 3.992691, D_real_e : 4.119506, D_fake_e : 2.798431, G_e : 2.878016, nre_measure : 4.260842, k_curr : 0.051350\n",
      "D_e : 3.988741, D_real_e : 4.098520, D_fake_e : 2.788063, G_e : 2.867520, nre_measure : 4.245811, k_curr : 0.055393\n",
      "D_e : 3.969721, D_real_e : 4.083957, D_fake_e : 2.783912, G_e : 2.863641, nre_measure : 4.228962, k_curr : 0.041754\n",
      "D_e : 3.964693, D_real_e : 4.068124, D_fake_e : 2.760506, G_e : 2.841173, nre_measure : 4.214809, k_curr : 0.059993\n",
      "D_e : 3.932750, D_real_e : 4.047207, D_fake_e : 2.761050, G_e : 2.841546, nre_measure : 4.178861, k_curr : 0.036190\n",
      "D_e : 3.925977, D_real_e : 4.030010, D_fake_e : 2.745545, G_e : 2.820401, nre_measure : 4.169521, k_curr : 0.037886\n",
      "D_e : 3.899542, D_real_e : 4.019978, D_fake_e : 2.730939, G_e : 2.807709, nre_measure : 4.148955, k_curr : 0.055456\n",
      "D_e : 3.876956, D_real_e : 3.986681, D_fake_e : 2.713694, G_e : 2.791890, nre_measure : 4.121256, k_curr : 0.052058\n",
      "D_e : 3.878994, D_real_e : 3.988570, D_fake_e : 2.708354, G_e : 2.790633, nre_measure : 4.137063, k_curr : 0.055883\n",
      "D_e : 3.867437, D_real_e : 3.979334, D_fake_e : 2.704922, G_e : 2.789056, nre_measure : 4.132816, k_curr : 0.046021\n",
      "D_e : 3.860317, D_real_e : 3.947697, D_fake_e : 2.684739, G_e : 2.765736, nre_measure : 4.098088, k_curr : 0.039444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 3.836349, D_real_e : 3.946985, D_fake_e : 2.688452, G_e : 2.764888, nre_measure : 4.095105, k_curr : 0.033849\n",
      "D_e : 3.824446, D_real_e : 3.916403, D_fake_e : 2.670891, G_e : 2.742170, nre_measure : 4.058151, k_curr : 0.031923\n",
      "D_e : 3.813382, D_real_e : 3.907304, D_fake_e : 2.667120, G_e : 2.736979, nre_measure : 4.039294, k_curr : 0.026696\n",
      "D_e : 3.788704, D_real_e : 3.897010, D_fake_e : 2.658836, G_e : 2.727558, nre_measure : 4.036994, k_curr : 0.027674\n",
      "D_e : 3.775998, D_real_e : 3.859026, D_fake_e : 2.635490, G_e : 2.695930, nre_measure : 3.976331, k_curr : 0.042759\n",
      "D_e : 3.776897, D_real_e : 3.868565, D_fake_e : 2.625512, G_e : 2.709754, nre_measure : 4.005148, k_curr : 0.037838\n",
      "D_e : 3.739236, D_real_e : 3.857619, D_fake_e : 2.627132, G_e : 2.695128, nre_measure : 4.002892, k_curr : 0.052414\n",
      "D_e : 3.746586, D_real_e : 3.845087, D_fake_e : 2.629654, G_e : 2.698992, nre_measure : 3.986143, k_curr : 0.031607\n",
      "D_e : 3.715900, D_real_e : 3.817916, D_fake_e : 2.596226, G_e : 2.669538, nre_measure : 3.959109, k_curr : 0.040015\n",
      "D_e : 3.701741, D_real_e : 3.804883, D_fake_e : 2.599716, G_e : 2.668156, nre_measure : 3.933571, k_curr : 0.026748\n",
      "D_e : 3.688522, D_real_e : 3.794724, D_fake_e : 2.580515, G_e : 2.651148, nre_measure : 3.923105, k_curr : 0.041192\n",
      "D_e : 3.692469, D_real_e : 3.784094, D_fake_e : 2.571900, G_e : 2.651083, nre_measure : 3.911934, k_curr : 0.034986\n",
      "D_e : 3.651651, D_real_e : 3.770061, D_fake_e : 2.566708, G_e : 2.637668, nre_measure : 3.899714, k_curr : 0.038834\n",
      "D_e : 3.661681, D_real_e : 3.751272, D_fake_e : 2.558190, G_e : 2.620615, nre_measure : 3.877880, k_curr : 0.053605\n",
      "D_e : 3.630749, D_real_e : 3.747461, D_fake_e : 2.562566, G_e : 2.629810, nre_measure : 3.875217, k_curr : 0.035159\n",
      "D_e : 3.655174, D_real_e : 3.727687, D_fake_e : 2.529275, G_e : 2.602715, nre_measure : 3.870514, k_curr : 0.036703\n",
      "D_e : 3.617224, D_real_e : 3.702104, D_fake_e : 2.527950, G_e : 2.592467, nre_measure : 3.839161, k_curr : 0.033919\n",
      "D_e : 3.611961, D_real_e : 3.718786, D_fake_e : 2.528076, G_e : 2.604734, nre_measure : 3.846174, k_curr : 0.029484\n",
      "D_e : 3.601068, D_real_e : 3.678916, D_fake_e : 2.509620, G_e : 2.574777, nre_measure : 3.804391, k_curr : 0.030785\n",
      "D_e : 3.600209, D_real_e : 3.674142, D_fake_e : 2.500078, G_e : 2.567050, nre_measure : 3.801811, k_curr : 0.044366\n",
      "D_e : 3.582220, D_real_e : 3.673841, D_fake_e : 2.512599, G_e : 2.576821, nre_measure : 3.801356, k_curr : 0.029995\n",
      "D_e : 3.565449, D_real_e : 3.650414, D_fake_e : 2.490862, G_e : 2.553247, nre_measure : 3.763844, k_curr : 0.035713\n",
      "D_e : 3.561342, D_real_e : 3.637711, D_fake_e : 2.489226, G_e : 2.547131, nre_measure : 3.758949, k_curr : 0.033659\n",
      "D_e : 3.550628, D_real_e : 3.635003, D_fake_e : 2.475762, G_e : 2.540876, nre_measure : 3.757447, k_curr : 0.043812\n",
      "D_e : 3.537429, D_real_e : 3.613119, D_fake_e : 2.471277, G_e : 2.535980, nre_measure : 3.737653, k_curr : 0.024783\n",
      "D_e : 3.546834, D_real_e : 3.603697, D_fake_e : 2.455069, G_e : 2.514910, nre_measure : 3.734863, k_curr : 0.046281\n",
      "D_e : 3.513563, D_real_e : 3.608057, D_fake_e : 2.457995, G_e : 2.522277, nre_measure : 3.738460, k_curr : 0.055697\n",
      "D_e : 3.516882, D_real_e : 3.595231, D_fake_e : 2.456984, G_e : 2.524732, nre_measure : 3.717373, k_curr : 0.033100\n",
      "total time :  11403.741495847702\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f960511e278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            \n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e = sess.run([G_optim, G_loss], {u : u_, z : z_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "        \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f, new_measure : %.6f, k_curr : %6f'%(np.mean(D_error), np.mean(D_real_error),\n",
    "            np.mean(D_fake_error), np.mean(G_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "\n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_DCGANs/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21484614181518552,\n",
       " 0.4403569145202636,\n",
       " 0.67283201980590812,\n",
       " 0.90988243103027333,\n",
       " 1.1515795135498046,\n",
       " 1.3960924949645994,\n",
       " 1.6439387626647948,\n",
       " 1.8962808227539061,\n",
       " 2.1526349487304683,\n",
       " 2.4081364288330076,\n",
       " 2.6667501792907711,\n",
       " 2.9254244461059566,\n",
       " 3.1846267471313472,\n",
       " 3.4445239830017087,\n",
       " 3.7024795341491696,\n",
       " 3.9612258186340328,\n",
       " 4.2182501487731932,\n",
       " 4.4745885124206541,\n",
       " 4.7296576919555662,\n",
       " 4.981740070343017,\n",
       " 5.2327743072509758,\n",
       " 5.4828489112853998,\n",
       " 5.7307616577148428,\n",
       " 5.9779571952819817,\n",
       " 6.2241046218872063,\n",
       " 6.4677043190002435,\n",
       " 6.7116811866760244,\n",
       " 6.9505331687927239,\n",
       " 7.1866796569824212,\n",
       " 7.4222404632568351,\n",
       " 7.6567484703063959,\n",
       " 7.8877649650573725,\n",
       " 8.1170873947143551,\n",
       " 8.3447176856994627,\n",
       " 8.5716341323852543,\n",
       " 8.7959032554626475,\n",
       " 9.0193061828613299,\n",
       " 9.2415335617065448,\n",
       " 9.4633621520996112,\n",
       " 9.680485565185549,\n",
       " 9.8964322090148951,\n",
       " 10.111449237823489,\n",
       " 10.323951278686526,\n",
       " 10.535109203338624,\n",
       " 10.74454905319214,\n",
       " 10.950949966430665,\n",
       " 11.155376346588136,\n",
       " 11.358644523620606,\n",
       " 11.562643222808839,\n",
       " 11.76284022140503,\n",
       " 11.961146541595459,\n",
       " 12.15750156402588,\n",
       " 12.353137588500978,\n",
       " 12.545202503204347,\n",
       " 12.735409820556642,\n",
       " 12.921662582397461,\n",
       " 13.107712020874024,\n",
       " 13.290375011444093,\n",
       " 13.472818145751955,\n",
       " 13.649790241241456,\n",
       " 13.823445034027101,\n",
       " 13.988795444488526,\n",
       " 14.155032123565674,\n",
       " 14.31576826095581,\n",
       " 14.461542472839355,\n",
       " 14.601605648040771,\n",
       " 14.728198093414306,\n",
       " 14.844869434356688,\n",
       " 14.952761199951171,\n",
       " 15.055551631927489,\n",
       " 15.148433948516844,\n",
       " 15.243609115600584,\n",
       " 15.329831359863279,\n",
       " 15.429798969268797,\n",
       " 15.533275905609129,\n",
       " 15.630873760223386,\n",
       " 15.723403148651121,\n",
       " 15.804844669342039,\n",
       " 15.878794185638425,\n",
       " 15.952486415863033,\n",
       " 16.028914043426511,\n",
       " 16.100822139739986,\n",
       " 16.169967082977291,\n",
       " 16.242811885833735,\n",
       " 16.316656734466548,\n",
       " 16.37853360366821,\n",
       " 16.451983020782468,\n",
       " 16.510502079010006,\n",
       " 16.562924968719479,\n",
       " 16.613160106658931,\n",
       " 16.662737270355219,\n",
       " 16.704780544281,\n",
       " 16.754544612884516,\n",
       " 16.795923145294182,\n",
       " 16.837175617218008,\n",
       " 16.864302211761466,\n",
       " 16.906349330902092,\n",
       " 16.940312152862543,\n",
       " 16.978816852569572,\n",
       " 17.013001735687247,\n",
       " 17.049048618316643,\n",
       " 17.072673603057854,\n",
       " 17.103426708221427,\n",
       " 17.125153820037834,\n",
       " 17.139241489410391,\n",
       " 17.141259403228752,\n",
       " 17.165331150054925,\n",
       " 17.164855648040763,\n",
       " 17.150876628875725,\n",
       " 17.124788692474358,\n",
       " 17.109166042327875,\n",
       " 17.088451282501214,\n",
       " 17.067853069305414,\n",
       " 17.051835643768303,\n",
       " 17.052667507171623,\n",
       " 17.038754833221429,\n",
       " 17.013522411346429,\n",
       " 17.008356800079341,\n",
       " 17.000120281219477,\n",
       " 16.99291276931762,\n",
       " 16.97598863601684,\n",
       " 16.940711040496822,\n",
       " 16.908347476959225,\n",
       " 16.868439304351803,\n",
       " 16.839951511383052,\n",
       " 16.801902416229243,\n",
       " 16.771452426910397,\n",
       " 16.730626766204832,\n",
       " 16.711339870452878,\n",
       " 16.681757511138912,\n",
       " 16.642832675933832,\n",
       " 16.617650066375727,\n",
       " 16.579524158477778,\n",
       " 16.553752765655513,\n",
       " 16.506681217193599,\n",
       " 16.455203388214105,\n",
       " 16.417815036773675,\n",
       " 16.385506771087641,\n",
       " 16.341197658538814,\n",
       " 16.283251022338863,\n",
       " 16.256006553649897,\n",
       " 16.212232696533199,\n",
       " 16.161031890869136,\n",
       " 16.106626663208004,\n",
       " 16.041970260620115,\n",
       " 15.962458671569822,\n",
       " 15.89195951080322,\n",
       " 15.816614715576168,\n",
       " 15.745602203369137,\n",
       " 15.677665786743161,\n",
       " 15.597478637695309,\n",
       " 15.551075119018551,\n",
       " 15.488632720947262,\n",
       " 15.419194446563717,\n",
       " 15.354611232757565,\n",
       " 15.306248523712155,\n",
       " 15.24640119552612,\n",
       " 15.174872020721432,\n",
       " 15.109015308380124,\n",
       " 15.033466236114499,\n",
       " 14.950546306610105,\n",
       " 14.871859775543211,\n",
       " 14.797692638397214,\n",
       " 14.72209468078613,\n",
       " 14.650257095336912,\n",
       " 14.577376651763913,\n",
       " 14.49731929016113,\n",
       " 14.422589244842527,\n",
       " 14.370249134063718,\n",
       " 14.309644351959225,\n",
       " 14.256085842132565,\n",
       " 14.207467021942135,\n",
       " 14.168495792388912,\n",
       " 14.110208705902096,\n",
       " 14.043125331878658,\n",
       " 13.97094733047485,\n",
       " 13.900270980834957,\n",
       " 13.836576210021969,\n",
       " 13.774064163208005,\n",
       " 13.709513031005857,\n",
       " 13.641324394226071,\n",
       " 13.570199409484861,\n",
       " 13.495886512756345,\n",
       " 13.411630916595456,\n",
       " 13.329787883758542,\n",
       " 13.25695759963989,\n",
       " 13.17029971694946,\n",
       " 13.082184974670406,\n",
       " 12.956782844543453,\n",
       " 12.839823387145991,\n",
       " 12.734635398864741,\n",
       " 12.63364501190185,\n",
       " 12.529210681915277,\n",
       " 12.439654514312737,\n",
       " 12.347523712158196,\n",
       " 12.25410398101806,\n",
       " 12.154052230834955,\n",
       " 12.048529018402093,\n",
       " 11.946726291656487,\n",
       " 11.85260065841674,\n",
       " 11.754090362548821,\n",
       " 11.649447463989251,\n",
       " 11.552612930297844,\n",
       " 11.455532413482658,\n",
       " 11.349809909820548,\n",
       " 11.252119262695304,\n",
       " 11.151929199218742,\n",
       " 11.049583137512199,\n",
       " 10.953395809173577,\n",
       " 10.841049541473382,\n",
       " 10.743443176269524,\n",
       " 10.627552631378167,\n",
       " 10.518110042572015,\n",
       " 10.402532764434808,\n",
       " 10.286883853912347,\n",
       " 10.164977619171136,\n",
       " 10.059493030548088,\n",
       " 9.942897949218743,\n",
       " 9.8328977851867609,\n",
       " 9.7279936027526794,\n",
       " 9.6103840370178162,\n",
       " 9.4937534370422298,\n",
       " 9.3706586074829037,\n",
       " 9.2579715156555107,\n",
       " 9.1502998809814375,\n",
       " 9.0337274589538499,\n",
       " 8.908541957855217,\n",
       " 8.788192432403557,\n",
       " 8.6687917022705001,\n",
       " 8.5438764266967695,\n",
       " 8.4294537925720139,\n",
       " 8.3106181221008217,\n",
       " 8.1972589111328045,\n",
       " 8.0781049385070727,\n",
       " 7.9729185752868581,\n",
       " 7.8561152153015064,\n",
       " 7.7352416839599538,\n",
       " 7.6243398094177177,\n",
       " 7.5016198616027765,\n",
       " 7.3749761390685968,\n",
       " 7.2421388092040946,\n",
       " 7.1201142387390064,\n",
       " 6.9916969223022392,\n",
       " 6.8598098220825126,\n",
       " 6.7276417541503832,\n",
       " 6.5832146797180098,\n",
       " 6.4385512313842694,\n",
       " 6.2906312789916914,\n",
       " 6.147584510803215,\n",
       " 6.0057288055419846,\n",
       " 5.858293319702141,\n",
       " 5.7162671623229908,\n",
       " 5.5795349502563401,\n",
       " 5.4427736663818287,\n",
       " 5.2925600547790452,\n",
       " 5.1566617507934494,\n",
       " 5.0203000640869062,\n",
       " 4.8765485992431561,\n",
       " 4.7322057800292887,\n",
       " 4.5977050781249922,\n",
       " 4.4519033775329513,\n",
       " 4.3217223243713301,\n",
       " 4.1782006111144945,\n",
       " 4.0236072731017991,\n",
       " 3.8762488365173264,\n",
       " 3.7292499084472581,\n",
       " 3.5983554267883227,\n",
       " 3.4555596427917408,\n",
       " 3.3190112113952566,\n",
       " 3.1766009330749441,\n",
       " 3.0274449501037526,\n",
       " 2.8661020622253348,\n",
       " 2.6974346885681082,\n",
       " 2.5416604385375905,\n",
       " 2.3880836601257251,\n",
       " 2.240458343505852,\n",
       " 2.1028378639221117,\n",
       " 1.955375511169426,\n",
       " 1.8138351364135665,\n",
       " 1.6775921669006271,\n",
       " 1.5279128303527756,\n",
       " 1.387801177978508,\n",
       " 1.2528984260559004,\n",
       " 1.1036295204162518,\n",
       " 0.95137752151488464,\n",
       " 0.80553017807006033,\n",
       " 0.64656497955321457,\n",
       " 0.48805632019042156,\n",
       " 0.32889751434325354,\n",
       " 0.17286773300170075,\n",
       " 0.025175415039054266,\n",
       " -0.1251706199646079,\n",
       " -0.2771521453857505,\n",
       " -0.42039547348023298,\n",
       " -0.54498088455201033,\n",
       " -0.70535803985596546,\n",
       " -0.82418952941895374,\n",
       " -0.89825661087036979,\n",
       " -0.97492314147950077,\n",
       " -1.0654376678466884,\n",
       " -1.1092171745300379,\n",
       " -1.1349894523620692,\n",
       " -1.1491980323791591,\n",
       " -1.1612815246582118,\n",
       " -1.1420739822387782,\n",
       " -1.1147469635009852,\n",
       " -1.0773867492675868,\n",
       " -1.0264027328491299,\n",
       " -0.97177143478394434,\n",
       " -0.90953722381592672,\n",
       " -0.83968193817139558,\n",
       " -0.76546591186524326,\n",
       " -0.69346485519410073,\n",
       " -0.61264471817017496,\n",
       " -0.52044654464722573,\n",
       " -0.41124243545533123,\n",
       " -0.31202142715455,\n",
       " -0.20564194488526294,\n",
       " -0.096913566589364522,\n",
       " 0.020062198638906958,\n",
       " 0.12985798263548898,\n",
       " 0.24721946334837958,\n",
       " 0.36490182876586003,\n",
       " 0.47143527221678772,\n",
       " 0.58192721557616267,\n",
       " 0.69224439239501034,\n",
       " 0.79370853042601619,\n",
       " 0.88287944793700246,\n",
       " 0.94608919525145563,\n",
       " 0.9838444786071685,\n",
       " 0.99995565795897512,\n",
       " 0.99470781326293012,\n",
       " 0.9933173294067289,\n",
       " 0.98364633178709993,\n",
       " 0.97220682525633817,\n",
       " 0.96085541915892603,\n",
       " 0.95483867645262721,\n",
       " 0.95542814254759789,\n",
       " 0.94105111312865253,\n",
       " 0.92887680053709976,\n",
       " 0.91488452529906261,\n",
       " 0.8976550483703517,\n",
       " 0.88979958343504895,\n",
       " 0.87247217941283217,\n",
       " 0.866303993225088,\n",
       " 0.85375481414793952,\n",
       " 0.85615885162352545,\n",
       " 0.85244766616820322,\n",
       " 0.84426606369017587,\n",
       " 0.84506966018675789,\n",
       " 0.8540376358032129,\n",
       " 0.85785454559325192,\n",
       " 0.86822648239134759,\n",
       " 0.87274934387206049,\n",
       " 0.87159820556639644,\n",
       " 0.87600371932982413,\n",
       " 0.88318530273436513,\n",
       " 0.89026633834837876,\n",
       " 0.88767112350462873,\n",
       " 0.8822207794189354,\n",
       " 0.88141980361937478,\n",
       " 0.8815790519714255,\n",
       " 0.87243959045409147,\n",
       " 0.87184078979491175,\n",
       " 0.85238117218016562,\n",
       " 0.84407402420042932,\n",
       " 0.84713726425169888,\n",
       " 0.8409041099548239,\n",
       " 0.83398581695555629,\n",
       " 0.82963468551634723,\n",
       " 0.82303969955443312,\n",
       " 0.80846809387206009,\n",
       " 0.79632968902586865,\n",
       " 0.79243228912352492,\n",
       " 0.76846864700316364,\n",
       " 0.75355802154539986,\n",
       " 0.75992867279051701,\n",
       " 0.75258752059935485,\n",
       " 0.74577535629271419,\n",
       " 0.74175349426268489,\n",
       " 0.73738092422484303,\n",
       " 0.71747690963744071,\n",
       " 0.70367349243163013,\n",
       " 0.70111456680296802,\n",
       " 0.7008095741271867,\n",
       " 0.68536313629149326,\n",
       " 0.66649267959593661,\n",
       " 0.65304331207274324,\n",
       " 0.62751081085204008,\n",
       " 0.62748535919188375,\n",
       " 0.62496457672118055,\n",
       " 0.62380666732787005,\n",
       " 0.60570029830931527,\n",
       " 0.60987744140623901,\n",
       " 0.61739484024046754,\n",
       " 0.60837073516844609,\n",
       " 0.60003632736204959,\n",
       " 0.58716122817992067,\n",
       " 0.56972218322752799,\n",
       " 0.56436846160887566,\n",
       " 0.56110279464720569,\n",
       " 0.54802479171751817,\n",
       " 0.52575099182127794,\n",
       " 0.51413282012938344,\n",
       " 0.50241359329222524,\n",
       " 0.48653563308714709,\n",
       " 0.4837999114990123,\n",
       " 0.45816053390501815,\n",
       " 0.45622869873045757,\n",
       " 0.45969451904295761,\n",
       " 0.46762018585203963,\n",
       " 0.45553898239134627,\n",
       " 0.47302900695799666,\n",
       " 0.47415940475462748,\n",
       " 0.47588923645018411,\n",
       " 0.49089855194090676,\n",
       " 0.47423527526854348,\n",
       " 0.47278487396239111,\n",
       " 0.47450693130492039,\n",
       " 0.48307576370238126,\n",
       " 0.48827250671385586,\n",
       " 0.48743721771239101,\n",
       " 0.49708872604368987,\n",
       " 0.49732936096190272,\n",
       " 0.50403517532347497,\n",
       " 0.51616819000242997,\n",
       " 0.52516223525999828,\n",
       " 0.54286155319212714,\n",
       " 0.5516501998901252,\n",
       " 0.55585116195677553,\n",
       " 0.56640383911131653,\n",
       " 0.57757767105101376,\n",
       " 0.59628789901732226,\n",
       " 0.62520037460325972,\n",
       " 0.65384171676634562,\n",
       " 0.6704481735229374,\n",
       " 0.6823901939391972,\n",
       " 0.68882733154295694,\n",
       " 0.70813928222655065,\n",
       " 0.70527179718016386,\n",
       " 0.72284716415404082,\n",
       " 0.72547165298460725,\n",
       " 0.71178630447386504,\n",
       " 0.72876844024657017,\n",
       " 0.7401934928893924,\n",
       " 0.74177167129515409,\n",
       " 0.74321616744993924,\n",
       " 0.75513454437254668,\n",
       " 0.77258597946165797,\n",
       " 0.75711543273924586,\n",
       " 0.74665063858031033,\n",
       " 0.74369021606444119,\n",
       " 0.74809136199949977,\n",
       " 0.7530086135864138,\n",
       " 0.74357203292845475,\n",
       " 0.75026789093016377,\n",
       " 0.74724196243284935,\n",
       " 0.74057650375365014,\n",
       " 0.74669910430907005,\n",
       " 0.74463290023802509,\n",
       " 0.73862579727171651,\n",
       " 0.74788649368284932,\n",
       " 0.73010683059691184,\n",
       " 0.72056311416624774,\n",
       " 0.69919359207152121,\n",
       " 0.69661349105833759,\n",
       " 0.69273756027220479,\n",
       " 0.7083316345214723,\n",
       " 0.71298982238768327,\n",
       " 0.70568940734862073,\n",
       " 0.70431300354002702,\n",
       " 0.714955020904529,\n",
       " 0.71336926650999777,\n",
       " 0.71815242385863054,\n",
       " 0.71275697326658949,\n",
       " 0.71707802963255629,\n",
       " 0.70681489562987077,\n",
       " 0.71062366104124775,\n",
       " 0.70703276443480245,\n",
       " 0.71293066787718529,\n",
       " 0.71902947616575952,\n",
       " 0.73408176803587666,\n",
       " 0.72689441299437274,\n",
       " 0.71339441299437267,\n",
       " 0.70676991653441168,\n",
       " 0.70197402954100341,\n",
       " 0.69498802947996818,\n",
       " 0.70386943054197992,\n",
       " 0.71392633056639399,\n",
       " 0.68648480224608144,\n",
       " 0.68851869964598378,\n",
       " 0.69944677734373772,\n",
       " 0.69945110321043691,\n",
       " 0.69762722778319075,\n",
       " 0.67830690002440164,\n",
       " 0.65988579559324934,\n",
       " 0.66386390304564191,\n",
       " 0.6516870307922239,\n",
       " 0.65372171401976298,\n",
       " 0.6378504333495969,\n",
       " 0.59417170333861058,\n",
       " 0.57789350891112035,\n",
       " 0.56863818740843475,\n",
       " 0.56647196578978232,\n",
       " 0.54863342285154992,\n",
       " 0.55920775604246786,\n",
       " 0.53143358612059288,\n",
       " 0.52934694671629601,\n",
       " 0.50383140182493857,\n",
       " 0.4984478645324581,\n",
       " 0.49904608535765338,\n",
       " 0.5056321029662959,\n",
       " 0.53026861953734084,\n",
       " 0.53004176712034867,\n",
       " 0.51584623336790725,\n",
       " 0.51507962036131538,\n",
       " 0.51578535079954779,\n",
       " 0.51249135971068061,\n",
       " 0.51228602218626651,\n",
       " 0.51450914764403011,\n",
       " 0.52677602767943044,\n",
       " 0.52995408248900078,\n",
       " 0.54279378890989916,\n",
       " 0.53472765350340501,\n",
       " 0.52485855484007493,\n",
       " 0.52068713760374685,\n",
       " 0.53458350753782879,\n",
       " 0.54452775955198895,\n",
       " 0.54118686294554363,\n",
       " 0.55290627670286785,\n",
       " 0.56592780303953771,\n",
       " 0.5638299636840689,\n",
       " 0.56360899353026028,\n",
       " 0.55836548614500636,\n",
       " 0.56540332031248686,\n",
       " 0.57138911437986961,\n",
       " 0.577195129394518,\n",
       " 0.58223733901976216,\n",
       " 0.57674761199949842,\n",
       " 0.58557578659056286,\n",
       " 0.61334868240355112,\n",
       " 0.63122155761717413,\n",
       " 0.64528597640989871,\n",
       " 0.65680668640135376,\n",
       " 0.66663180923460563,\n",
       " 0.65209403991697867,\n",
       " 0.6433690986633166,\n",
       " 0.64882138061522088,\n",
       " 0.65140173721312122,\n",
       " 0.65885638046263295,\n",
       " 0.68912828063963483,\n",
       " 0.71537979507444927,\n",
       " 0.68911252593992778,\n",
       " 0.6970408401489121,\n",
       " 0.67592990493773042,\n",
       " 0.65945095443724211,\n",
       " 0.63461756515501555,\n",
       " 0.64228668975828695,\n",
       " 0.65092999649046468,\n",
       " 0.66434266281126542,\n",
       " 0.65607534408567947,\n",
       " 0.6507064514160017,\n",
       " 0.65844132614134343,\n",
       " 0.66128156280516182,\n",
       " 0.65708354949949777,\n",
       " 0.63442284393309145,\n",
       " 0.6287698593139508,\n",
       " 0.63518753433226127,\n",
       " 0.63867545700071826,\n",
       " 0.6437937736511089,\n",
       " 0.63926015090940969,\n",
       " 0.62673175430296435,\n",
       " 0.61127463531492721,\n",
       " 0.61468064498899944,\n",
       " 0.607842533111558,\n",
       " 0.60129529190062048,\n",
       " 0.59957428741453644,\n",
       " 0.59368107986448759,\n",
       " 0.59042278671263215,\n",
       " 0.57967949676512243,\n",
       " 0.56981129455564972,\n",
       " 0.56821816253660673,\n",
       " 0.57287100982664574,\n",
       " 0.56901651763914574,\n",
       " 0.5384328041076516,\n",
       " 0.52410864639280785,\n",
       " 0.50730992126463403,\n",
       " 0.49414325714109886,\n",
       " 0.50171883010862817,\n",
       " 0.50382683944700701,\n",
       " 0.49861219787596206,\n",
       " 0.49414896392820812,\n",
       " 0.47942300796507331,\n",
       " 0.4992631378173682,\n",
       " 0.5122016525268408,\n",
       " 0.50443962860105951,\n",
       " 0.50717690277098137,\n",
       " 0.51843151855467273,\n",
       " 0.51465513992308087,\n",
       " 0.5283123817443699,\n",
       " 0.52488935470579567,\n",
       " 0.53342716979978977,\n",
       " 0.52896803283689908,\n",
       " 0.52506445312498495,\n",
       " 0.52278335952757282,\n",
       " 0.53468976211546337,\n",
       " 0.52430034637449652,\n",
       " 0.49424416351316836,\n",
       " 0.50824436950682061,\n",
       " 0.50822404861448667,\n",
       " 0.51656952667234801,\n",
       " 0.52060567092893983,\n",
       " 0.52977866363523862,\n",
       " 0.5549320144653167,\n",
       " 0.56627027893064874,\n",
       " 0.55627225494383237,\n",
       " 0.55064473342893971,\n",
       " 0.56135529708860765,\n",
       " 0.56965948104856856,\n",
       " 0.55875521850584398,\n",
       " 0.5834199905395353,\n",
       " 0.57214109420774817,\n",
       " 0.55493836212156655,\n",
       " 0.55530835723875405,\n",
       " 0.55486447906492586,\n",
       " 0.53929963684080473,\n",
       " 0.51915509033201568,\n",
       " 0.52004698944090233,\n",
       " 0.49982458496092186,\n",
       " 0.50727490997312885,\n",
       " 0.50823240280149795,\n",
       " 0.50586728286741589,\n",
       " 0.4921843719482264,\n",
       " 0.46833403396604861,\n",
       " 0.43162229156492554,\n",
       " 0.42786169433592158,\n",
       " 0.42214152145384148,\n",
       " 0.42064258575437857,\n",
       " 0.43101697921751331,\n",
       " 0.42614086151121444,\n",
       " 0.42824645233152692,\n",
       " 0.42594083404539407,\n",
       " 0.42943788909910496,\n",
       " 0.42273813247679048,\n",
       " 0.42215550231931975,\n",
       " 0.42088666534422209,\n",
       " 0.41239655685423182,\n",
       " 0.40303654479978845,\n",
       " 0.38864965438841148,\n",
       " 0.39062696838377281,\n",
       " 0.37224250793455405,\n",
       " 0.37274915313719076,\n",
       " 0.372099384307845,\n",
       " 0.35948002624510084,\n",
       " 0.35767341613767895,\n",
       " 0.36504049682615547,\n",
       " 0.37282317352293276,\n",
       " 0.37372950363157531,\n",
       " 0.3644824447631671,\n",
       " 0.36925805282591123,\n",
       " 0.36452381515501281,\n",
       " 0.37241879272459288,\n",
       " 0.37585995101927061,\n",
       " 0.36813013839720027,\n",
       " 0.36806340026853818,\n",
       " 0.37353619384763975,\n",
       " 0.39352798461912414,\n",
       " 0.40684509277342101,\n",
       " 0.40681736373899718,\n",
       " 0.41440063095091123,\n",
       " 0.41387948989866513,\n",
       " 0.41905272293089169,\n",
       " 0.41265904998777642,\n",
       " 0.410329296112044,\n",
       " 0.41912121200559865,\n",
       " 0.42785078430174123,\n",
       " 0.44222217178343065,\n",
       " 0.44567807769773732,\n",
       " 0.44841379165647755,\n",
       " 0.45997607803343066,\n",
       " 0.47548618698118456,\n",
       " 0.49311575317381151,\n",
       " 0.4869347572326494,\n",
       " 0.4925845794677568,\n",
       " 0.49062674713133103,\n",
       " 0.4767282028198076,\n",
       " 0.48527890014646774,\n",
       " 0.47155899047849897,\n",
       " 0.47212088012693648,\n",
       " 0.47741262817381147,\n",
       " 0.49214659500120406,\n",
       " 0.49129146575926069,\n",
       " 0.5048948364257646,\n",
       " 0.50873207473753213,\n",
       " 0.51929681396482708,\n",
       " 0.52552857208250281,\n",
       " 0.51759489059446573,\n",
       " 0.52941553497312788,\n",
       " 0.52892842102049109,\n",
       " 0.50490830230711214,\n",
       " 0.49905364227293242,\n",
       " 0.49870729446409451,\n",
       " 0.50495706558225861,\n",
       " 0.5087579841613602,\n",
       " 0.50485705947874304,\n",
       " 0.47601243209837191,\n",
       " 0.47381238174436802,\n",
       " 0.46646809387205357,\n",
       " 0.45860168457029576,\n",
       " 0.46390867996214141,\n",
       " 0.47906292343137968,\n",
       " 0.46999790573118438,\n",
       " 0.47363815689085237,\n",
       " 0.47189837646482696,\n",
       " 0.47577041244505158,\n",
       " 0.4773696632385086,\n",
       " 0.45926669692991484,\n",
       " 0.45087679672239528,\n",
       " 0.46365963363645774,\n",
       " 0.4543097496032546,\n",
       " 0.46074120712278582,\n",
       " 0.45778099822996354,\n",
       " 0.44716016769407485,\n",
       " 0.43393526840208263,\n",
       " 0.422423282623274,\n",
       " 0.40549984359739505,\n",
       " 0.4141251029968091,\n",
       " 0.41306910705564698,\n",
       " 0.41478430938718996,\n",
       " 0.40648447799680909,\n",
       " 0.41200571441648681,\n",
       " 0.42141961288450436,\n",
       " 0.41045667266843988,\n",
       " 0.42491041564939691,\n",
       " 0.42553884887693599,\n",
       " 0.41005208969114493,\n",
       " 0.39976231384275623,\n",
       " 0.39557762145994374,\n",
       " 0.38620126342771716,\n",
       " 0.3853320274352855,\n",
       " 0.37722756195066637,\n",
       " 0.37037264251707258,\n",
       " 0.38115248489378156,\n",
       " 0.36369715118406476,\n",
       " 0.36791454315183819,\n",
       " 0.35631894302366435,\n",
       " 0.36508791351316627,\n",
       " 0.37949712753294162,\n",
       " 0.37676656341550996,\n",
       " 0.38407583618162322,\n",
       " 0.35408847427366424,\n",
       " 0.34806848907468962,\n",
       " 0.31781533813474822,\n",
       " 0.33409701156614469,\n",
       " 0.32067216873167204,\n",
       " 0.2880953445434396,\n",
       " 0.28959275054929895,\n",
       " 0.30194019699094932,\n",
       " 0.29824811172483601,\n",
       " 0.29617070770261922,\n",
       " 0.28093351745603717,\n",
       " 0.28028152084348834,\n",
       " 0.28623843002317584,\n",
       " 0.28066928100584182,\n",
       " 0.29970259475706251,\n",
       " 0.30177181625364452,\n",
       " 0.31160397720335153,\n",
       " 0.31896788406370308,\n",
       " 0.31868553924558785,\n",
       " 0.31058550643919136,\n",
       " 0.33212095260618352,\n",
       " 0.33696332550047059,\n",
       " 0.35331622695921083,\n",
       " 0.36056631469724792,\n",
       " 0.39349765777586121,\n",
       " 0.4137973518371405,\n",
       " 0.43460048675535334,\n",
       " 0.45266327285764824,\n",
       " 0.45616593170164238,\n",
       " 0.46167891311643733,\n",
       " 0.45621303939817559,\n",
       " 0.43337088394163259,\n",
       " 0.41765332412717943,\n",
       " 0.42680522537229659,\n",
       " 0.40695241546629074,\n",
       " 0.40337315750120284,\n",
       " 0.41365439224241374,\n",
       " 0.43069747161863442,\n",
       " 0.443538070678693,\n",
       " 0.4499529495239078,\n",
       " 0.46899068832395663,\n",
       " 0.46623145675657379,\n",
       " 0.46998756790159329,\n",
       " 0.46886047744749171,\n",
       " 0.46648081970213034,\n",
       " 0.46985020446775533,\n",
       " 0.46448208999631979,\n",
       " 0.47194603347776509,\n",
       " 0.44799080657957174,\n",
       " 0.44629204177854631,\n",
       " 0.45157403945921037,\n",
       " 0.45929993820188614,\n",
       " 0.45290171432493298,\n",
       " 0.45396547317503061,\n",
       " 0.44533494949338998,\n",
       " 0.44511081695554816,\n",
       " 0.44953566360471803,\n",
       " 0.41612765884397579,\n",
       " 0.39419683456419063,\n",
       " 0.38934783554075314,\n",
       " 0.39352854919431757,\n",
       " 0.39404480743406362,\n",
       " 0.39635173416135849,\n",
       " 0.37005991363523544,\n",
       " 0.36514645767210063,\n",
       " 0.35412402725217873,\n",
       " 0.3538194427490049,\n",
       " 0.3372904624938779,\n",
       " 0.30339099502561617,\n",
       " 0.30062483215330171,\n",
       " 0.26141534423826263,\n",
       " 0.22608298492429776,\n",
       " 0.25588716125486416,\n",
       " 0.25870895767210045,\n",
       " 0.27072755813596761,\n",
       " 0.25191970825193438,\n",
       " 0.26242861175535231,\n",
       " 0.27681075286863355,\n",
       " 0.27955768966672923,\n",
       " 0.2741618385314753,\n",
       " 0.28724373245237372,\n",
       " 0.26073897171018623,\n",
       " 0.26898995208738352,\n",
       " 0.28100786590574289,\n",
       " 0.28238525009153392,\n",
       " 0.30289786148069409,\n",
       " 0.30335820388792062,\n",
       " 0.29890633010862372,\n",
       " 0.2932766227721979,\n",
       " 0.3036788291930963,\n",
       " 0.3186443672179986,\n",
       " 0.33129636383054745,\n",
       " 0.32387596511838923,\n",
       " 0.33227853393552792,\n",
       " 0.32434961700437553,\n",
       " 0.33492449951169972,\n",
       " 0.35297106933591849,\n",
       " 0.35013161468503956,\n",
       " 0.35440574645994188,\n",
       " 0.34370419311521527,\n",
       " 0.35048364257810588,\n",
       " 0.36209407043455116,\n",
       " 0.35004645156858433,\n",
       " 0.346980457305889,\n",
       " 0.35189408111570342,\n",
       " 0.35028486251829127,\n",
       " 0.34560059356687522,\n",
       " 0.34582683563230493,\n",
       " 0.34453749847410181,\n",
       " 0.33184080505369162,\n",
       " 0.33357568359373069,\n",
       " 0.30172173690793963,\n",
       " 0.29857057952878924,\n",
       " 0.27930184936521502,\n",
       " 0.27125221633909197,\n",
       " 0.29718851852415057,\n",
       " 0.30670615768430681,\n",
       " 0.32877589797971696,\n",
       " 0.33396952819822279,\n",
       " 0.32927966690061533,\n",
       " 0.3238246002197071,\n",
       " 0.32048188400266608,\n",
       " 0.30435752868650395,\n",
       " 0.33054679870603521,\n",
       " 0.32000386047361329,\n",
       " 0.31178595352170901,\n",
       " 0.30880075454709965,\n",
       " 0.32011200714109378,\n",
       " 0.30460283660886722,\n",
       " 0.29289092636106451,\n",
       " 0.29950389862058596,\n",
       " 0.28686001586912108,\n",
       " 0.27537522125242186,\n",
       " 0.28344176101682617,\n",
       " 0.2778018760680957,\n",
       " 0.28745817947385738,\n",
       " 0.30213896560666986,\n",
       " 0.30227175140378898,\n",
       " 0.31595747756956044,\n",
       " 0.33638135147092763,\n",
       " 0.35208329772947256,\n",
       " 0.3389552268981737,\n",
       " 0.34770602416990221,\n",
       " 0.33975660324094709,\n",
       " 0.33525683212278301,\n",
       " 0.3501258850097459,\n",
       " 0.34878196334836892,\n",
       " 0.3524709625243943,\n",
       " 0.33701332855222632,\n",
       " 0.33063133239744114,\n",
       " 0.31410151672361303,\n",
       " 0.31733331680295868,\n",
       " 0.30061739730832976,\n",
       " 0.30238721466062463,\n",
       " 0.3036217117309371,\n",
       " 0.30748704528806597,\n",
       " 0.30194107818601518,\n",
       " 0.28425416183469682,\n",
       " 0.28952563095090772,\n",
       " 0.2875727081298628,\n",
       " 0.29860511016843699,\n",
       " 0.3085906181335249,\n",
       " 0.30925410842893503,\n",
       " 0.31689279937742132,\n",
       " 0.33502821350095646,\n",
       " 0.33614967346189395,\n",
       " 0.3393334274291791,\n",
       " 0.33806617736814393,\n",
       " 0.34563405609128844,\n",
       " 0.35473236465452085,\n",
       " 0.35482101821897399,\n",
       " 0.35316419982908143,\n",
       " 0.36166478347776304,\n",
       " 0.36490243148801693,\n",
       " 0.33758699417112237,\n",
       " 0.34789278030393489,\n",
       " 0.33613587951658136,\n",
       " 0.34160374832151297,\n",
       " 0.34618790435788988,\n",
       " 0.35250329208371994,\n",
       " 0.35551380920408127,\n",
       " 0.37141026687620038,\n",
       " 0.36551794052121989,\n",
       " 0.3668756904601847,\n",
       " 0.36711529541013588,\n",
       " 0.36156528472898353,\n",
       " 0.35849900817869051,\n",
       " 0.35586343002317289,\n",
       " 0.34454995346067291,\n",
       " 0.35445938491819245,\n",
       " 0.34334019470212801,\n",
       " 0.34121807479856353,\n",
       " 0.3215384216308389,\n",
       " 0.32072430419919823,\n",
       " 0.33749161529538962,\n",
       " 0.32768773269651263,\n",
       " 0.31908542633054582,\n",
       " 0.32588846969602431,\n",
       " 0.32144226455686414,\n",
       " 0.31261635589597547,\n",
       " 0.32573750686643443,\n",
       " 0.33393062591550665,\n",
       " 0.33071517181394416,\n",
       " 0.33635173034665899,\n",
       " 0.32272657394407106,\n",
       " 0.30066222381589724,\n",
       " 0.30043608665464233,\n",
       " 0.28815981483457392,\n",
       " 0.2785698871612341,\n",
       " 0.2607210750579626,\n",
       " 0.26034041786191769,\n",
       " 0.24307521247861691,\n",
       " 0.2220854663848669,\n",
       " 0.23298653602598018,\n",
       " 0.22607701683042353,\n",
       " 0.20961288642881218,\n",
       " 0.21310959434507193,\n",
       " 0.20831977272031604,\n",
       " 0.18527735900876816,\n",
       " 0.19962714767453962,\n",
       " 0.20623604965207867,\n",
       " 0.20271364021299174,\n",
       " 0.22145212745664405,\n",
       " 0.23795675086972998,\n",
       " 0.24571523094175143,\n",
       " 0.24385402488706392,\n",
       " 0.25442410469053073,\n",
       " 0.22483922386167329,\n",
       " 0.20175047111509123,\n",
       " 0.1875770835876254,\n",
       " 0.20000986480710781,\n",
       " 0.2026758880615023,\n",
       " 0.1967849502563265,\n",
       " 0.20790059280393391,\n",
       " 0.20486467552182941,\n",
       " 0.21986436271665363,\n",
       " 0.23483248710630206,\n",
       " 0.24280655097959306,\n",
       " 0.23900469589231277,\n",
       " 0.24157731628415843,\n",
       " 0.25660066604612131,\n",
       " 0.27242564582822576,\n",
       " 0.28697229957578435,\n",
       " 0.30287765693662416,\n",
       " 0.30403617286679996,\n",
       " 0.31174245452878724,\n",
       " 0.28995194625852355,\n",
       " 0.28649438476560363,\n",
       " 0.29160932350156554,\n",
       " 0.30460456275937803,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
