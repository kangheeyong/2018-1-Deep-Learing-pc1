{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum DCGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_DCGANs_for_FD_v2'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     80
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def G(x,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "\n",
    "        conv1 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "    r5= tf.add(conv5, 0 ,name=name)#1*1*100\n",
    "  \n",
    "    return r5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_epoch = 50\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.0\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,isTrain,name='G_sample') # G(z)\n",
    "E_z = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z, isTrain, reuse=True, name ='re_image')\n",
    "re_z = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "\n",
    "re_image_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_image - u)**2, axis=[1,2,3])) , name = 're_image_loss') \n",
    "E_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z-z)**2, axis=[1,2,3])) , name = 'E_loss')             \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "    \n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss, var_list=G_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    E_AE_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(re_image_loss, var_list=E_vars, name='E_AE_optim')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -70.810, D_real_e : 49.805, D_fake_e : 34.559, G_e : 34.919, new_measure : 62.190, k_curr : -0.153992\n",
      "D_e : 33.327, D_real_e : 37.214, D_fake_e : 25.410, G_e : 25.809, new_measure : 40.184, k_curr : 0.518746\n",
      "D_e : 29.785, D_real_e : 31.594, D_fake_e : 22.011, G_e : 22.248, new_measure : 33.262, k_curr : 0.149240\n",
      "D_e : 26.876, D_real_e : 27.098, D_fake_e : 18.883, G_e : 19.005, new_measure : 28.860, k_curr : 0.047487\n",
      "D_e : 22.926, D_real_e : 23.206, D_fake_e : 16.178, G_e : 16.264, new_measure : 24.405, k_curr : -0.007760\n",
      "D_e : 19.508, D_real_e : 20.187, D_fake_e : 14.033, G_e : 14.128, new_measure : 21.151, k_curr : 0.001468\n",
      "D_e : 17.407, D_real_e : 17.517, D_fake_e : 12.223, G_e : 12.252, new_measure : 18.751, k_curr : 0.029176\n",
      "D_e : 15.196, D_real_e : 15.131, D_fake_e : 10.595, G_e : 10.617, new_measure : 16.186, k_curr : -0.041939\n",
      "D_e : 13.295, D_real_e : 13.644, D_fake_e : 9.417, G_e : 9.515, new_measure : 14.528, k_curr : 0.058671\n",
      "D_e : 11.985, D_real_e : 11.964, D_fake_e : 8.326, G_e : 8.369, new_measure : 12.778, k_curr : 0.075934\n",
      "D_e : 10.879, D_real_e : 10.686, D_fake_e : 7.566, G_e : 7.558, new_measure : 11.473, k_curr : -0.140754\n",
      "D_e : 9.768, D_real_e : 9.887, D_fake_e : 6.783, G_e : 6.844, new_measure : 10.489, k_curr : 0.074755\n",
      "D_e : 9.234, D_real_e : 9.303, D_fake_e : 6.465, G_e : 6.530, new_measure : 9.914, k_curr : 0.025000\n",
      "D_e : 8.533, D_real_e : 8.622, D_fake_e : 5.987, G_e : 6.045, new_measure : 9.081, k_curr : -0.000492\n",
      "D_e : 8.010, D_real_e : 8.095, D_fake_e : 5.591, G_e : 5.655, new_measure : 8.484, k_curr : 0.031156\n",
      "D_e : 7.426, D_real_e : 7.480, D_fake_e : 5.173, G_e : 5.231, new_measure : 7.798, k_curr : 0.043897\n",
      "D_e : 6.709, D_real_e : 6.799, D_fake_e : 4.690, G_e : 4.768, new_measure : 7.047, k_curr : 0.018239\n",
      "D_e : 6.277, D_real_e : 6.332, D_fake_e : 4.359, G_e : 4.429, new_measure : 6.574, k_curr : 0.027461\n",
      "D_e : 6.023, D_real_e : 6.111, D_fake_e : 4.197, G_e : 4.282, new_measure : 6.370, k_curr : 0.016612\n",
      "D_e : 5.860, D_real_e : 5.942, D_fake_e : 4.084, G_e : 4.163, new_measure : 6.185, k_curr : 0.008093\n",
      "D_e : 5.684, D_real_e : 5.793, D_fake_e : 3.964, G_e : 4.044, new_measure : 6.031, k_curr : 0.038641\n",
      "D_e : 5.570, D_real_e : 5.663, D_fake_e : 3.880, G_e : 3.964, new_measure : 5.904, k_curr : 0.039168\n",
      "D_e : 5.438, D_real_e : 5.549, D_fake_e : 3.794, G_e : 3.885, new_measure : 5.771, k_curr : 0.036636\n",
      "D_e : 5.363, D_real_e : 5.449, D_fake_e : 3.741, G_e : 3.821, new_measure : 5.676, k_curr : 0.017971\n",
      "D_e : 5.263, D_real_e : 5.348, D_fake_e : 3.656, G_e : 3.741, new_measure : 5.530, k_curr : 0.024904\n",
      "D_e : 5.230, D_real_e : 5.298, D_fake_e : 3.626, G_e : 3.708, new_measure : 5.508, k_curr : 0.027127\n",
      "D_e : 5.123, D_real_e : 5.218, D_fake_e : 3.583, G_e : 3.657, new_measure : 5.408, k_curr : 0.016727\n",
      "D_e : 5.046, D_real_e : 5.139, D_fake_e : 3.518, G_e : 3.590, new_measure : 5.335, k_curr : 0.036415\n",
      "D_e : 4.995, D_real_e : 5.086, D_fake_e : 3.480, G_e : 3.562, new_measure : 5.263, k_curr : 0.030504\n",
      "D_e : 4.923, D_real_e : 5.026, D_fake_e : 3.441, G_e : 3.518, new_measure : 5.222, k_curr : 0.030388\n",
      "D_e : 4.880, D_real_e : 4.980, D_fake_e : 3.404, G_e : 3.485, new_measure : 5.167, k_curr : 0.032819\n",
      "D_e : 4.852, D_real_e : 4.935, D_fake_e : 3.368, G_e : 3.452, new_measure : 5.133, k_curr : 0.040699\n",
      "D_e : 4.791, D_real_e : 4.891, D_fake_e : 3.344, G_e : 3.433, new_measure : 5.075, k_curr : 0.015023\n",
      "D_e : 4.773, D_real_e : 4.858, D_fake_e : 3.317, G_e : 3.402, new_measure : 5.047, k_curr : 0.010727\n",
      "D_e : 4.696, D_real_e : 4.788, D_fake_e : 3.264, G_e : 3.352, new_measure : 4.969, k_curr : 0.009983\n",
      "D_e : 4.690, D_real_e : 4.764, D_fake_e : 3.244, G_e : 3.325, new_measure : 4.946, k_curr : 0.038323\n",
      "D_e : 4.609, D_real_e : 4.696, D_fake_e : 3.210, G_e : 3.294, new_measure : 4.869, k_curr : 0.020919\n",
      "D_e : 4.588, D_real_e : 4.676, D_fake_e : 3.203, G_e : 3.271, new_measure : 4.855, k_curr : 0.025597\n",
      "D_e : 4.546, D_real_e : 4.632, D_fake_e : 3.163, G_e : 3.241, new_measure : 4.804, k_curr : 0.029679\n",
      "D_e : 4.521, D_real_e : 4.604, D_fake_e : 3.138, G_e : 3.222, new_measure : 4.787, k_curr : 0.033373\n",
      "D_e : 4.487, D_real_e : 4.584, D_fake_e : 3.116, G_e : 3.204, new_measure : 4.746, k_curr : 0.047317\n",
      "D_e : 4.453, D_real_e : 4.523, D_fake_e : 3.098, G_e : 3.171, new_measure : 4.702, k_curr : 0.032799\n",
      "D_e : 4.407, D_real_e : 4.507, D_fake_e : 3.076, G_e : 3.153, new_measure : 4.674, k_curr : 0.037000\n",
      "D_e : 4.388, D_real_e : 4.477, D_fake_e : 3.053, G_e : 3.131, new_measure : 4.655, k_curr : 0.043567\n",
      "D_e : 4.335, D_real_e : 4.443, D_fake_e : 3.037, G_e : 3.117, new_measure : 4.622, k_curr : 0.024378\n",
      "D_e : 4.334, D_real_e : 4.431, D_fake_e : 3.020, G_e : 3.102, new_measure : 4.600, k_curr : 0.022835\n",
      "D_e : 4.317, D_real_e : 4.413, D_fake_e : 3.006, G_e : 3.084, new_measure : 4.572, k_curr : 0.036316\n",
      "D_e : 4.280, D_real_e : 4.371, D_fake_e : 2.978, G_e : 3.061, new_measure : 4.533, k_curr : 0.034939\n",
      "D_e : 4.262, D_real_e : 4.358, D_fake_e : 2.967, G_e : 3.049, new_measure : 4.517, k_curr : 0.039086\n",
      "D_e : 4.233, D_real_e : 4.347, D_fake_e : 2.964, G_e : 3.045, new_measure : 4.508, k_curr : 0.032633\n",
      "total time :  3706.314795255661\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmYVNW97//3t6pHoJmbZmgUUFFBAWVQgwMaEycSTWISo1HUqPEcjYmek0SPJzd678m5GTVOMZcYo/GnYowRiYkxDrTKcUIRUXBCRWWQUaRbeqjq+v7+2Lu6q9tuqKaHXXR9Xs9Tzx5q19qrluCHtfeqvczdERERyTWxqCsgIiLSFgWUiIjkJAWUiIjkJAWUiIjkJAWUiIjkJAWUiIjkpMgDysziZvaSmT0Ybo81s+fMbKWZ3WNmRVHXUUREel7kAQV8F3gtY/tnwLXuvjfwEfCtSGolIiKRijSgzKwSOAm4Jdw24Bjgz+EhtwOnRFM7ERGJUkHE5/818AOgLNweAmx192S4vRoY1dYHzewC4AKA0tLSqaNHj+5URVKpFLFYLnQoo6e2aEnt0ZLaoyW1R7Ns2+LNN9/c5O7lOzsusoAys9nABnd/0cxmdfTz7j4XmAswbdo0f+GFFzpVn6qqKmbN6nA1eiW1RUtqj5bUHi2pPZpl2xZm9l425UXZg5oJfNHMTgRKgP7AdcBAMysIe1GVwJoI6ygiIhGJrF/q7le4e6W7jwFOAx539zOAhcCp4WFzgAciqqKIiEQoFy+c/hC4zMxWEtyT+n3E9RERkQhEPUgCAHevAqrC9XeAGVHWR0REopeLPSgREREFlIiI5CYFlIiI5CQFFPDOxho216airoaIiGTI+4CqqU9y/HVP8Y9ViairIiIiGfI+oPoVF/C5/St4Zm2ShqR6USIiuSLvAwrgK1NHUZOAhW9s+PSb7j1fIRERUUABHLlPOf2LjPteXN3yjW3r4OfjYMkfo6mYiEgeU0ABBfEYnxkZ5/HXN7C5pr75jY9XQ+0WWHAJLLs3ugqKiOQhBVRo5qhCkilnwctrm3cma4Nl2Qi4/9vw2l+jqZyISB5SQIVGl8U4YFR//px5mS8Z9qa+dDOMOhjuPQfeeiSaCoqI5BkFVIZTD65k+dptvLZuW7AjWRcs+wyFM/4Mw/aHe74J7z4ZXSVFRPKEAirDF6eMojCeMVgiEQZUQQmUDoQz58OgsXDXafD+c9FVVEQkDyigMgzuW8Qx+w1j/tK1JBpTzT2oguJg2XcInDUfyobDnafC2peiq6yISC+ngGrlKwdXsqmmniff3NgcUIWlzQeUDYc5C6BkINzxJXj/2WgqKiLSyymgWjl6v2EM6VvEfUtWf7oHlTagEs7+K5QOhtu/AK/8uecrKiLSyymgWimMx/jilJE8umIDtbWfBDsLSj994KAxcN6jUDkd7vsWPPFzPXVCRKQLKaDacOrUShoaU6xcswksDvF2Jh7uMxjOvB8mnQYLfwLz/6V5aLqIiHSKAqoNE0cOYL/hZaxctykYwbcjBcXwpd/C0VfCy3cH96W2b+mZioqI9GIKqHacOrWSbdU1JOPFOz/YDI76AXzl97B6MdxyLGx+u/srKSLSiymg2nHylFGUWoLtqXYu77XlwFNhzl+h9iP4wwmw9YPuq6CISC+ngGpHeVkxe/SP8XFDjMZUBwY/7HEonPP34Ee+d30N6rZ1XyVFRHoxBdQOjBkYpyZVyB3PrOrYB4ftD1+7HTa9CffOgUbN1isi0lEKqB0YVuoUl/Th6gdXcP9Lq3f+gUx7HQ2zr4W3H4e/f19D0EVEOqgDN1jyTyxZz5iKwRxWPoR/v3cZpYVxjj9gRPYFHHwWbHkXFl0Dg8fBzEu6r7IiIr2MelA7kqwnVlTK786axuTKAXzn7peoamta+B055kcw8UvwyI9gxQPdU08RkV5IAbUjyVooKKFvcQF/OGcG4yvK+PYdL/LM25uzLyMWg1NuhsoZ8JcLYPUL3VdfEZFeRAG1I8n6pufwDSgt5I5vHcIeg/vwrdsXs+T9j7Ivp7AUvnF38KDZu0+Dj1Z1T31FRHoRBdSOJOtaPIdvcN8i/r/zDqG8rJizb32e5Ws/zr6svkPh9HuhsQH+eAp8vKYbKiwi0nsooHYkUfepJ5lX9C/hzvMOoV9xAWf+/nlWrO3A75zKx8M3/wLbN8NtJ8G2tV1cYRGR3kMBtSPJ+jafxVc5qA93nn8oxQUxvvG7Z1n6wdbsy6ycFoTUJ5sUUiIiOxBZQJnZaDNbaGYrzGy5mX033D/YzB4xs7fC5aCo6kiyFgrbfljs2KF9+dO3D2NAaSFn/O5ZnnunAwMnRk+HM/8CNRvhttmwbV0XVVhEpPeIsgeVBP7N3ScAhwIXmdkE4HLgMXffB3gs3O55qVRwv2gHTzMfPbgP9154GCMGljLnD8/zxJsbsy9/9Az45n1Qsx5uV0iJiLQWWUC5+zp3XxKuVwOvAaOAk4Hbw8NuB06JpIKN4bxOrWfTbaWifwn3XHAo44b24/zbX+Dh5R9mf449DglCqvrDYGbe6g58VkSklzPPgUfwmNkY4EngAOB9dx8Y7jfgo/R2q89cAFwAUFFRMXXevHmdqkNNTQ39+vVr2i5IVHP4/3yTt/Y+jzWVX9jp5z9JOL96oY5V21JccGAxh47M/iEdA7auYNKyq6krGcorB/4v6korduk7dJXWbZHv1B4tqT1aUns0y7Ytjj766BfdfdpOD3T3SF9AP+BF4Mvh9tZW73+0szKmTp3qnbVw4cKWOz5e6/7j/u6Lb826jOq6hH/tt0/7mMsf9Duffa9jFVj1tPtPRrn/nwr3p65xTzZ07PNd6FNtkefUHi2pPVpSezTLti2AFzyLfIh0FJ+ZFQL3AXe6+1/C3evNbET4/gigg88W6iLJumC5sxl1M/QrLuC2c2Zw1Phy/uP+V7hqwXISjansPrznYXDRs7D3Z+HRq+C3R8B7z3S83iIivUSUo/gM+D3wmrtfk/HWAmBOuD4HiOYBdumAamcUX3tKi+LcctY0zp05ltueXsVZv3+eLZ80ZPfhAZVw2p3wjXnQUAN/OB4euFhTyItIXoqyBzUTOBM4xsyWhq8TgZ8CnzOzt4Bjw+2etws9qLSCeIz/9YUJ/Oqrk3nx/Y/4wg2LOvbUiX1PgIueg89cAkvvghumwkt3asoOEckrUY7iW+Tu5u6T3H1K+Pq7u29298+6+z7ufqy7R9N9SKZH8XU8oNK+MrWSe799GI0p5ys3P82Clzvwo9yivvD5/wMXPgVD94EH/hX+eLKe4ycieUNPkmhPojZYdiKgACaPHshfv3M4B4wcwCV3v8T/fei1jk0hXzERzvkHnPQrWPMi/OYwePZmSDV2ql4iIrlOAdWeZHa/g8pGeVkxd51/KKcfsgf/74l3+MbcZ1m5oTr7AmIxmH4e/OuzsOdM+MflcOvxsPGNTtdNRCRXKaDa0zRIonTHx2WpqCDGf3/pQH751cm8sb6aE657imseeZO6RAd6QgNHwxn3wpfmwua34LeHw5O/gMZEl9RRRCSXKKDa0zRIovM9qEynTq3k0cuO4sQDR3D9Y29x4nVPdWwCRDOY/HW46HnY7yR4/L/g/x0Fby/s0nqKiERNAdWeTozi25nysmKuO+0gbj93BolUim/87lm+f+/LfJTtcHSAfsPgq7fB1++Ehmq44xS482u67CcivYYCqj1dMIpvZ44aX84/v3cUFx61F395aQ3HXvMEdz//Pslsf9wLsP9suGgxfO5/w/vPBIMoHrwseFK6iMhuTAHVni4axbczpUVxLj9hPx78zuGMHdqXK/7yCidc9xQLX9+QftTTzhWWwMzvwiUvwbRz4cXb4PqDYNG1waSLIiK7IQVUe3qgB5Vp/xH9uffCw/jtNw8m0ZjinNsWc8Ytz/Hqmg5OK3/SL4PRfmMODx6ZdM3+8NDlsH55t9VdRKQ7KKDak6yFeFEwxLuHmBnHHzCCf156FFd9YQKvrdvG7BsWcdk9S1mztTb7gsrHw+nz4Oy/wdgjYfEtcPNnYO7R8MKtUNeB0BMRiUj2c0Lkm3ame+8JRQUxzp45li9PreQ3C9/m1v95lwdfWcfXp43m20eNo3JQn+wKGnN48PpkMyy7B166Ax68FP7xHzDhZJj0VRh7FMQLu/cLiYjsAgVUe5J1XT7EvKP6lxRy+Qn7ceZhe3Lj428xb/H73P38+5xy0Cj+ZdZe7FWe5Rw0fYfAYf8Kh/4LrF0CS+6AV++DZfOgZADsexJM+CKMO7rDD8cVEekuCqj2JOqgoGt+pNtZowaW8n+/PInvHLMPc598h3mL3+e+Jas58cARXDRrbyaM7J9dQWYwamrwOv6n8M5CWPEAvPE3ePkuKCqD8ccx1PeChhlQlGVPTUSkGyig2pMDPajWRg4s5aovTuTiY/bm94ve5Y5n3uNvy9Zx5PhyZk8awef2r2BQ36LsCissCZ6avu8JkGyAVU8GYfX63zhg+5/hzRtgn8/B/l+E8cdBcVn3fjkRkVYUUO2J8B7UzgztV8wPj9+PC4/ci9ueXsWfXviAH/x5GfGYcdi4IRx/wHCOmzic8rIsA7agCPY+NniddC1LF/yGKUXvwWt/DUIrXhy8NyEMq9JB3fsFRURQQLUvWZvz92MG9Cnku8fuwyWf3ZtX1nzMQ69+yD9e/ZD/nP8qP3rgVaaPGcyJBwznxANHMKx/lt8lXsDWQZNg1iVwws/hg+dgxYLmS4EAZSODKUDK94Wh44P1oeOhbERwGVFEpAsooNqTwz2o1syMSZUDmVQ5kB8cty9vrK/moVeCsLrqryu4+sEVzBgzmNmTRnD8ASOy71nF4rDnZ4LXcf8dTPfx7hOweSVsehOW3h08ZimtZCCMPAhGHQwjDw6W/Ud2z5cWkV5PAdWeZN1ueSnLzNhveH/2G96fSz83npUbqnlw2ToeXLaOHz2wnB8vWM6h44Zw0qQRHDW+PPsh67EYjJ4evNLcofrDIKw2vQkfvhKMElz0a/DwKe39hgehVTEx7GntA0P2gZIsB3aISN5SQLUnUQdlu0cPakf2HlbG944t43vHjueND6v527K1PLhsHVfe/yoAIweUMH3sYKaPGcyMsYPZO9uh6xBczus/IniNO6p5f6I2CKs1S2DtS0FovfXP5tCCILjSgVW+PwzbH4ZNCIbEi4iggGpfsm63ucSXrX2Hl7Hv8H259HPjeXN9Dc++s5nn393C029v5oGlwXT0A/sUMrZfiuW+ksmVAzmwcgADSjv4Q97CUhg9I3ilJRuC6eo3vRnMZbUpfL16X8snW/Qd1hxWFROCS4Xl+0Fcf1RF8o3+1rcnWZ/zgyR2lZmFYVXGnM+Mwd15b/N2nl+1hcXvbuHJ19bwi4ebp+0YV96XKZUDmTx6IPuP6E9F/2LKy4rpU9SBPz4FRcEjmMrHt9yfvky4YQVseC18rYAlt0Nie3BMYR8YMSW4p5X+HdfAPTQgQ6SXU0C1J1nb63pQ7TEzxgzty5ihffnatNFUVX3EQTNmsmzNVl7+YCtLP/iYp1Zu4i8vrWnxuT5FccrLihnar5jyfsWMGFjCmCFBOWOG9GHUwFIK4jt5lmHmZcK9P9u8P5WCLe8ElwfXvBi8nv8dNN4YvF88AAbtAQP3DF97wKBwvWx48ISMWLyLW0pEepICqj270Si+7jCgTyFH7FPOEfuUA+DufLitjjfX17Cxup5NNfVsrG5+rdxYw5NvbWR7Q/N9poKYMXpwH8YM6UPloD6MHFjKyIEljBgQLCv6l1DYXoDFYjB07+A16WvBvmQDbFgOq18IJmbc+n4wovDtx5t7W00MSgdC6WDoM7h5WVwGRf2gqG/zsrhfuN4vWC8uC56qUdwv536sLZJPFFDt6YX3oDrDzBgxoJQRA9p//JO7s7G6nlWbt7Nq0yes2hy83t20nSXvb+Xj2kSL42MGw8pKGDWolMqmV5+m5ciBJRQXZPSCCoqCEYEjD2p9YvhkUxBYW1cFkzVu3wy1W2D7lmBZvTaYcqShGuprWg7Y2JF4ETOtGF7sF/x5KCgJLv0WlAbhVdgn2C4sDdczlvHi4N5ZvCh8FYZPyC8MPpt+xYvDssPjLB70/szAYsG2xYLPxwp0aVPyhgKqLY1JSCUVUB1kZgzrX8Kw/iXMGDv4U+9/Up9k3ce1rN1ax9qttaz9OFiu+aiWJe9/xIPL1tGY8ozyYHj/EvYY3Ic9h/RhzyF9m9YrB/VhUJ9CzCw4sF958KqcuvOKukNjAzR8AvXVwbKhJlyvCQKsvropzDa8+yajKoYE/2hJ1gUjPJN1wTGfbAp6b4na5mVjfVc2a0sWC8OsuDkkC0qaw68pCAub12MFzS+LtdxOB2issOXnYgVhSMYzlgUQi1G+4XVYsS0Mz1j4fiz479BUdmFz+ZnnJvzvlQ5fLPh8U10z6q0gznsKqLYkw1lodXmnS/UtLmDvYWXsPazt5/olG1Osr65n9ZbtrP6olg8+2s77m7fz3pbtPP76RjbVrG5xfFE8RnlZMcP6F1NRVkJF/2KG9S+hf2khfQrj9CmKU1oUp09RQdN6SWGckoJYsCwZRLzPp4O0tbeqqhg1a1b2XzTVGAZVAzQmgmUq0byebAiXdcGl5Mb6YJledwdPBa9UY/N6Y6I5JJP1GcvasOx0+fVBeKa3vTH4B1cqvcxYzzwmSxMBVmTfHLssVtjqPmIYWOngirUK4qaALmjudWYGYVOgxloFb7xV0Lb1atWbTX/WYoxbswYSj7csy+IZ5wnDvsU/EOI0h3Ws5Xrrf0Q0fT4su13WXNd0eel9sTbqnu6lZ36udVtlfv/M9iko6ZF7vAqotqQDqjA3nmaeLwriMUYNLGXUwFIOaeP9T+qTvL9lO+9t3s6arbVsqK5j47Z61lfX8fbGGp5+exPb6pIdOmdRPEZxYRBYxengKoxRUhBvWv/4ozrmf/gSRQUxCuPBK1i35u14sF2QXi8wCmLhvlgR8XgxhbEYBXFr+lxRceZnmz8fM4jHjJilX8G2dWePIpVqDtFUIriKkBls3hgek2Tx888xfdrU5uD0VBiqGQHYmAhDMF1m+N+l6dgU4M0hnBngTWFbHx5H8JlgpXk71Rgen/EPgfQrM+TT50nvS6ZDO+N7pbfxVt8rFbyPNx/f9A+H4DuPSiZgLc3vpevcm531AIyb1e2nUUC1RT2onNS3uID9R/Rn/xHtP4WiLtHItroE2+sb2d7QSG0iyfaGcL2hkbpE+EqmwvVgWZ/MXA+W2xuSbPkkxUfVKdY3bCXRmCLRmKIhmaIhXGZckex2ZsHAk3gsCL94uB5sWxB+sSDsCsKwKwyDzgxiZmHQ0SL4YhZ8NmZBObGYEW861ojHyAhLY+2a4TwaixOLFQRlWPo4w4BYzJrqG7NwX6s6xIymz8QtOGdBQfN3iYffxzJCOrOM9D7LKK9pf4wW76fP3/T58Ph0vTM/i9H0uZiBYc1XJGlZpoXrT1VVMSuzh+0ZwZsOvlQy3JfRg8XD4M0MUG91XGbPN9H6j0TLc5LxeTzM8sy6ZARx5r7M87cI6Mx6ZYSvp2DwuK7/A94GBVRbkuE9BN2D2u0EvZ44dOHsIFWt/weUoTHlJBpTJFNOIhkGWGOKRKOTTC9TwfvJcF9DY4pko7c4NjP83CHlTqN7cLss5cF2qvmVbFqmwjqE50sFy2SjN627Q6MHxzY0BmWnHFJhGSkPykulgnMmGz08xmlMBYNfGj14P+WQSCZh9aqmclNhPfNVzCD+yN+bQq71PwKaAy0MO1q9Z7QIXoPgKhutwjAMS8gI0BYh2nxsrFWIpsM2Hd7N+5rLSx8PzfXMlNmB/964gew/sNuatIkCqi2J2mCpgJKdCHow4bX4POlwtxXYHgaoQ1PAkrGe8uC9dMilMgIvmUqRSgVh1xiGeWMqMyiD8lMZZaVSGeVmhHj6mBbndXC86RxkBGtT2SmnMVyHzDIyv0vzPics1513V71H5R57tPhOjSkPjs8oy1uU0XZZ6To0dX6a3stYJ33Fs7n+7bV7czuAeyroQGW0U/rYludpbof21Cayv2fZGQqotqgHJdIhFl4mzEdVVeuYNWu/qKvRK+3kZ/55SvegREQil7MBZWbHm9kbZrbSzC7v0ZMnw0t8GsUnIhKZnAwoM4sDNwEnABOAb5jZhB6rQNMlPvWgRESikpMBBcwAVrr7O+7eAMwDTu6xszdd4tM9KBGRqOTqIIlRwAcZ26uh5W83zewC4AKAiooKqqqqOnXCmpqapjKGr1vKfsCzLyylrnRdp8rdHWW2hag9WlN7tKT2aNbVbZGrAbVT7j4XmAswbdo0b+93KtlqMXR28dvwBhx6+Cwoq+hUubujHf3uJx+pPVpSe7Sk9mjW1W2Rq5f41gCjM7Yrw309Q/egREQil6sBtRjYx8zGmlkRcBqwoMfOrlF8IiKRy8lLfO6eNLOLgYeBOHCruy/vsQqke1Dxoh47pYiItJSTAQXg7n8H/h7JydOTFWo+GhGRyOTqJb5oJTSbrohI1BRQbdF07yIikVNAtSVZB4UKKBGRKCmg2qIelIhI5BRQbUnW6zdQIiIRU0C1JVELBfoNlIhIlBRQbVEPSkQkcgqotugelIhI5BRQbdEoPhGRyCmg2qIelIhI5BRQbdE9KBGRyCmg2qJRfCIikVNAtUU9KBGRyCmgWnPXPSgRkRyggGqtsQFwjeITEYmYAqq1ZF2wVA9KRCRSCqjW0rPp6h6UiEikFFCtJWqDpUbxiYhESgHVmnpQIiI5QQHVWjLdg9I9KBGRKCmgWkv3oDSKT0QkUgqo1jSKT0QkJyigWksooEREckFWAWVm3zWz/hb4vZktMbPPd3flIqEelIhITsi2B3Wuu28DPg8MAs4EftpttYpS0yg+BZSISJSyDSgLlycCd7j78ox9vUt6FJ8GSYiIRCrbgHrRzP5JEFAPm1kZkOq+akVIPSgRkZxQkOVx3wKmAO+4+3YzGwyc033VilDTPSj9UFdEJErZ9qAOA95w961m9k3gP4GPu69aEWoaxadHHYmIRCnbgLoZ2G5mk4F/A94G/thttYpSsg4sDvFsO5ciItIdsg2opLs7cDJwo7vfBJR1X7UipMkKRURyQrYBVW1mVxAML/+bmcWAwl09qZn9wsxeN7NlZna/mQ3MeO8KM1tpZm+Y2XG7eo5dlqzTCD4RkRyQbUB9Hagn+D3Uh0Al8ItOnPcR4AB3nwS8CVwBYGYTgNOAicDxwG/MLN6J83ScelAiIjkhq4AKQ+lOYICZzQbq3H2X70G5+z/dPRluPksQeBBcQpzn7vXu/i6wEpixq+fZJYk6jeATEckBWY0EMLOvEfSYqgh+oHuDmX3f3f/cBXU4F7gnXB9FEFhpq8N9bdXpAuACgIqKCqqqqjpViZqaGqqqqpj44WpK6xt5oZPl7c7SbSEBtUdLao+W1B7Nurotsh2qdiUw3d03AJhZOfAo0G5AmdmjwPC2ynL3B8JjrgSSBL2zDnH3ucBcgGnTpvmsWbM6WkQLVVVVzJo1C1bfCEUNdLa83VlTWwig9mhN7dGS2qNZV7dFtgEVS4dTaDM7uTzo7sfu6H0zOxuYDXw2HCEIsAYYnXFYZbiv5+gelIhITsg2oP5hZg8Dd4fbXwf+vqsnNbPjgR8AR7n79oy3FgB3mdk1wEhgH+D5XT3PLknWQXHvHEEvIrI7ySqg3P37ZvYVYGa4a66739+J894IFAOPmBnAs+5+obsvN7M/ASsILv1d5O6NnThPxyXroG95j55SREQ+LevHJbj7fcB9XXFSd997B+/9BPhJV5xnl2gUn4hITthhQJlZNeBtvQW4u/fvllpFKVmv5/CJiOSAHQaUu+ffzZikelAiIrkg2ydJ5A+N4hMRyQkKqNb0LD4RkZyggMqUSkFjg3pQIiI5QAGVSbPpiojkDAVUpqRm0xURyRUKqEzJ+mCpHpSISOQUUJmStcGyUD0oEZGoKaAyqQclIpIzFFCZEmEPSqP4REQip4DK1NSDUkCJiERNAZWpaRSfAkpEJGoKqEz6HZSISM5QQGVKB5RG8YmIRE4BlUmj+EREcoYCKpNG8YmI5AwFVCaN4hMRyRkKqEwaxScikjMUUJkUUCIiOUMBlSlZB/EiiKlZRESipv8TZ0rWq/ckIpIjFFCZErUaYi4ikiMUUJmS9ZqsUEQkRyigMiXr1IMSEckRCqhMyTrdgxIRyREKqEzJOihUQImI5AIFVKaEelAiIrlCAZVJ96BERHKGAiqTfgclIpIzFFCZkrUKKBGRHBFpQJnZv5mZm9nQcNvM7HozW2lmy8zs4B6tULJegyRERHJEZAFlZqOBzwPvZ+w+AdgnfF0A3NyjldIwcxGRnBFlD+pa4AeAZ+w7GfijB54FBprZiB6rkUbxiYjkjIIoTmpmJwNr3P1lM8t8axTwQcb26nDfujbKuICgl0VFRQVVVVWdqlNNTQ2eqOX9Net5t5Nl7e5qamo63Z69idqjJbVHS2qPZl3dFt0WUGb2KDC8jbeuBP6D4PLeLnP3ucBcgGnTpvmsWbM6UxxPPP4YRoo999qXPY/qXFm7u6qqKjrbnr2J2qMltUdLao9mXd0W3RZQ7n5sW/vN7EBgLJDuPVUCS8xsBrAGGJ1xeGW4r9vFUunp3vU7KBGRXNDj96Dc/RV3H+buY9x9DMFlvIPd/UNgAXBWOJrvUOBjd//U5b3uEEslgpVCPc1cRCQXRHIPagf+DpwIrAS2A+f01IljqYZgRT0oEZGcEHlAhb2o9LoDF0VRj+aA0ig+EZFcoCdJhJou8SmgRERyggIqpB6UiEhuUUCFdA9KRCS3KKBCTQGlUXwiIjlBARWKN6oHJSKSSxRQId2DEhHJLQqokEbxiYjkFgVUSD0oEZHcooAKaRSfiEhuUUCF9Cw+EZHcooAKBU8zN4gXRV0VERFBAdUklkoE959aTqAoIiIRUUCFYqkG3X8SEckhCqhQEFAawScikisUUKFYKgGFCigRkVyhgAqpByUiklsUUKF4o+5BiYjkEgVUKOhB6TdQIiK5QgEV0ig+EZHcooAKNf0OSkREcoLO7ebTAAAPKElEQVQCKhRLNWgUn4hIDlFAhTSKT0QktyigQroHJSKSWxRQoeAelEbxiYjkCgVUSD0oEZHcooACcCeue1AiIjmlIOoK5ITGcDZdjeITkU5KJBKsXr2aurq6qKvS4wYMGMBrr73WtF1SUkJlZSWFhYW7VJ4CCiBRGyzVgxKRTlq9ejVlZWWMGTMGy7P55aqrqykrKwPA3dm8eTOrV69m7Nixu1SeLvEBJOuDpe5BiUgn1dXVMWTIkLwLp9bMjCFDhnSqJ6mAAkiGDahRfCLSBfI9nNI62w6RBZSZfcfMXjez5Wb284z9V5jZSjN7w8yO65HKNAWUelAiIrkikntQZnY0cDIw2d3rzWxYuH8CcBowERgJPGpm4929sVsr1BRQugclIpIroupB/QvwU3evB3D3DeH+k4F57l7v7u8CK4EZ3V6b9D0ojeITkV5g/fr1nH766YwbN46pU6dy2GGHcf/997d5bFVVFbNnz+7hGmYnqlF844EjzOwnQB3w7+6+GBgFPJtx3OpwX/fSKD4R6QZX/3U5K9Zu69IyJ4zsz4+/MLHd992dU045hTlz5nDXXXcB8N5777FgwYIurUdP6LaAMrNHgeFtvHVleN7BwKHAdOBPZjaug+VfAFwAUFFRQVVV1S7XdfDmF5gEvLhsBdWrkrtcTm9RU1PTqfbsbdQeLak9WmrdHgMGDKC6uhqAREOCxsauvUORaEg0ld+Wqqoq4vE4Z5xxRtNxgwcP5uyzz27zc9u3byeZTFJdXc2WLVu46KKLWLVqFaWlpVx//fUccMABLFq0iB/+8IdAMPDhoYce4pNPPmkqM5lMcu2113LIIYd86hx1dXW7/ufF3Xv8BfwDODpj+22gHLgCuCJj/8PAYTsrb+rUqd4py+e7/7i/+7pXOldOL7Fw4cKoq5BT1B4tqT1aat0eK1asiKYioeuuu86/973vZX38woUL/aSTTnJ394svvtivuuoqd3d/7LHHfPLkye7uPnv2bF+0aJG7u1dXV3sikfBf/vKX/l//9V/u7p5MJn3btm2+bdu2T5XfVnsAL3gWWRHVPaj5wNEAZjYeKAI2AQuA08ys2MzGAvsAz3d7bZp+B6VLfCLSu1x00UVMnjyZ6dOn7/TYRYsWceaZZwJwzDHHsHnzZrZt28bMmTO57LLLuP7669m6dSsFBQVMnz6dP/zhD1x11VW88sorTT/Q7UpRBdStwDgzexWYB8wJg3U58CdgBUEv6yLv7hF80DyKT4MkRGQ3N3HiRJYsWdK0fdNNN/HYY4+xcePGXS7z8ssv55ZbbqG2tpaZM2fy+uuvc+SRR/Lkk08yatQozj77bP74xz92RfVbiCSg3L3B3b/p7ge4+8Hu/njGez9x973cfV93f6hHKpTQMHMR6R2OOeYY6urquPnmm5v2bd++PavPHnHEEdx5551AcC9r6NCh9O/fn7fffpsDDzyQH/7wh0yfPp3XX3+d9957j4qKCs4//3zOO++8FqHYVfQsPtAPdUWk1zAz5s+fz6WXXsrPf/5zysvL6du3Lz/72c92+tmrrrqKc889l0mTJtGnTx9uv/12AH7961+zcOFCYrEYEydO5IQTTmDevHn84he/oLCwkH79+nVLD0oBBRn3oPSoIxHZ/Y0YMYJ58+ZldeysWbOYNWsWEIz2mz9//qeOueGGGz61b86cOcyZM6fFvh2NLtwVehYfQLIWJwZx5bWISK7Q/5EBkvWkYkXEo66HiEg3efjhh5t+y5Q2duzYdp8wkQsUUADJOhrjCigR6b2OO+44jjuuZ56/3VV0iQ8gUUcqtmszPoqISPdQQAEk60jFiqKuhYiIZFBAgQJKRCQHKaBAASUikoMUUBCO4tM9KBHpHeLxOFOmTGHixIlMnjyZX/3qV6RSqXaPz9U5oTSKD9SDEpHu8dDl8OErXVvm8APhhJ/u8JDS0lKWLl0KwIYNGzj99NPZtm0bV199ddfWpZupBwXhKD4FlIj0PsOGDWPu3LnceOON6WmMdmjLli2ccsopTJo0iUMPPZRly5YB8MQTTzBlyhSmTJnCQQcdRHV1NevWrePII49kypQpHHDAATz99NNdWnf1oCDsQfWPuhYi0tvspKfTU8aNG0djYyMbNmygoqJih8f++Mc/5qCDDmL+/Pk8/vjjnHXWWSxdupRf/vKX3HTTTcycOZOamhpKSkqYO3cuxx13HFdeeSWNjY2sX7++S+utgIIgoEr0oFgRkUWLFnHfffcBbc8JdcYZZ/DlL3+ZyspKpk+fzrnnnksikeCUU05hr7326tK66BIfhD0oDZIQkd7pnXfeIR6PM2zYsF0uI5s5oe66664urLV6UIHwWXwiIr3Nxo0bufDCC7n44osxs50en54T6kc/+lGbc0IdeOCBLF68mNdff53S0lIqKys5//zzqa+v5+WXX+7SuiugoOlZfCIivUFtbS1TpkwhkUhQUFDAmWeeyWWXXZbVZzszJ9RvfvObLv0eCiiAK9ezqmohe0ZdDxGRLtDY2Nih47tqTijNB9UdYjE8pmeZi4jkEvWgRETyxO42J5QCSkSki7l7VgMSelpPzwmVzQ+Dd0SX+EREulBJSQmbN2/u9P+cd3fuzubNmykpKdnlMtSDEhHpQpWVlaxevZqNGzdGXZUeV1dX1yKQSkpKqKys3OXyFFAiIl2osLCQsWPHRl2NSFRVVXHQQQd1WXm6xCciIjlJASUiIjlJASUiIjnJesNIEzPbCLzXyWKGApu6oDq9gdqiJbVHS2qPltQezbJtiz3dvXxnB/WKgOoKZvaCu0+Luh65QG3RktqjJbVHS2qPZl3dFrrEJyIiOUkBJSIiOUkB1Wxu1BXIIWqLltQeLak9WlJ7NOvSttA9KBERyUnqQYmISE5SQImISE7K+4Ays+PN7A0zW2lml0ddn55mZrea2QYzezVj32Aze8TM3gqXg6KsY08ys9FmttDMVpjZcjP7brg/79rEzErM7Hkzezlsi6vD/WPN7Lnw78w9ZlYUdV17kpnFzewlM3sw3M7b9jCzVWb2ipktNbMXwn1d9nclrwPKzOLATcAJwATgG2Y2Idpa9bjbgONb7bsceMzd9wEeC7fzRRL4N3efABwKXBT+mcjHNqkHjnH3ycAU4HgzOxT4GXCtu+8NfAR8K8I6RuG7wGsZ2/neHke7+5SM3z912d+VvA4oYAaw0t3fcfcGYB5wcsR16lHu/iSwpdXuk4Hbw/XbgVN6tFIRcvd17r4kXK8m+B/RKPKwTTxQE24Whi8HjgH+HO7Pi7ZIM7NK4CTglnDbyOP2aEeX/V3J94AaBXyQsb063JfvKtx9Xbj+IVARZWWiYmZjgIOA58jTNgkvZy0FNgCPAG8DW909GR6Sb39nfg38AEiF20PI7/Zw4J9m9qKZXRDu67K/K5oPSnbI3d3M8u63CGbWD7gP+J67b8ucvjuf2sTdG4EpZjYQuB/YL+IqRcbMZgMb3P1FM5sVdX1yxOHuvsbMhgGPmNnrmW929u9Kvveg1gCjM7Yrw335br2ZjQAIlxsirk+PMrNCgnC6093/Eu7O6zZx963AQuAwYKCZpf9xm09/Z2YCXzSzVQS3A44BriN/2wN3XxMuNxD8A2YGXfh3Jd8DajGwTzgKpwg4DVgQcZ1ywQJgTrg+B3ggwrr0qPCewu+B19z9moy38q5NzKw87DlhZqXA5wjuyS0ETg0Py4u2AHD3K9y90t3HEPy/4nF3P4M8bQ8z62tmZel14PPAq3Th35W8f5KEmZ1IcF05Dtzq7j+JuEo9yszuBmYRPCZ/PfBjYD7wJ2APgmlMvuburQdS9EpmdjjwFPAKzfcZ/oPgPlRetYmZTSK4yR0n+Mfsn9z9f5vZOIIexGDgJeCb7l4fXU17XniJ79/dfXa+tkf4ve8PNwuAu9z9J2Y2hC76u5L3ASUiIrkp3y/xiYhIjlJAiYhITlJAiYhITlJAiYhITlJAiYhITlJAieyEmd2ys4cIm9ltZnZqG/vHmNnp3Ve7rmNmZ5vZjTs5ZpaZfaan6iT5TQElshPufp67r9jFj48BuiWgwqfx97RZgAJKeoQCSvKCmX3fzC4J1681s8fD9WPM7M5w/fNm9oyZLTGze8Pn8WFmVWY2LVz/lpm9Gc6T9LtWPY4jzexpM3snozf1U+CIcL6cS1vVaZaZPWlmf7NgTrLfmllsJ3VZZWY/M7MlwFdbldeiF2dmNVmc55z09yF4lE/6s18I5zh6ycweNbOK8OG5FwKXht/niPBpE/eZ2eLwNRORLqKAknzxFHBEuD4N6Bc+c+8I4EkzGwr8J3Csux8MvABcllmAmY0EfkQwT9RMPv3g1BHA4cBsgmCCYC6cp8L5cq5to14zgO8QzEe2F/DlLOqy2d0Pdvd5Hfj+bZ1nBHB1+F0OD99LWwQc6u4HETwl4Qfuvgr4LcHcR1Pc/SmCZ9Fd6+7Tga8QTkMh0hX0NHPJFy8CU82sP8FEfEsIguoI4BKC0JkA/E/45PIi4JlWZcwAnkg/tsXM7gXGZ7w/391TwAozy3aKgefd/Z2wvLsJgqJuJ3W5J8uyd3aeJFDl7hvD/fdkfJ9K4J4wxIqAd9sp91hgQsbT3vubWb+MeaREdpkCSvKCuyfM7F3gbOBpYBlwNLA3wQNQ9wIecfdvdOI0mc9fs3aPalW1NrZtJ3X5pJ39ScKrIuElvMypx9s6z47cAFzj7gvC585d1c5xMYKeVt1OyhPpMF3ik3zyFPDvwJPh+oXASx48kPJZYKaZ7Q1NT2oe3+rzi4GjzGxQOL3CV7I4ZzVQtoP3Z4RP048BXye4tJZNXdqyCpgarn+RYAbcHZ3nufD7DAkvd2be0xpA87QRczL2t/4+/yS4dEhY1ylZ1FMkKwooySdPEdwnesbd1xNcSnsKILzMdTZwt5ktI7ik1uIeUzj3zX8DzwP/QxAIH+/knMuARjN7ufUgidBi4EaCXty7wP3Z1KUdvyMInJcJ5m3K7Gm1dZ51BD2jZ8Lv81rG8VcB95rZi8CmjP1/Bb6UHiRBcHl0mpktM7MVBKEv0iX0NHORDkjfXwl7UPcTTNFy/84+105ZswinbOjKOkZ1HpGuph6USMdcZWZLCSZme5dg7iwR6QbqQYmISE5SD0pERHKSAkpERHKSAkpERHKSAkpERHKSAkpERHLS/w8ulM4m0k4BzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8112995e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            \n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e = sess.run([G_optim, G_loss], {u : u_, z : z_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "                 \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, new_measure : %.3f, k_curr : %3f'%(np.mean(D_error), np.mean(D_real_error),\n",
    "            np.mean(D_fake_error), np.mean(G_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "    \n",
    "        \n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_DCGANs_for_FD_v2/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 're_z:0' shape=(?, 1, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19897860717773433,\n",
       " 0.40926763153076162,\n",
       " 0.6238506698608397,\n",
       " 0.84492298507690411,\n",
       " 1.0711724777221678,\n",
       " 1.3007973785400389,\n",
       " 1.5344515571594237,\n",
       " 1.7719298515319823,\n",
       " 2.012912265777588,\n",
       " 2.2552802047729492,\n",
       " 2.4985787582397463,\n",
       " 2.7433792228698732,\n",
       " 2.9888390235900881,\n",
       " 3.2322349052429202,\n",
       " 3.477311435699463,\n",
       " 3.7219559631347656,\n",
       " 3.9644976425170899,\n",
       " 4.2056137809753418,\n",
       " 4.445647449493408,\n",
       " 4.6828838806152344,\n",
       " 4.9179960823059083,\n",
       " 5.1534117851257326,\n",
       " 5.3854251365661625,\n",
       " 5.613756954193116,\n",
       " 5.8409775085449223,\n",
       " 6.0643572578430183,\n",
       " 6.2875136184692391,\n",
       " 6.5070209045410161,\n",
       " 6.7274163436889651,\n",
       " 6.9410709953308105,\n",
       " 7.1536305923461914,\n",
       " 7.3579407806396482,\n",
       " 7.55638907623291,\n",
       " 7.7514461898803706,\n",
       " 7.9400659599304193,\n",
       " 8.1197356147766104,\n",
       " 8.2851362800598132,\n",
       " 8.4356319503784167,\n",
       " 8.5730931549072249,\n",
       " 8.6888482551574686,\n",
       " 8.7835742759704569,\n",
       " 8.8758724594116192,\n",
       " 8.9565418052673316,\n",
       " 9.0272593460082984,\n",
       " 9.0882134513854957,\n",
       " 9.1515842628478978,\n",
       " 9.2347139930725071,\n",
       " 9.3203713645935036,\n",
       " 9.3939050636291483,\n",
       " 9.4717390823364234,\n",
       " 9.5554984855651828,\n",
       " 9.6426451301574687,\n",
       " 9.7181117668151842,\n",
       " 9.8006180076599101,\n",
       " 9.8849892539978015,\n",
       " 9.9745114898681635,\n",
       " 10.046460666656493,\n",
       " 10.115389961242675,\n",
       " 10.133402870178221,\n",
       " 10.121903781890868,\n",
       " 10.084015903472899,\n",
       " 10.061051910400389,\n",
       " 10.067552959442137,\n",
       " 10.081691307067869,\n",
       " 10.06233198547363,\n",
       " 10.045072074890134,\n",
       " 10.051053237915037,\n",
       " 10.049467353820798,\n",
       " 10.033644187927244,\n",
       " 10.013404220581053,\n",
       " 10.006322341918944,\n",
       " 9.991146800994871,\n",
       " 9.9844420509338363,\n",
       " 9.9795565376281719,\n",
       " 9.9608287391662582,\n",
       " 9.9291154212951636,\n",
       " 9.9081043243408171,\n",
       " 9.87410586929321,\n",
       " 9.815609577178952,\n",
       " 9.7655215759277318,\n",
       " 9.7228045921325652,\n",
       " 9.6519815025329549,\n",
       " 9.5954565086364703,\n",
       " 9.5322579803466745,\n",
       " 9.4442755165100039,\n",
       " 9.3561726074218683,\n",
       " 9.2655015068054141,\n",
       " 9.1847216720580995,\n",
       " 9.0810056037902775,\n",
       " 8.985559616088862,\n",
       " 8.900726413726801,\n",
       " 8.818184009551997,\n",
       " 8.7196309165954542,\n",
       " 8.6182861366271926,\n",
       " 8.4981519851684517,\n",
       " 8.3892762641906682,\n",
       " 8.2956714324951122,\n",
       " 8.1896498985290478,\n",
       " 8.0807403221130318,\n",
       " 7.9661528968810984,\n",
       " 7.8570609207153268,\n",
       " 7.7360665168762157,\n",
       " 7.5986842231750442,\n",
       " 7.4641418685913044,\n",
       " 7.3471608963012649,\n",
       " 7.2142908477783152,\n",
       " 7.0696981544494575,\n",
       " 6.9285130920410101,\n",
       " 6.7979643898010202,\n",
       " 6.6626968879699655,\n",
       " 6.5009523582458444,\n",
       " 6.3559661331176702,\n",
       " 6.2139087562560977,\n",
       " 6.0768667907714784,\n",
       " 5.9278958244323672,\n",
       " 5.7742285614013609,\n",
       " 5.6235077743530208,\n",
       " 5.4562630462646418,\n",
       " 5.3098236579894955,\n",
       " 5.1600922050476008,\n",
       " 5.0088677635192802,\n",
       " 4.8631776924133234,\n",
       " 4.6980615196227964,\n",
       " 4.5612495765685974,\n",
       " 4.4028984718322697,\n",
       " 4.240544158935541,\n",
       " 4.100170341491693,\n",
       " 3.942202255249017,\n",
       " 3.7733654403686461,\n",
       " 3.6162927436828549,\n",
       " 3.4606342124938902,\n",
       " 3.2856599121093688,\n",
       " 3.1406161422729428,\n",
       " 2.9612328720092709,\n",
       " 2.7888545455932552,\n",
       " 2.6130505218505795,\n",
       " 2.4459439811706476,\n",
       " 2.2613807220458919,\n",
       " 2.0967639694213802,\n",
       " 1.942997909545892,\n",
       " 1.7827421302795345,\n",
       " 1.5954699172973568,\n",
       " 1.4157984313964778,\n",
       " 1.233269626617425,\n",
       " 1.0517411117553643,\n",
       " 0.86230709457396781,\n",
       " 0.68529186248778617,\n",
       " 0.50718287658690719,\n",
       " 0.33527763748168254,\n",
       " 0.15780825042723917,\n",
       " 0.0071689910888602382,\n",
       " -0.15853495407105186,\n",
       " -0.33689362716675503,\n",
       " -0.50056902694702843,\n",
       " -0.65458704376221399,\n",
       " -0.83591133499146208,\n",
       " -1.0249114723205637,\n",
       " -1.2046491432190012,\n",
       " -1.3818821907043528,\n",
       " -1.5611520004272532,\n",
       " -1.7284426345825268,\n",
       " -1.8751561241149974,\n",
       " -2.019181118011482,\n",
       " -2.1827646560669018,\n",
       " -2.3074117546081618,\n",
       " -2.4447982025146562,\n",
       " -2.6309005508422931,\n",
       " -2.826981533050545,\n",
       " -3.0024558906555256,\n",
       " -3.1723554458618244,\n",
       " -3.3232749748230059,\n",
       " -3.4703156433105549,\n",
       " -3.5882091255188069,\n",
       " -3.6500859451294025,\n",
       " -3.7385207977295001,\n",
       " -3.7992900428772054,\n",
       " -3.8906441307067952,\n",
       " -3.9380323410034261,\n",
       " -4.0219302139282309,\n",
       " -4.0637607078552325,\n",
       " -4.1125015869140702,\n",
       " -4.1286851196289138,\n",
       " -4.1299814529419026,\n",
       " -4.1091312713623127,\n",
       " -4.0782949371337969,\n",
       " -4.0063914299011305,\n",
       " -3.9835549659729077,\n",
       " -3.906447937011726,\n",
       " -3.8359549179077219,\n",
       " -3.7551872062683178,\n",
       " -3.630502723693855,\n",
       " -3.5300069618225169,\n",
       " -3.4353302001953199,\n",
       " -3.3334144248962474,\n",
       " -3.2768722496032785,\n",
       " -3.1249145030975414,\n",
       " -2.9721988258361889,\n",
       " -2.8371757125854566,\n",
       " -2.6792481365203931,\n",
       " -2.5697102413177562,\n",
       " -2.4243988246917798,\n",
       " -2.261646612167366,\n",
       " -2.0997060394287184,\n",
       " -1.9321508026123122,\n",
       " -1.763479234695442,\n",
       " -1.6460592746734692,\n",
       " -1.4948775825500562,\n",
       " -1.3463200798034742,\n",
       " -1.2042144393920973,\n",
       " -1.0161494312286452,\n",
       " -0.83981461906433852,\n",
       " -0.66337651824951926,\n",
       " -0.49413078308106229,\n",
       " -0.31430020523072055,\n",
       " -0.14049485969544223,\n",
       " 0.01035733032225794,\n",
       " 0.16767247200011437,\n",
       " 0.32312627029418173,\n",
       " 0.4936469402313155,\n",
       " 0.66858119392394244,\n",
       " 0.83958062171935255,\n",
       " 1.0225882301330489,\n",
       " 1.1940623950958174,\n",
       " 1.3665287227630536,\n",
       " 1.5359864959716718,\n",
       " 1.6777680873870771,\n",
       " 1.8162064304351728,\n",
       " 1.9504391040801923,\n",
       " 2.0491100597381511,\n",
       " 2.1332593212127606,\n",
       " 2.230914213180534,\n",
       " 2.2788844356536786,\n",
       " 2.375169725418083,\n",
       " 2.4510599956512373,\n",
       " 2.4103300189971848,\n",
       " 2.4263259944915694,\n",
       " 2.3919733066558759,\n",
       " 2.326854436874382,\n",
       " 2.2296230640411299,\n",
       " 2.1286102008819503,\n",
       " 2.0154453144073408,\n",
       " 1.9051810894012373,\n",
       " 1.8006369495391767,\n",
       " 1.6890894451141278,\n",
       " 1.559074468612663,\n",
       " 1.4458594799041669,\n",
       " 1.3365096759796062,\n",
       " 1.2130816211700359,\n",
       " 1.0939673442840496,\n",
       " 0.98920927619933285,\n",
       " 0.86890379524230155,\n",
       " 0.72088060951232102,\n",
       " 0.60549865150450843,\n",
       " 0.48315732383727206,\n",
       " 0.36925536155699856,\n",
       " 0.24337427330016262,\n",
       " 0.14028935813902979,\n",
       " 0.015251268386832495,\n",
       " -0.11893577766419292,\n",
       " -0.22646980857849958,\n",
       " -0.30345960426331409,\n",
       " -0.39645548820496451,\n",
       " -0.50227501487732784,\n",
       " -0.54514242362976928,\n",
       " -0.58671439170838258,\n",
       " -0.61617254447937875,\n",
       " -0.62682536125183974,\n",
       " -0.65500797080994522,\n",
       " -0.65834919166565808,\n",
       " -0.64254862403870494,\n",
       " -0.61784254264832406,\n",
       " -0.60831504631043343,\n",
       " -0.57005521202088261,\n",
       " -0.52750005531311894,\n",
       " -0.49092617225647833,\n",
       " -0.42973736763001347,\n",
       " -0.37288162422181037,\n",
       " -0.29963775825501354,\n",
       " -0.24008503532410536,\n",
       " -0.15399226188660536,\n",
       " -0.11703089714051165,\n",
       " -0.026857065200814406,\n",
       " 0.04128808784483988,\n",
       " 0.11278959846495704,\n",
       " 0.099836816787710933,\n",
       " 0.11559730339049411,\n",
       " 0.14017243766783782,\n",
       " 0.19079075431822845,\n",
       " 0.24921276283263272,\n",
       " 0.33518584251402916,\n",
       " 0.4322710437774569,\n",
       " 0.50905140113829672,\n",
       " 0.59292220497130455,\n",
       " 0.64851311683653889,\n",
       " 0.67568312644957595,\n",
       " 0.68227724647521071,\n",
       " 0.72817095375060126,\n",
       " 0.72839899635314032,\n",
       " 0.76101804542540596,\n",
       " 0.759034791946402,\n",
       " 0.72381752967833557,\n",
       " 0.68580450248717351,\n",
       " 0.63989103889464416,\n",
       " 0.56873312568663636,\n",
       " 0.55043057441710508,\n",
       " 0.5173051624298004,\n",
       " 0.47357509803771053,\n",
       " 0.44759246253966367,\n",
       " 0.33803190803526911,\n",
       " 0.25438228416441949,\n",
       " 0.18806036567687062,\n",
       " 0.10158864784239796,\n",
       " 0.025598825454702628,\n",
       " -0.034881246566781779,\n",
       " -0.10741478538514117,\n",
       " -0.12866168022156696,\n",
       " -0.13952762413025838,\n",
       " -0.15030284690857873,\n",
       " -0.14651355552674281,\n",
       " -0.13589779472352015,\n",
       " -0.1220661373138522,\n",
       " -0.15535163688660614,\n",
       " -0.1545387210846042,\n",
       " -0.15647025489808075,\n",
       " -0.14373247337342254,\n",
       " -0.13349078559876437,\n",
       " -0.085805318832406968,\n",
       " -0.056253606796274176,\n",
       " -0.047792898178110138,\n",
       " -0.015867155075082823,\n",
       " 0.010553857803335148,\n",
       " 0.025962972640981602,\n",
       " 0.07812320518492688,\n",
       " 0.11873684501646982,\n",
       " 0.15977770805357919,\n",
       " 0.18128182792662606,\n",
       " 0.23472494316100104,\n",
       " 0.28847817802428227,\n",
       " 0.35703555870055181,\n",
       " 0.38852523231505376,\n",
       " 0.39054869651793461,\n",
       " 0.37003205299376468,\n",
       " 0.38199011802672367,\n",
       " 0.36904311180113775,\n",
       " 0.39173216819762213,\n",
       " 0.39674561882018072,\n",
       " 0.40050914192198739,\n",
       " 0.39062094306944833,\n",
       " 0.27309200477599127,\n",
       " 0.22752414894103035,\n",
       " 0.17985366249083501,\n",
       " 0.14246237754820804,\n",
       " 0.13385400199889161,\n",
       " 0.13341227149962404,\n",
       " 0.10440322685240723,\n",
       " 0.10587075614928221,\n",
       " 0.1115810031890771,\n",
       " 0.1179574337005517,\n",
       " 0.097106466293325117,\n",
       " 0.031034833908071199,\n",
       " -0.02864644432068858,\n",
       " -0.04116515541077647,\n",
       " -0.035851186752329206,\n",
       " -0.044294118881235467,\n",
       " -0.037141366958628061,\n",
       " -0.05518736076355972,\n",
       " -0.044024415969858566,\n",
       " -0.038981382370005063,\n",
       " -0.041907445907602717,\n",
       " -0.036885553359995313,\n",
       " -0.014200784683237513,\n",
       " -0.014211465835581263,\n",
       " 0.0080352764129538942,\n",
       " 0.032456068038930433,\n",
       " 0.075185991287221449,\n",
       " 0.11506595802306127,\n",
       " 0.15153022575377414,\n",
       " 0.19132646751402804,\n",
       " 0.23337855720519013,\n",
       " 0.28540934181212374,\n",
       " 0.34994025993346162,\n",
       " 0.38197403144835418,\n",
       " 0.42714586448668423,\n",
       " 0.46700759696959437,\n",
       " 0.50325272941588339,\n",
       " 0.4111273479461568,\n",
       " 0.33467252159117633,\n",
       " 0.26793666648863723,\n",
       " 0.19191050529478956,\n",
       " 0.11439918708800244,\n",
       " 0.056010210037221189,\n",
       " 0.0088588047027485031,\n",
       " -0.020984071731577691,\n",
       " -0.02115913581849176,\n",
       " -0.0079568614959820219,\n",
       " -0.0076543445587261699,\n",
       " -0.022260412216196901,\n",
       " -0.046739660263071928,\n",
       " -0.030290594100962572,\n",
       " -0.063296392440806329,\n",
       " -0.061689077377329773,\n",
       " -0.059235651016245804,\n",
       " -0.060561040878306376,\n",
       " -0.032730756759654062,\n",
       " -0.0052437152862653963,\n",
       " 0.015076093673695529,\n",
       " 0.0390264568328752,\n",
       " 0.071568159103382989,\n",
       " 0.079547048568715015,\n",
       " 0.11724261665343177,\n",
       " 0.14070406532286536,\n",
       " 0.15980137825011145,\n",
       " 0.19067164039610751,\n",
       " 0.2158793506622208,\n",
       " 0.23703924751280675,\n",
       " 0.2174856853485001,\n",
       " 0.18744710731505282,\n",
       " 0.18681427955626376,\n",
       " 0.14767664909361727,\n",
       " 0.13242627906798249,\n",
       " 0.13642063331602935,\n",
       " 0.13733886146544341,\n",
       " 0.17502492713927154,\n",
       " 0.20919295692442777,\n",
       " 0.23020327949522856,\n",
       " 0.23031031608580471,\n",
       " 0.254173974990834,\n",
       " 0.26665167427061914,\n",
       " 0.26251962471007229,\n",
       " 0.28443409538267966,\n",
       " 0.31573700141905658,\n",
       " 0.33296819877623429,\n",
       " 0.3530004100799452,\n",
       " 0.35781929588316785,\n",
       " 0.36765148353575572,\n",
       " 0.3148128032684217,\n",
       " 0.31301307868956429,\n",
       " 0.29346122169493538,\n",
       " 0.26886170387266972,\n",
       " 0.22342922019957398,\n",
       " 0.19631491279600949,\n",
       " 0.18835436058043328,\n",
       " 0.18177036857603873,\n",
       " 0.18004888725279652,\n",
       " 0.1759846210479625,\n",
       " 0.15922033119200543,\n",
       " 0.1445589237213023,\n",
       " 0.16673342323302104,\n",
       " 0.17739533424376322,\n",
       " 0.20532561302183938,\n",
       " 0.19610996437071632,\n",
       " 0.198544401168812,\n",
       " 0.20633655357359715,\n",
       " 0.20454383277891938,\n",
       " 0.22219661140440766,\n",
       " 0.07778323554991548,\n",
       " 0.0053975009918099948,\n",
       " -0.061658285141002508,\n",
       " -0.097763692855846282,\n",
       " -0.14992214012147131,\n",
       " -0.18487219429017249,\n",
       " -0.18009746742249672,\n",
       " -0.18434373664857095,\n",
       " -0.20602083778382488,\n",
       " -0.19314986991883468,\n",
       " -0.1712830791473503,\n",
       " -0.11438145256043625,\n",
       " -0.085875661849987059,\n",
       " -0.035765844345104256,\n",
       " 0.0087997379302863707,\n",
       " 0.058250951766956272,\n",
       " 0.098014429092395688,\n",
       " 0.12531702995299138,\n",
       " 0.072703371047962081,\n",
       " 0.046183778762805827,\n",
       " 0.019886915206897622,\n",
       " 0.022619405746448388,\n",
       " -0.0095600681305047612,\n",
       " -0.0062330341339227584,\n",
       " 0.02140579032896784,\n",
       " 0.038630651473987351,\n",
       " 0.038512655258167029,\n",
       " 0.06017922019957328,\n",
       " 0.093875112533557639,\n",
       " 0.124802148818958,\n",
       " 0.1684840641021611,\n",
       " 0.2077830944061162,\n",
       " 0.24286558341978806,\n",
       " 0.28577716636656536,\n",
       " 0.31407223701475873,\n",
       " 0.35085315513609661,\n",
       " 0.39489675331114543,\n",
       " 0.37977381324766885,\n",
       " 0.41447906684874303,\n",
       " 0.45454224205015903,\n",
       " 0.45507154273985628,\n",
       " 0.48095464134215116,\n",
       " 0.47465208244322532,\n",
       " 0.47751607322691669,\n",
       " 0.38002052879332293,\n",
       " 0.34131793785094011,\n",
       " 0.30233507728575454,\n",
       " 0.26190183830260022,\n",
       " 0.23245882987974864,\n",
       " 0.23257238578795175,\n",
       " 0.19163741111754157,\n",
       " 0.15620514488218998,\n",
       " 0.14341582298277589,\n",
       " 0.13174448585509033,\n",
       " 0.14257063865660399,\n",
       " 0.15112577247618403,\n",
       " 0.14350877571104731,\n",
       " 0.15763987541197505,\n",
       " 0.14235702323912347,\n",
       " 0.16739710426329335,\n",
       " 0.17173360633848866,\n",
       " 0.18442614173887925,\n",
       " 0.071877847671496436,\n",
       " 0.047263936996447597,\n",
       " 0.0099808826446409413,\n",
       " -0.049085424423230183,\n",
       " -0.079979612350476292,\n",
       " -0.094454153060925528,\n",
       " -0.10228854179383572,\n",
       " -0.098635824203503697,\n",
       " -0.092657960891736127,\n",
       " -0.096353914260876763,\n",
       " -0.076763532638562323,\n",
       " -0.070101877212536939,\n",
       " -0.07408929252625765,\n",
       " -0.061091505050671742,\n",
       " -0.032801668167126832,\n",
       " 0.0059324207305782237,\n",
       " -0.066596765518201095,\n",
       " -0.087365179061902276,\n",
       " -0.078370801925671835,\n",
       " -0.12583989524842576,\n",
       " -0.12546195030213672,\n",
       " -0.11117211723328908,\n",
       " -0.07444718742371878,\n",
       " -0.029600473403943416,\n",
       " -0.0082990245819219606,\n",
       " 0.0051810359954705901,\n",
       " 0.060334051132189308,\n",
       " 0.082804445266710769,\n",
       " 0.096740137100206861,\n",
       " 0.1158581066131463,\n",
       " 0.16436230278013847,\n",
       " 0.20784069252012868,\n",
       " 0.26318763160704273,\n",
       " 0.31792809867857591,\n",
       " 0.31034674644468918,\n",
       " 0.34629016304014815,\n",
       " 0.37577222633360519,\n",
       " 0.39888317680357588,\n",
       " 0.4349581394195427,\n",
       " 0.46826376152037275,\n",
       " 0.49820273017882,\n",
       " 0.51469500923155431,\n",
       " 0.51874592399595865,\n",
       " 0.53586308860777498,\n",
       " 0.53940558052061682,\n",
       " 0.32946323585508946,\n",
       " 0.30651518058775551,\n",
       " 0.26119808769224767,\n",
       " 0.2360635280609,\n",
       " 0.21918610191343904,\n",
       " 0.18803127479551909,\n",
       " 0.18123056983946439,\n",
       " 0.10830711174009915,\n",
       " 0.050271402358995604,\n",
       " 0.015068742752061985,\n",
       " -0.001731801986707554,\n",
       " -0.011473138809217348,\n",
       " -0.010585420608533755,\n",
       " -0.0069743709564341451,\n",
       " -0.012669523239148989,\n",
       " -0.023282960891736895,\n",
       " -0.059199476242078705,\n",
       " -0.08451086616517442,\n",
       " -0.11138371849061388,\n",
       " -0.099232866287244745,\n",
       " -0.10865130424500842,\n",
       " -0.10575514030457876,\n",
       " -0.1183738574981823,\n",
       " -0.088362409591688174,\n",
       " -0.062762247085584671,\n",
       " -0.05464141654969603,\n",
       " -0.020901948928846441,\n",
       " -0.0018062305450573836,\n",
       " 0.032117959976182844,\n",
       " 0.056883817672716039,\n",
       " 0.1018157062530383,\n",
       " 0.14226866340635863,\n",
       " 0.16543667411802856,\n",
       " 0.18921443748472777,\n",
       " 0.23441367912291136,\n",
       " 0.25499201011656369,\n",
       " 0.27797347450255,\n",
       " 0.28920509147642692,\n",
       " 0.28545710182188588,\n",
       " 0.31304098701475697,\n",
       " 0.30570725440977647,\n",
       " 0.31196749305723737,\n",
       " 0.29890401268004008,\n",
       " 0.28851127052305764,\n",
       " 0.2271812496185166,\n",
       " 0.19170444297789158,\n",
       " 0.14847591972349702,\n",
       " 0.11342087364195405,\n",
       " 0.078867021560655226,\n",
       " 0.055661733627305608,\n",
       " 0.04514628410337982,\n",
       " 0.031198995590196198,\n",
       " -0.013644727706922974,\n",
       " -0.022744737625135895,\n",
       " -0.046564378738417164,\n",
       " -0.080232072830214052,\n",
       " -0.093196550369276557,\n",
       " -0.090280096054091027,\n",
       " -0.083360322952284419,\n",
       " -0.095248170852675076,\n",
       " -0.084276136398329385,\n",
       " -0.059267724990858681,\n",
       " -0.057351713180555958,\n",
       " -0.043131628036512992,\n",
       " -0.031367456436171226,\n",
       " -0.029159235000624381,\n",
       " -0.0089368762970110996,\n",
       " 0.029490739822373643,\n",
       " 0.037140920639024015,\n",
       " 0.050245584487900945,\n",
       " 0.089279481887803269,\n",
       " 0.11226630210875051,\n",
       " 0.14161756706236378,\n",
       " 0.15868297767637746,\n",
       " 0.18787570571897999,\n",
       " 0.18363094711302294,\n",
       " 0.18265385627745162,\n",
       " 0.19694613075254924,\n",
       " 0.19447828483580118,\n",
       " 0.1598937854766703,\n",
       " 0.14092014884947304,\n",
       " 0.11842109870909216,\n",
       " 0.069820779800400745,\n",
       " 0.061830522537217124,\n",
       " 0.055501831054673156,\n",
       " 0.055292137145981753,\n",
       " 0.078211353301987613,\n",
       " 0.090267864227280559,\n",
       " 0.11314329910276884,\n",
       " 0.12885108947752469,\n",
       " 0.12661998748777858,\n",
       " 0.13594499206541527,\n",
       " 0.1131013545989846,\n",
       " 0.15460872077940449,\n",
       " 0.16336153221128924,\n",
       " 0.17727905464170915,\n",
       " 0.12701564025877457,\n",
       " 0.10016603469847181,\n",
       " 0.095569396972641704,\n",
       " 0.093319946289047953,\n",
       " 0.099446063995346778,\n",
       " 0.099687458038315524,\n",
       " 0.072523103713974704,\n",
       " 0.068605810165390718,\n",
       " 0.066901189804062566,\n",
       " 0.066453111648544963,\n",
       " 0.066307909011826205,\n",
       " 0.041859237670883821,\n",
       " 0.053092910766586947,\n",
       " 0.051693923950180673,\n",
       " 0.050086536407456056,\n",
       " 0.068096580505356441,\n",
       " 0.088985721588120098,\n",
       " 0.088781835556015612,\n",
       " 0.085806043624863243,\n",
       " 0.089725261688217706,\n",
       " 0.092789892196640522,\n",
       " 0.10772987174986316,\n",
       " 0.11201817703245592,\n",
       " 0.13726016044615219,\n",
       " 0.14548281860350082,\n",
       " 0.12071269607542463,\n",
       " 0.11925852584837385,\n",
       " 0.12051718902586407,\n",
       " 0.13777793502806132,\n",
       " 0.13271035003660622,\n",
       " 0.1253349189758152,\n",
       " 0.09654971504209936,\n",
       " 0.10921002578733859,\n",
       " 0.060246358871445013,\n",
       " 0.082430704116806336,\n",
       " 0.094094396591171539,\n",
       " 0.10662369728086879,\n",
       " 0.10391006469725063,\n",
       " 0.10105784034727502,\n",
       " 0.087285589218124598,\n",
       " 0.088857431411728097,\n",
       " 0.081262855529770084,\n",
       " 0.088262271881088442,\n",
       " 0.089949951171859929,\n",
       " 0.12102252578733842,\n",
       " 0.12678020858763137,\n",
       " 0.12885500335691846,\n",
       " 0.11925380897520457,\n",
       " 0.14229737663267528,\n",
       " 0.13789876937864692,\n",
       " 0.11503512954710393,\n",
       " 0.073552951812728917,\n",
       " 0.086909702301010139,\n",
       " 0.089688785552963265,\n",
       " 0.11194972991941834,\n",
       " 0.13100129318235779,\n",
       " 0.12882211303709409,\n",
       " 0.11660420989988707,\n",
       " 0.10191303443907161,\n",
       " 0.099564073562606742,\n",
       " 0.097852968215927039,\n",
       " 0.096095634460433871,\n",
       " 0.090536956787094017,\n",
       " 0.070159767150863514,\n",
       " 0.062230644226058819,\n",
       " 0.066577924728378146,\n",
       " 0.04349527549742109,\n",
       " 0.031779211044296068,\n",
       " 0.030647541046127096,\n",
       " 0.01846976852415444,\n",
       " 0.026279430389388798,\n",
       " 0.018506704330428815,\n",
       " 0.01340511894224522,\n",
       " -0.0066674156189120148,\n",
       " 0.010645875930770595,\n",
       " 0.024464097976669014,\n",
       " 0.047035429000838916,\n",
       " 0.058475219726546922,\n",
       " 0.03485832977293362,\n",
       " 0.037521522521957056,\n",
       " 0.068109149932845725,\n",
       " 0.076695867538436518,\n",
       " 0.08879781913755759,\n",
       " 0.073957750320418916,\n",
       " 0.058838636398299775,\n",
       " 0.075996782302840793,\n",
       " 0.062188159942611301,\n",
       " 0.0600710887908779,\n",
       " 0.060022947311385709,\n",
       " 0.061624570846541944,\n",
       " 0.055464653015121018,\n",
       " 0.026661018371566331,\n",
       " 0.029292045593246011,\n",
       " 0.026734760284408111,\n",
       " 0.034963348388656154,\n",
       " 0.0089416580200038093,\n",
       " -0.001441753387466908,\n",
       " 0.010551265716536992,\n",
       " 0.021249635696395375,\n",
       " 0.035545413970931507,\n",
       " 0.060090782165511572,\n",
       " 0.078430719375594551,\n",
       " 0.08486674690245001,\n",
       " 0.10285406112669315,\n",
       " 0.13464069747923219,\n",
       " 0.11422498130796754,\n",
       " 0.12230562400816283,\n",
       " 0.13569504165647825,\n",
       " 0.14261611938474972,\n",
       " 0.15222045135496456,\n",
       " 0.1683017177581628,\n",
       " 0.18366553115843134,\n",
       " 0.20732709121702508,\n",
       " 0.18848289108274774,\n",
       " 0.17536903953550653,\n",
       " 0.17520299911497428,\n",
       " 0.14803145408628776,\n",
       " 0.15009321022032093,\n",
       " 0.13464827537535023,\n",
       " 0.13385746955869984,\n",
       " 0.15749589729307481,\n",
       " 0.13818826103208845,\n",
       " 0.11944088935850444,\n",
       " 0.1038917160034019,\n",
       " 0.11479387092588722,\n",
       " 0.11942873764036475,\n",
       " 0.086707738876326657,\n",
       " 0.083787250518782691,\n",
       " 0.088282667160018022,\n",
       " 0.087236360549910574,\n",
       " 0.070524444580061921,\n",
       " 0.080449165344222057,\n",
       " 0.08940313911436365,\n",
       " 0.0871992683410482,\n",
       " 0.093106784820540384,\n",
       " 0.11642913055418296,\n",
       " 0.12237184143064779,\n",
       " 0.12640977859495442,\n",
       " 0.11140481185911456,\n",
       " 0.11076704597471514,\n",
       " 0.096667467117293265,\n",
       " 0.11073883628843584,\n",
       " 0.088822420120222942,\n",
       " 0.10425109481809888,\n",
       " 0.085424423217757076,\n",
       " 0.0652312984466389,\n",
       " 0.069459665298445522,\n",
       " 0.079864599227888869,\n",
       " 0.093427459716780473,\n",
       " 0.022166147232039232,\n",
       " 0.0070492496490314055,\n",
       " -0.012833333969132658,\n",
       " -0.057043401718156111,\n",
       " -0.086111225128190319,\n",
       " -0.092366220474259658,\n",
       " -0.082362884521500895,\n",
       " -0.076832609176652272,\n",
       " -0.085210947036759715,\n",
       " -0.063632217407243147,\n",
       " -0.058184507370011726,\n",
       " -0.050707679748551775,\n",
       " -0.080948015213029315,\n",
       " -0.093668304443375996,\n",
       " -0.11219025802613969,\n",
       " -0.096374408721940474,\n",
       " -0.069123321533219789,\n",
       " -0.066997825622575266,\n",
       " -0.069685062408463963,\n",
       " -0.038428525924699329,\n",
       " -0.053206689834611444,\n",
       " -0.025549606323258919,\n",
       " -0.0106435699463058,\n",
       " 0.0042209320068191999,\n",
       " 0.013368309020979356,\n",
       " 0.022588279724104356,\n",
       " 0.03476977539060825,\n",
       " 0.071917516708357276,\n",
       " 0.089845033645613123,\n",
       " 0.11192180252073519,\n",
       " 0.12613215255735627,\n",
       " 0.16012036705015409,\n",
       " 0.1492396850585769,\n",
       " 0.14685932540891869,\n",
       " 0.14602466392515404,\n",
       " 0.15558254814146261,\n",
       " 0.16669459915159443,\n",
       " 0.16341764068601825,\n",
       " 0.15893470573423601,\n",
       " 0.17892048835752702,\n",
       " 0.19010392189024186,\n",
       " 0.19627618598936292,\n",
       " 0.19599289894102306,\n",
       " 0.19831250953672616,\n",
       " 0.18693822669981211,\n",
       " 0.18433709144590585,\n",
       " 0.17938784980772224,\n",
       " 0.19346085929868903,\n",
       " 0.19785526466367923,\n",
       " 0.17874001693723879,\n",
       " 0.1318030834197827,\n",
       " 0.12300951957700926,\n",
       " 0.10475398445127682,\n",
       " 0.090317798614484815,\n",
       " 0.069725135803205496,\n",
       " 0.025831708908063869,\n",
       " -0.010547943115251562,\n",
       " -0.033502870559709588,\n",
       " -0.035391986846941059,\n",
       " -0.05609205245973406,\n",
       " -0.069991863250749686,\n",
       " -0.088972517013567079,\n",
       " -0.088443933486955756,\n",
       " -0.11614762687684835,\n",
       " -0.10478915786744897,\n",
       " -0.091159893035905992,\n",
       " -0.099763877868669693,\n",
       " -0.10433639526368925,\n",
       " -0.077784002304094532,\n",
       " -0.065996936798113096,\n",
       " -0.057919054031389479,\n",
       " -0.059093255996721508,\n",
       " -0.045318555831926587,\n",
       " -0.071634237289446151,\n",
       " -0.060614517211931521,\n",
       " -0.063756195068376847,\n",
       " -0.040331468582170814,\n",
       " -0.01376765632631146,\n",
       " -0.051871009826677701,\n",
       " -0.061949672698992174,\n",
       " -0.072868991851824216,\n",
       " -0.063056087493914087,\n",
       " -0.058832231521624062,\n",
       " -0.063694375991838914,\n",
       " -0.052942707061785207,\n",
       " -0.024311748504656321,\n",
       " -0.08763876152040341,\n",
       " -0.12345134544374327,\n",
       " -0.11798527145387512,\n",
       " -0.15170275497438296,\n",
       " -0.12722040176393376,\n",
       " -0.12919995498659004,\n",
       " -0.11283084487916818,\n",
       " -0.099514221191424043,\n",
       " -0.090538806915301015,\n",
       " -0.06900506591798658,\n",
       " -0.033406549453753182,\n",
       " 0.005144578933697963,\n",
       " 0.042953443527203795,\n",
       " 0.054465669631940106,\n",
       " 0.079604026794415667,\n",
       " 0.10954851341245767,\n",
       " 0.1064232902526676,\n",
       " 0.13778861427305333,\n",
       " 0.15630124092100253,\n",
       " 0.13941596603391754,\n",
       " 0.15293287467954739,\n",
       " 0.16117953300474266,\n",
       " 0.16752463912962057,\n",
       " 0.14851743698118305,\n",
       " 0.15782354354856587,\n",
       " 0.12692901229856585,\n",
       " 0.16378482627866839,\n",
       " 0.18772932624815078,\n",
       " 0.18105781555173964,\n",
       " 0.18453890037534804,\n",
       " 0.18030019378660289,\n",
       " 0.18239739608762826,\n",
       " 0.17412929534910285,\n",
       " 0.17479300498960579,\n",
       " 0.1667842693328675,\n",
       " 0.16873739051817022,\n",
       " 0.15124497413633428,\n",
       " 0.17897244834898074,\n",
       " 0.16880151176450808,\n",
       " 0.1508877811431702,\n",
       " 0.099590194702130153,\n",
       " 0.088655612945538351,\n",
       " 0.058991506576519796,\n",
       " 0.027389493942242424,\n",
       " 0.014765083312969942,\n",
       " 0.011811370849591007,\n",
       " -0.022004053115863124,\n",
       " -0.013596784591693202,\n",
       " -0.026319883346576027,\n",
       " -0.021551572799701046,\n",
       " -0.012980375289935429,\n",
       " -0.027467237472552631,\n",
       " -0.021183277130145405,\n",
       " -0.032821395874041918,\n",
       " -0.10828477478029191,\n",
       " -0.14632654762269914,\n",
       " -0.16592127418519914,\n",
       " -0.18502392768861714,\n",
       " -0.1997635555267519,\n",
       " -0.24312341308595603,\n",
       " -0.231438676834125,\n",
       " -0.21371436500551175,\n",
       " -0.21505323600770901,\n",
       " -0.19209560203554105,\n",
       " -0.16987887001039456,\n",
       " -0.16414416694642972,\n",
       " -0.15629744338991119,\n",
       " -0.14514505195619537,\n",
       " -0.12316905784608795,\n",
       " -0.1084781475067325,\n",
       " -0.12775504112245517,\n",
       " -0.10181254386903721,\n",
       " -0.079370790481586045,\n",
       " -0.069916681289691515,\n",
       " -0.075514774322528441,\n",
       " -0.06449911499025307,\n",
       " -0.063834493637103681,\n",
       " -0.07681069946290936,\n",
       " -0.048208837509174027,\n",
       " -0.0080699939727971043,\n",
       " 0.025594362258892327,\n",
       " 0.042804798126201901,\n",
       " 0.081906288146953832,\n",
       " 0.11720026779172922,\n",
       " 0.10740020751951242,\n",
       " 0.13359341049192452,\n",
       " 0.13792141532896066,\n",
       " 0.15345701026914621,\n",
       " 0.12770362472532293,\n",
       " 0.14200476074216861,\n",
       " 0.14697240066526429,\n",
       " 0.15378120994565977,\n",
       " 0.14288566780088435,\n",
       " 0.13928070640562068,\n",
       " 0.15907542610166558,\n",
       " 0.16431738662717826,\n",
       " 0.16938982200620656,\n",
       " 0.15305284690855031,\n",
       " 0.13071275901792531,\n",
       " 0.12703345108030323,\n",
       " 0.13305902481077198,\n",
       " 0.11805374908445358,\n",
       " 0.12003217887876509,\n",
       " 0.097872613906841252,\n",
       " 0.072791690826396888,\n",
       " 0.091763353347759188,\n",
       " 0.080411701202373453,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
