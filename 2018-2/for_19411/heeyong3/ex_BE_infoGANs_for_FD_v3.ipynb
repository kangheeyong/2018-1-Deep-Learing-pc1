{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum infoGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_infoGANs_for_FD_v3'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     123
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_epoch = 50\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.001\n",
    "gamma = 0.7\n",
    "k_curr = 0.0\n",
    "c_size = 10\n",
    "\n",
    "\n",
    "def G(x,c,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "        x_concat = tf.concat([x,c],3)\n",
    "        conv1 = tf.layers.conv2d_transpose(x_concat,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,1024,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "        #\n",
    "        \n",
    "        fc0  = tf.reshape(conv4, (-1, 4*4*1024))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[4*4*1024, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "        fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "        \n",
    "        \n",
    "    r5 = tf.nn.tanh(tf.layers.batch_normalization(conv5,training=isTrain), name = name)#4*4*512\n",
    "  \n",
    "  \n",
    "    return r5, tf.reshape(fc1,(-1,1,1,c_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "def Q_cat(x,reuse = False, name = 'Q_cat') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('Q_cat', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        fc0  = tf.reshape(x, (-1, 100))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[100, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "    fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "    \n",
    "    return tf.reshape(fc1, (-1,1,1,c_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "c = tf.placeholder(tf.float32,shape=(None,1,1,c_size),name = 'c')    #x_z = G(z,c)\n",
    "\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,c,name='G_sample') # G(z)\n",
    "E_z, E_c = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z,E_c, isTrain, reuse=True, name ='re_image')\n",
    "re_z, re_c = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "\n",
    "\n",
    "re_z_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z - z)**2, axis=[1,2,3])) , name = 're_z_loss') \n",
    "re_c_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(re_c + 1e-8), axis = [1,2,3]),name = 're_c_loss')\n",
    "re_image_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_image - u)**2, axis=[1,2,3])) , name = 're_image_loss') \n",
    "\n",
    "\n",
    "E_loss = tf.add(re_z_loss, re_c_loss, name = 'E_loss')                       \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "Q_fake = Q_cat(D_enc(G_sample, isTrain,reuse=True), reuse=False, name='Q_fake')\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "Q_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(Q_fake + 1e-8), axis = [1,2,3]),name = 'Q_loss')\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "Q_vars = [var for var in T_vars if var.name.startswith('Q')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss + Q_loss, var_list=G_vars+Q_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    E_AE_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(re_image_loss, var_list=E_vars, name='E_AE_optim')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 30.714, D_real_e : 40.475, D_fake_e : 26.956, G_e : 27.525, Q_e : 2.127, new_measure : 47.387, k_curr : 0.226277\n",
      "D_e : 23.627, D_real_e : 26.613, D_fake_e : 18.862, G_e : 19.158, Q_e : 2.108, new_measure : 27.943, k_curr : 0.078170\n",
      "D_e : 19.634, D_real_e : 20.005, D_fake_e : 14.188, G_e : 14.267, Q_e : 2.039, new_measure : 21.672, k_curr : 0.004366\n",
      "D_e : 15.989, D_real_e : 15.927, D_fake_e : 11.070, G_e : 11.078, Q_e : 1.938, new_measure : 17.220, k_curr : 0.024312\n",
      "D_e : 12.979, D_real_e : 13.080, D_fake_e : 9.260, G_e : 9.276, Q_e : 1.778, new_measure : 14.028, k_curr : -0.009285\n",
      "D_e : 11.604, D_real_e : 11.417, D_fake_e : 8.011, G_e : 8.016, Q_e : 1.598, new_measure : 12.275, k_curr : -0.015864\n",
      "D_e : 10.254, D_real_e : 10.144, D_fake_e : 7.030, G_e : 7.058, Q_e : 1.429, new_measure : 10.866, k_curr : -0.003694\n",
      "D_e : 9.186, D_real_e : 9.271, D_fake_e : 6.338, G_e : 6.392, Q_e : 1.281, new_measure : 9.873, k_curr : 0.023752\n",
      "D_e : 8.687, D_real_e : 8.710, D_fake_e : 6.206, G_e : 6.262, Q_e : 1.157, new_measure : 9.307, k_curr : -0.022612\n",
      "D_e : 8.296, D_real_e : 8.352, D_fake_e : 5.589, G_e : 5.651, Q_e : 1.052, new_measure : 8.897, k_curr : 0.032085\n",
      "D_e : 7.672, D_real_e : 7.778, D_fake_e : 5.431, G_e : 5.493, Q_e : 0.965, new_measure : 8.215, k_curr : 0.018542\n",
      "D_e : 7.154, D_real_e : 7.252, D_fake_e : 4.981, G_e : 5.058, Q_e : 0.891, new_measure : 7.578, k_curr : 0.023770\n",
      "D_e : 6.447, D_real_e : 6.523, D_fake_e : 4.509, G_e : 4.576, Q_e : 0.828, new_measure : 6.810, k_curr : 0.020911\n",
      "D_e : 5.992, D_real_e : 6.082, D_fake_e : 4.183, G_e : 4.262, Q_e : 0.774, new_measure : 6.316, k_curr : 0.019615\n",
      "D_e : 5.780, D_real_e : 5.861, D_fake_e : 4.014, G_e : 4.088, Q_e : 0.726, new_measure : 6.099, k_curr : 0.023683\n",
      "D_e : 5.617, D_real_e : 5.709, D_fake_e : 3.923, G_e : 4.006, Q_e : 0.683, new_measure : 5.959, k_curr : 0.020920\n",
      "D_e : 5.466, D_real_e : 5.562, D_fake_e : 3.775, G_e : 3.852, Q_e : 0.645, new_measure : 5.788, k_curr : 0.032326\n",
      "D_e : 5.342, D_real_e : 5.451, D_fake_e : 3.720, G_e : 3.807, Q_e : 0.611, new_measure : 5.666, k_curr : 0.034835\n",
      "D_e : 5.238, D_real_e : 5.382, D_fake_e : 3.675, G_e : 3.772, Q_e : 0.581, new_measure : 5.602, k_curr : 0.033551\n",
      "D_e : 5.157, D_real_e : 5.279, D_fake_e : 3.602, G_e : 3.702, Q_e : 0.553, new_measure : 5.492, k_curr : 0.031670\n",
      "D_e : 5.119, D_real_e : 5.219, D_fake_e : 3.566, G_e : 3.666, Q_e : 0.528, new_measure : 5.427, k_curr : 0.028133\n",
      "D_e : 5.033, D_real_e : 5.136, D_fake_e : 3.472, G_e : 3.574, Q_e : 0.505, new_measure : 5.364, k_curr : 0.034042\n",
      "D_e : 4.980, D_real_e : 5.079, D_fake_e : 3.480, G_e : 3.589, Q_e : 0.484, new_measure : 5.309, k_curr : 0.024374\n",
      "D_e : 4.923, D_real_e : 5.001, D_fake_e : 3.406, G_e : 3.506, Q_e : 0.464, new_measure : 5.215, k_curr : 0.022843\n",
      "D_e : 4.888, D_real_e : 4.971, D_fake_e : 3.371, G_e : 3.477, Q_e : 0.446, new_measure : 5.189, k_curr : 0.023607\n",
      "D_e : 4.849, D_real_e : 4.934, D_fake_e : 3.349, G_e : 3.458, Q_e : 0.429, new_measure : 5.149, k_curr : 0.022449\n",
      "D_e : 4.798, D_real_e : 4.874, D_fake_e : 3.304, G_e : 3.407, Q_e : 0.414, new_measure : 5.092, k_curr : 0.023686\n",
      "D_e : 4.742, D_real_e : 4.814, D_fake_e : 3.261, G_e : 3.364, Q_e : 0.399, new_measure : 5.016, k_curr : 0.025527\n",
      "D_e : 4.708, D_real_e : 4.778, D_fake_e : 3.248, G_e : 3.364, Q_e : 0.386, new_measure : 5.007, k_curr : 0.019982\n",
      "D_e : 4.669, D_real_e : 4.734, D_fake_e : 3.217, G_e : 3.333, Q_e : 0.373, new_measure : 4.979, k_curr : 0.014591\n",
      "D_e : 4.614, D_real_e : 4.677, D_fake_e : 3.134, G_e : 3.248, Q_e : 0.361, new_measure : 4.909, k_curr : 0.021943\n",
      "D_e : 4.598, D_real_e : 4.666, D_fake_e : 3.161, G_e : 3.268, Q_e : 0.350, new_measure : 4.898, k_curr : 0.021524\n",
      "D_e : 4.576, D_real_e : 4.635, D_fake_e : 3.127, G_e : 3.242, Q_e : 0.340, new_measure : 4.863, k_curr : 0.022083\n",
      "D_e : 4.533, D_real_e : 4.592, D_fake_e : 3.099, G_e : 3.207, Q_e : 0.330, new_measure : 4.821, k_curr : 0.024002\n",
      "D_e : 4.497, D_real_e : 4.565, D_fake_e : 3.079, G_e : 3.193, Q_e : 0.321, new_measure : 4.762, k_curr : 0.024642\n",
      "D_e : 4.463, D_real_e : 4.531, D_fake_e : 3.068, G_e : 3.175, Q_e : 0.312, new_measure : 4.754, k_curr : 0.023841\n",
      "D_e : 4.416, D_real_e : 4.488, D_fake_e : 3.028, G_e : 3.137, Q_e : 0.304, new_measure : 4.698, k_curr : 0.025323\n",
      "D_e : 4.400, D_real_e : 4.468, D_fake_e : 3.020, G_e : 3.131, Q_e : 0.296, new_measure : 4.675, k_curr : 0.024476\n",
      "D_e : 4.379, D_real_e : 4.446, D_fake_e : 3.025, G_e : 3.130, Q_e : 0.288, new_measure : 4.657, k_curr : 0.019561\n",
      "D_e : 4.342, D_real_e : 4.405, D_fake_e : 2.962, G_e : 3.066, Q_e : 0.281, new_measure : 4.618, k_curr : 0.024503\n",
      "D_e : 4.336, D_real_e : 4.401, D_fake_e : 2.973, G_e : 3.088, Q_e : 0.275, new_measure : 4.623, k_curr : 0.022498\n",
      "D_e : 4.293, D_real_e : 4.355, D_fake_e : 2.954, G_e : 3.049, Q_e : 0.268, new_measure : 4.563, k_curr : 0.022302\n",
      "D_e : 4.274, D_real_e : 4.337, D_fake_e : 2.933, G_e : 3.044, Q_e : 0.262, new_measure : 4.541, k_curr : 0.019949\n",
      "D_e : 4.276, D_real_e : 4.334, D_fake_e : 2.937, G_e : 3.038, Q_e : 0.256, new_measure : 4.540, k_curr : 0.018717\n",
      "D_e : 4.232, D_real_e : 4.295, D_fake_e : 2.914, G_e : 3.012, Q_e : 0.251, new_measure : 4.496, k_curr : 0.017113\n",
      "D_e : 4.198, D_real_e : 4.251, D_fake_e : 2.861, G_e : 2.958, Q_e : 0.245, new_measure : 4.476, k_curr : 0.021956\n",
      "D_e : 4.207, D_real_e : 4.265, D_fake_e : 2.868, G_e : 2.985, Q_e : 0.240, new_measure : 4.487, k_curr : 0.022097\n",
      "D_e : 4.172, D_real_e : 4.233, D_fake_e : 2.858, G_e : 2.965, Q_e : 0.235, new_measure : 4.428, k_curr : 0.021500\n",
      "D_e : 4.144, D_real_e : 4.206, D_fake_e : 2.845, G_e : 2.951, Q_e : 0.230, new_measure : 4.391, k_curr : 0.019680\n",
      "D_e : 4.138, D_real_e : 4.202, D_fake_e : 2.800, G_e : 2.912, Q_e : 0.226, new_measure : 4.398, k_curr : 0.028107\n",
      "total time :  4769.59327507019\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5596c2ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.set_random_seed(int(time.time()))\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "    \n",
    "    one_hot = np.eye(c_size)\n",
    "    temp2 = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4])\n",
    "    test_c = one_hot[temp2].reshape([-1,1,1,c_size])\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    Q_error=[]\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            temp1 = np.random.randint(0,10,(batch_size))                                                                                                                                     \n",
    "            c_ = one_hot[temp1].reshape([-1,1,1,c_size])\n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, c : c_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e,Q_e = sess.run([G_optim, G_loss,Q_loss], {u : u_, z : z_, c : c_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            Q_error.append(Q_e)\n",
    "\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, Q_e : %.3f, new_measure : %.3f, k_curr : %3f'\n",
    "              %(np.mean(D_error), np.mean(D_real_error),np.mean(D_fake_error), np.mean(G_error),\n",
    "                np.mean(Q_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, c : test_c, isTrain : False})       \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "    \n",
    "    \n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_infoGANs_for_FD_v3/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 're_z:0' shape=(?, 1, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.019468553543090822,\n",
       " 0.04026400299072265,\n",
       " 0.061689060211181632,\n",
       " 0.08362643013000487,\n",
       " 0.10600821037292479,\n",
       " 0.12886114692687986,\n",
       " 0.1520805812835693,\n",
       " 0.17578715171813961,\n",
       " 0.19987262649536128,\n",
       " 0.22393819694519038,\n",
       " 0.24826513442993159,\n",
       " 0.27283307113647454,\n",
       " 0.29758060379028312,\n",
       " 0.32229133148193351,\n",
       " 0.34720723228454581,\n",
       " 0.37189029312133781,\n",
       " 0.39639181613922109,\n",
       " 0.42071768245697011,\n",
       " 0.44499780426025382,\n",
       " 0.46901111660003653,\n",
       " 0.49261714572906484,\n",
       " 0.51594084434509269,\n",
       " 0.53905758399963366,\n",
       " 0.56182398662567123,\n",
       " 0.58430557060241683,\n",
       " 0.60616753330230699,\n",
       " 0.62753137092590316,\n",
       " 0.64807164688110341,\n",
       " 0.66829618263244617,\n",
       " 0.68752054672241203,\n",
       " 0.70566615295410151,\n",
       " 0.72296960525512688,\n",
       " 0.73953231849670398,\n",
       " 0.75411979751586899,\n",
       " 0.76756458091735824,\n",
       " 0.78048499565124496,\n",
       " 0.79091335601806623,\n",
       " 0.79979460220336895,\n",
       " 0.80840978507995587,\n",
       " 0.81618667297363268,\n",
       " 0.82279196090698226,\n",
       " 0.82740845069885238,\n",
       " 0.83094952278137191,\n",
       " 0.8339269027709959,\n",
       " 0.83745334663391091,\n",
       " 0.84067957572936991,\n",
       " 0.84307582626342747,\n",
       " 0.84628199653625458,\n",
       " 0.84998945236206025,\n",
       " 0.85354210510253881,\n",
       " 0.85843277282714814,\n",
       " 0.86401010093688935,\n",
       " 0.86959574394226047,\n",
       " 0.87372842140197726,\n",
       " 0.87486462059020964,\n",
       " 0.87518773193359345,\n",
       " 0.87349482574462856,\n",
       " 0.87391418838500945,\n",
       " 0.87430175094604456,\n",
       " 0.87112085762023894,\n",
       " 0.86519712295532192,\n",
       " 0.85691587486267051,\n",
       " 0.84759860992431602,\n",
       " 0.83747561683654748,\n",
       " 0.82706134948730436,\n",
       " 0.81785111846923797,\n",
       " 0.80945709648132291,\n",
       " 0.80030684661865203,\n",
       " 0.79091059799194308,\n",
       " 0.78258192749023414,\n",
       " 0.77034069290161111,\n",
       " 0.75782247695922833,\n",
       " 0.74924486236572252,\n",
       " 0.73863971710205067,\n",
       " 0.72893212242126459,\n",
       " 0.71879528160095207,\n",
       " 0.70820982704162594,\n",
       " 0.69625282630920404,\n",
       " 0.68386384086608876,\n",
       " 0.67289692344665519,\n",
       " 0.6590494705200195,\n",
       " 0.64509543609619135,\n",
       " 0.63254879722595214,\n",
       " 0.61883133964538573,\n",
       " 0.60455450782775877,\n",
       " 0.59125314178466792,\n",
       " 0.57705711975097651,\n",
       " 0.56168248901367179,\n",
       " 0.54750982170104967,\n",
       " 0.53383855400085434,\n",
       " 0.52057978019714335,\n",
       " 0.50651806983947734,\n",
       " 0.49193065681457498,\n",
       " 0.47719571266174293,\n",
       " 0.46502254829406714,\n",
       " 0.45278797950744604,\n",
       " 0.43819214363098119,\n",
       " 0.4242638969421384,\n",
       " 0.40969194450378388,\n",
       " 0.39513392028808564,\n",
       " 0.38070831565856905,\n",
       " 0.36745979614257784,\n",
       " 0.35658732643127411,\n",
       " 0.34187685241699189,\n",
       " 0.33033921279907197,\n",
       " 0.31570948104858371,\n",
       " 0.30165548706054657,\n",
       " 0.28922354583740201,\n",
       " 0.27714253654479948,\n",
       " 0.26446807899475067,\n",
       " 0.25155355949401825,\n",
       " 0.2390623031616208,\n",
       " 0.22763726387023897,\n",
       " 0.21675738983154269,\n",
       " 0.205501513290405,\n",
       " 0.19597563514709446,\n",
       " 0.18610153999328585,\n",
       " 0.17660576171874973,\n",
       " 0.16640192260742159,\n",
       " 0.1587444435119626,\n",
       " 0.14684189910888643,\n",
       " 0.13888256950378389,\n",
       " 0.13103964920043915,\n",
       " 0.12473143920898407,\n",
       " 0.11759486160278289,\n",
       " 0.10932659950256315,\n",
       " 0.10353054695129363,\n",
       " 0.096732603073119802,\n",
       " 0.088503089523315107,\n",
       " 0.08232234611511198,\n",
       " 0.077253139495849277,\n",
       " 0.076180219650268222,\n",
       " 0.074650219345092445,\n",
       " 0.072038418579101232,\n",
       " 0.070187131500243805,\n",
       " 0.068304891586303373,\n",
       " 0.068497438049316073,\n",
       " 0.068336726760863928,\n",
       " 0.067956021881103182,\n",
       " 0.064864107131957671,\n",
       " 0.065367123413085593,\n",
       " 0.066709581375121729,\n",
       " 0.065217575836181291,\n",
       " 0.067397533035277971,\n",
       " 0.068460448074340471,\n",
       " 0.069436638259887343,\n",
       " 0.069507202529906872,\n",
       " 0.071001408386230119,\n",
       " 0.072344144439696917,\n",
       " 0.074759581756591448,\n",
       " 0.076163774871825823,\n",
       " 0.081566653060912736,\n",
       " 0.08440289268493617,\n",
       " 0.087571948242187142,\n",
       " 0.088073784637450817,\n",
       " 0.08772286033630336,\n",
       " 0.088763397979735981,\n",
       " 0.090787070465087535,\n",
       " 0.091581409072875625,\n",
       " 0.094220341110229144,\n",
       " 0.098316644668578757,\n",
       " 0.10103920326232875,\n",
       " 0.10244915046691859,\n",
       " 0.10605724372863734,\n",
       " 0.10742018318176234,\n",
       " 0.11213417549133266,\n",
       " 0.11452659378051723,\n",
       " 0.11851061897277797,\n",
       " 0.12348822937011683,\n",
       " 0.12659600296020471,\n",
       " 0.13014302749633752,\n",
       " 0.13431909027099573,\n",
       " 0.13752138710021936,\n",
       " 0.14135538635253869,\n",
       " 0.1412835056304928,\n",
       " 0.14605837898254359,\n",
       " 0.14875860404968227,\n",
       " 0.1538880764007565,\n",
       " 0.15913005180358852,\n",
       " 0.1617310855865475,\n",
       " 0.16438151321411099,\n",
       " 0.16654934310913053,\n",
       " 0.16939381294250455,\n",
       " 0.17158391342163051,\n",
       " 0.17397188110351527,\n",
       " 0.17621932449340785,\n",
       " 0.17755708427429165,\n",
       " 0.18057055625915494,\n",
       " 0.18403266296386686,\n",
       " 0.18665398674011197,\n",
       " 0.19054315643310513,\n",
       " 0.19257399864196742,\n",
       " 0.19483465232849084,\n",
       " 0.19733461532592736,\n",
       " 0.19981662597656213,\n",
       " 0.20151145973205531,\n",
       " 0.20536588096618616,\n",
       " 0.20566550140380824,\n",
       " 0.20437894477844201,\n",
       " 0.2034425476074215,\n",
       " 0.20309645309448204,\n",
       " 0.20447701454162559,\n",
       " 0.20315302314758263,\n",
       " 0.20297878913879355,\n",
       " 0.20223191184997519,\n",
       " 0.20503504104614217,\n",
       " 0.20544411582946737,\n",
       " 0.20705085334777792,\n",
       " 0.20710728874206502,\n",
       " 0.20474871597289998,\n",
       " 0.20238407211303669,\n",
       " 0.20207911186218219,\n",
       " 0.20195173683166462,\n",
       " 0.20067254104614216,\n",
       " 0.19919433021545369,\n",
       " 0.2018974182128902,\n",
       " 0.19864843826293901,\n",
       " 0.19901283493041949,\n",
       " 0.19858466682434039,\n",
       " 0.19486223945617634,\n",
       " 0.19559216308593708,\n",
       " 0.19408249626159627,\n",
       " 0.19451002349853475,\n",
       " 0.1921331302642818,\n",
       " 0.19414734764099079,\n",
       " 0.19536543884277302,\n",
       " 0.1963955165863033,\n",
       " 0.19678601341247517,\n",
       " 0.19684208450317342,\n",
       " 0.19836448326110798,\n",
       " 0.19996758499145464,\n",
       " 0.20303783798217731,\n",
       " 0.20329564361572222,\n",
       " 0.20277828445434526,\n",
       " 0.20228175201415971,\n",
       " 0.20161216583251909,\n",
       " 0.20259390144348099,\n",
       " 0.20406267623901322,\n",
       " 0.20271276969909624,\n",
       " 0.20516885643005328,\n",
       " 0.20551826705932574,\n",
       " 0.20358438301086382,\n",
       " 0.20248705635070757,\n",
       " 0.2026370422363277,\n",
       " 0.20171861591339069,\n",
       " 0.20250612735748247,\n",
       " 0.20223233337402299,\n",
       " 0.20106596603393509,\n",
       " 0.1991756484985347,\n",
       " 0.19935229911804153,\n",
       " 0.20039141044616654,\n",
       " 0.20023968811035112,\n",
       " 0.20219562416076614,\n",
       " 0.20337447547912552,\n",
       " 0.20532699127197218,\n",
       " 0.20757678565978957,\n",
       " 0.20457439746856643,\n",
       " 0.2047803686141963,\n",
       " 0.20498998146057082,\n",
       " 0.20744070034027054,\n",
       " 0.20946648044586136,\n",
       " 0.21055532169341995,\n",
       " 0.21119596347808792,\n",
       " 0.2101003892898555,\n",
       " 0.21084626712799026,\n",
       " 0.21042088718414259,\n",
       " 0.21218436183929396,\n",
       " 0.21378236064910841,\n",
       " 0.2164147233963008,\n",
       " 0.2183413908004756,\n",
       " 0.21853379344940138,\n",
       " 0.2214361806869502,\n",
       " 0.22344598026275586,\n",
       " 0.22312030239105177,\n",
       " 0.22447998218536327,\n",
       " 0.22651652660369823,\n",
       " 0.22669145908355662,\n",
       " 0.22779977493286083,\n",
       " 0.22820124588012644,\n",
       " 0.22627671756744333,\n",
       " 0.22805459938049263,\n",
       " 0.22616597709655709,\n",
       " 0.22414368476867622,\n",
       " 0.22145523986816351,\n",
       " 0.22226815261840766,\n",
       " 0.22348442001342719,\n",
       " 0.22183151531219428,\n",
       " 0.22266893939971868,\n",
       " 0.22099524021148625,\n",
       " 0.21876828765869086,\n",
       " 0.21460999889373725,\n",
       " 0.21478688716888372,\n",
       " 0.21463431243896428,\n",
       " 0.21335733070373478,\n",
       " 0.21454467391967716,\n",
       " 0.21623872871398869,\n",
       " 0.21445447902679388,\n",
       " 0.21425370082855169,\n",
       " 0.21636610584258978,\n",
       " 0.21722248744964545,\n",
       " 0.21739124698638862,\n",
       " 0.21734857444763128,\n",
       " 0.21751262607574406,\n",
       " 0.21845952167510929,\n",
       " 0.21854515895843449,\n",
       " 0.21749090938568058,\n",
       " 0.21820985126495304,\n",
       " 0.2175204881668085,\n",
       " 0.22059281787872256,\n",
       " 0.22168855781555116,\n",
       " 0.22059201259612979,\n",
       " 0.21996267395019473,\n",
       " 0.21930625991821232,\n",
       " 0.21733409175872745,\n",
       " 0.21616304721832216,\n",
       " 0.21686203117370545,\n",
       " 0.21652633323669374,\n",
       " 0.21895629348754822,\n",
       " 0.21518051166534363,\n",
       " 0.21340054988861024,\n",
       " 0.2134739339828485,\n",
       " 0.21427506561279236,\n",
       " 0.21596655693054137,\n",
       " 0.21828266105651795,\n",
       " 0.21871420440673767,\n",
       " 0.22062680225372253,\n",
       " 0.22176359901428161,\n",
       " 0.21852656955718933,\n",
       " 0.21941862335205017,\n",
       " 0.2197623577117914,\n",
       " 0.21888003959655702,\n",
       " 0.21566479835510194,\n",
       " 0.20768931655883729,\n",
       " 0.20686753101348818,\n",
       " 0.20437920761108339,\n",
       " 0.20246479969024597,\n",
       " 0.20258903045654236,\n",
       " 0.20331715602874695,\n",
       " 0.20421543273925721,\n",
       " 0.20332249374389588,\n",
       " 0.20006540069580017,\n",
       " 0.2012295061111444,\n",
       " 0.20067993869781431,\n",
       " 0.19829094276428158,\n",
       " 0.20011211471557552,\n",
       " 0.20232897338867123,\n",
       " 0.19960532817840512,\n",
       " 0.19404461097717221,\n",
       " 0.19041435298919612,\n",
       " 0.18834000720977717,\n",
       " 0.18887531394958429,\n",
       " 0.18763288478851251,\n",
       " 0.18607615261077812,\n",
       " 0.18701260986328055,\n",
       " 0.18669301319122245,\n",
       " 0.18418252429962087,\n",
       " 0.18426774234771656,\n",
       " 0.18572861957549977,\n",
       " 0.1850653444290154,\n",
       " 0.1869071609497063,\n",
       " 0.17654821205139087,\n",
       " 0.17375121536254809,\n",
       " 0.1704685050964348,\n",
       " 0.16914175529479905,\n",
       " 0.1683942115783684,\n",
       " 0.16708664569854662,\n",
       " 0.16677775936126635,\n",
       " 0.16613401851653978,\n",
       " 0.16706728553771896,\n",
       " 0.16446445198059007,\n",
       " 0.16481453762054368,\n",
       " 0.16257462539672776,\n",
       " 0.16252094345092699,\n",
       " 0.16241083431243822,\n",
       " 0.15926829795837327,\n",
       " 0.1538246002197258,\n",
       " 0.15298535423278731,\n",
       " 0.14886788921356123,\n",
       " 0.14984603900909346,\n",
       " 0.14692937602996747,\n",
       " 0.1445381223678581,\n",
       " 0.14500324745178145,\n",
       " 0.14217510051727217,\n",
       " 0.14242299118041915,\n",
       " 0.13970661468505782,\n",
       " 0.13819851818084639,\n",
       " 0.13848424873351972,\n",
       " 0.14032626209258955,\n",
       " 0.14126181373596114,\n",
       " 0.14247556743621748,\n",
       " 0.14127234477996747,\n",
       " 0.14381063442230146,\n",
       " 0.14353073520660323,\n",
       " 0.14561187419891281,\n",
       " 0.14378990554809493,\n",
       " 0.14248056373596113,\n",
       " 0.14367221050262372,\n",
       " 0.1413844114303581,\n",
       " 0.14243438949584883,\n",
       " 0.14284978866577069,\n",
       " 0.14338212604522627,\n",
       " 0.14277047061920087,\n",
       " 0.14235555362701338,\n",
       " 0.14385435123443527,\n",
       " 0.14543662872314375,\n",
       " 0.14516597900390546,\n",
       " 0.14332537574767987,\n",
       " 0.14089581127166667,\n",
       " 0.14350164737701335,\n",
       " 0.14205312824249186,\n",
       " 0.14115482788085856,\n",
       " 0.14003352699279703,\n",
       " 0.13984355831146159,\n",
       " 0.13980720901489177,\n",
       " 0.1400377264022819,\n",
       " 0.13982024230956949,\n",
       " 0.13969378604888835,\n",
       " 0.13946919898986734,\n",
       " 0.13966094131469645,\n",
       " 0.13938561935424723,\n",
       " 0.13806294631957927,\n",
       " 0.13795963516235271,\n",
       " 0.13753096923828045,\n",
       " 0.13743943729400554,\n",
       " 0.13647812480926433,\n",
       " 0.13724496002197184,\n",
       " 0.1369908206939689,\n",
       " 0.13647006759643471,\n",
       " 0.13850817146301186,\n",
       " 0.13920906124114907,\n",
       " 0.14025965023040687,\n",
       " 0.14130995311736977,\n",
       " 0.14246450099944985,\n",
       " 0.14192476539611734,\n",
       " 0.14476081714630046,\n",
       " 0.14767702636718669,\n",
       " 0.14810343685150065,\n",
       " 0.14783959712982098,\n",
       " 0.14911913681030192,\n",
       " 0.14792181854247966,\n",
       " 0.1480221517562858,\n",
       " 0.14789457931518474,\n",
       " 0.14729281063079752,\n",
       " 0.14665233459472574,\n",
       " 0.14493761653900064,\n",
       " 0.14651856346130288,\n",
       " 0.14594189109802164,\n",
       " 0.14706445846557534,\n",
       " 0.1472894565582267,\n",
       " 0.14699259777069007,\n",
       " 0.14720142688751137,\n",
       " 0.14601093177795327,\n",
       " 0.14413976516723548,\n",
       " 0.14436702003478918,\n",
       " 0.14678502941131505,\n",
       " 0.14697493019103916,\n",
       " 0.14583971462249667,\n",
       " 0.14764614467620762,\n",
       " 0.14699584999084386,\n",
       " 0.14786936321258459,\n",
       " 0.14877584190368567,\n",
       " 0.14850871562957679,\n",
       " 0.14675477161407385,\n",
       " 0.14859189510345375,\n",
       " 0.14795504589080727,\n",
       " 0.14645653400421058,\n",
       " 0.14718770160674963,\n",
       " 0.14921951274871739,\n",
       " 0.15038737926483067,\n",
       " 0.15045097541808994,\n",
       " 0.14908577327728184,\n",
       " 0.14909041709899815,\n",
       " 0.14943149642944248,\n",
       " 0.14834666900634677,\n",
       " 0.1492993917465201,\n",
       " 0.14863079776763827,\n",
       " 0.14868749332427889,\n",
       " 0.14866930885314852,\n",
       " 0.14840210227966219,\n",
       " 0.14871341495513826,\n",
       " 0.15027103805541903,\n",
       " 0.15105635070800691,\n",
       " 0.1513301357269278,\n",
       " 0.14984964904785064,\n",
       " 0.1481400249481192,\n",
       " 0.14737577533721832,\n",
       " 0.14694842967986968,\n",
       " 0.14131334857940581,\n",
       " 0.13910951633453275,\n",
       " 0.13817185878753568,\n",
       " 0.13558539829254057,\n",
       " 0.13395199222564605,\n",
       " 0.13168733043670561,\n",
       " 0.12893809947967436,\n",
       " 0.12775041122436429,\n",
       " 0.12357214584350491,\n",
       " 0.12017590770721341,\n",
       " 0.1203981889724722,\n",
       " 0.11786826877593899,\n",
       " 0.11566504592895413,\n",
       " 0.11617078113555813,\n",
       " 0.11488553581237698,\n",
       " 0.11475682506561184,\n",
       " 0.11598829078674221,\n",
       " 0.11396854114532375,\n",
       " 0.11330618801116847,\n",
       " 0.11389725456237697,\n",
       " 0.11264467658996485,\n",
       " 0.11206430778503321,\n",
       " 0.11020939865112207,\n",
       " 0.11098821239471338,\n",
       " 0.11098138637542626,\n",
       " 0.11023129043579002,\n",
       " 0.10659239292144676,\n",
       " 0.10634414825439353,\n",
       " 0.10515162620544334,\n",
       " 0.10550350265502829,\n",
       " 0.10509637851714987,\n",
       " 0.10692689819335836,\n",
       " 0.10646949176788228,\n",
       " 0.10582808609008687,\n",
       " 0.10182408180236714,\n",
       " 0.10182619209289448,\n",
       " 0.10165390224456684,\n",
       " 0.10131984481811421,\n",
       " 0.10197038173675434,\n",
       " 0.10114842567443744,\n",
       " 0.10285271339416399,\n",
       " 0.1021491992950429,\n",
       " 0.10002127933502093,\n",
       " 0.10038031578063861,\n",
       " 0.099636112594603449,\n",
       " 0.099488955688475517,\n",
       " 0.099906455612181574,\n",
       " 0.099850017356871518,\n",
       " 0.10158646945953265,\n",
       " 0.099991457939146905,\n",
       " 0.098731779479979423,\n",
       " 0.098161125755309012,\n",
       " 0.098107243156432059,\n",
       " 0.096412536239622981,\n",
       " 0.09394750156402483,\n",
       " 0.094813270378111741,\n",
       " 0.092932406616209884,\n",
       " 0.093668338584898841,\n",
       " 0.092899117469786532,\n",
       " 0.094026267623900303,\n",
       " 0.094209797668455969,\n",
       " 0.093803856849669354,\n",
       " 0.092827194404600993,\n",
       " 0.091303198051451581,\n",
       " 0.088546953964232344,\n",
       " 0.087266783523558517,\n",
       " 0.08475034961700334,\n",
       " 0.086039681053160566,\n",
       " 0.082073656272887124,\n",
       " 0.082908450698851485,\n",
       " 0.081654131698607343,\n",
       " 0.081131002998350993,\n",
       " 0.078169549560545823,\n",
       " 0.078949641227721121,\n",
       " 0.07763728694915667,\n",
       " 0.073573192405699636,\n",
       " 0.073381332015990161,\n",
       " 0.071993579101561442,\n",
       " 0.072309737014769448,\n",
       " 0.065720093345641037,\n",
       " 0.063622810173033617,\n",
       " 0.059733063125609297,\n",
       " 0.057306747055052655,\n",
       " 0.057809767723082439,\n",
       " 0.057986478233336344,\n",
       " 0.056226957702635659,\n",
       " 0.053080984115599528,\n",
       " 0.052477865982054603,\n",
       " 0.052187156295775305,\n",
       " 0.055874464797972569,\n",
       " 0.052813272285460358,\n",
       " 0.054272091674803617,\n",
       " 0.056349042892454987,\n",
       " 0.051039376449583895,\n",
       " 0.04854064464568985,\n",
       " 0.048330190658568264,\n",
       " 0.04774697151183975,\n",
       " 0.045478564453123926,\n",
       " 0.044183769416808009,\n",
       " 0.043857151031493069,\n",
       " 0.045089730834959867,\n",
       " 0.045649241638182521,\n",
       " 0.04563308410644424,\n",
       " 0.045573879051207425,\n",
       " 0.046010122108458403,\n",
       " 0.048005019378661036,\n",
       " 0.046174853897093651,\n",
       " 0.045602966690062398,\n",
       " 0.041372745704649799,\n",
       " 0.042526983642577044,\n",
       " 0.043251816940306533,\n",
       " 0.044157105445860732,\n",
       " 0.045660507965086805,\n",
       " 0.048340291595457899,\n",
       " 0.051983453178404672,\n",
       " 0.051686628532408574,\n",
       " 0.05345499706268201,\n",
       " 0.053598917770384648,\n",
       " 0.051252293777464722,\n",
       " 0.053098279190062374,\n",
       " 0.053468220329283563,\n",
       " 0.050120592308043325,\n",
       " 0.051474590873717155,\n",
       " 0.050844320297240106,\n",
       " 0.049124067497252309,\n",
       " 0.049060191917418325,\n",
       " 0.049693056488036,\n",
       " 0.049540143394469105,\n",
       " 0.046369473266600451,\n",
       " 0.043217238998411976,\n",
       " 0.044835439491270862,\n",
       " 0.046160022544859729,\n",
       " 0.047803288269041858,\n",
       " 0.048566334152220565,\n",
       " 0.048762281227110703,\n",
       " 0.04998310737609752,\n",
       " 0.050938388442992047,\n",
       " 0.052110371398924667,\n",
       " 0.052420690727232773,\n",
       " 0.052380294609068713,\n",
       " 0.053587574195860702,\n",
       " 0.044316185951231796,\n",
       " 0.04502019691467174,\n",
       " 0.039599974632262072,\n",
       " 0.033815701484679064,\n",
       " 0.031097079849242051,\n",
       " 0.026679672431944686,\n",
       " 0.027497897338866071,\n",
       " 0.027235219764708357,\n",
       " 0.026683860206602888,\n",
       " 0.026569370460509136,\n",
       " 0.025281492614744976,\n",
       " 0.02371130542755015,\n",
       " 0.022649242782591655,\n",
       " 0.024113802528380229,\n",
       " 0.023981258201598001,\n",
       " 0.025654334640501809,\n",
       " 0.026948636436461282,\n",
       " 0.027274069595335793,\n",
       " 0.025316168785094094,\n",
       " 0.025739115142821144,\n",
       " 0.028045480728148293,\n",
       " 0.027660465431212256,\n",
       " 0.026730848693846534,\n",
       " 0.027519176483153173,\n",
       " 0.028489597892760104,\n",
       " 0.029667121696471042,\n",
       " 0.031647077178953952,\n",
       " 0.03075068645477182,\n",
       " 0.03143777484893686,\n",
       " 0.028752433586119477,\n",
       " 0.030836611747740571,\n",
       " 0.031481596565245455,\n",
       " 0.033679264640806977,\n",
       " 0.034312957763670743,\n",
       " 0.034147959518431484,\n",
       " 0.034853997802733239,\n",
       " 0.036006348991392906,\n",
       " 0.036609015083311852,\n",
       " 0.036686178779600911,\n",
       " 0.038558816719054034,\n",
       " 0.038544970512388997,\n",
       " 0.040718302726744461,\n",
       " 0.041486812400816725,\n",
       " 0.039482645225523756,\n",
       " 0.040293364524840163,\n",
       " 0.04067853164672737,\n",
       " 0.039140578460692216,\n",
       " 0.040226357841490555,\n",
       " 0.040378689575194165,\n",
       " 0.03974383525848274,\n",
       " 0.038150508689879223,\n",
       " 0.039150529098509593,\n",
       " 0.024704414939879221,\n",
       " 0.023074023246763983,\n",
       " 0.017610025405882634,\n",
       " 0.013097631835936343,\n",
       " 0.015314717292784486,\n",
       " 0.015929578018187316,\n",
       " 0.014395178985594543,\n",
       " 0.014232599449156554,\n",
       " 0.014525735092161925,\n",
       " 0.015077702331541808,\n",
       " 0.013574553871153625,\n",
       " 0.01373274688720587,\n",
       " 0.013966534614561826,\n",
       " 0.014546731376646785,\n",
       " 0.01292002391815069,\n",
       " 0.013344936561583306,\n",
       " 0.01396900501251104,\n",
       " 0.017369828033446097,\n",
       " 0.019703601074217581,\n",
       " 0.019305837631224416,\n",
       " 0.016104784393309375,\n",
       " 0.0141087305068958,\n",
       " 0.013082351875304,\n",
       " 0.013466084098814741,\n",
       " 0.015231054496763959,\n",
       " 0.017053292465208782,\n",
       " 0.021751395797728314,\n",
       " 0.021648635673521769,\n",
       " 0.020518518829344523,\n",
       " 0.019002997779845012,\n",
       " 0.018269465827940715,\n",
       " 0.017573115921019327,\n",
       " 0.018374826431273233,\n",
       " 0.01835718593597294,\n",
       " 0.01652378635406376,\n",
       " 0.016733632278441199,\n",
       " 0.017900859832762486,\n",
       " 0.018072280883787876,\n",
       " 0.019211969184874299,\n",
       " 0.019868629074095489,\n",
       " 0.020397095108031036,\n",
       " 0.023484021377562284,\n",
       " 0.025019938468931913,\n",
       " 0.023835702705382107,\n",
       " 0.023853449630736111,\n",
       " 0.025445863151549099,\n",
       " 0.029130276107786891,\n",
       " 0.028607196235655542,\n",
       " 0.029479726600645774,\n",
       " 0.027632619285582298,\n",
       " 0.028127424430845968,\n",
       " 0.029825036811827414,\n",
       " 0.031803382301329369,\n",
       " 0.032449622726439233,\n",
       " 0.032499824714659446,\n",
       " 0.030887381553648702,\n",
       " 0.031726854705809347,\n",
       " 0.032394803237913841,\n",
       " 0.031596458435057394,\n",
       " 0.034848569297789324,\n",
       " 0.033177379035948501,\n",
       " 0.034952627944945081,\n",
       " 0.035601715660094006,\n",
       " 0.037217973136900645,\n",
       " 0.037459095382689218,\n",
       " 0.038371921348570565,\n",
       " 0.034696869087218023,\n",
       " 0.037513321113585206,\n",
       " 0.038948414802550048,\n",
       " 0.036454301261900635,\n",
       " 0.038388827323912356,\n",
       " 0.038067852401732181,\n",
       " 0.037314242553709721,\n",
       " 0.037445929145811768,\n",
       " 0.025294722557066648,\n",
       " 0.023140492248533932,\n",
       " 0.021506127357481683,\n",
       " 0.019643751525877679,\n",
       " 0.015819957160948479,\n",
       " 0.015080220222471916,\n",
       " 0.010036787414549551,\n",
       " 0.0088093164443957418,\n",
       " 0.0066824392318713278,\n",
       " 0.0084056589126574599,\n",
       " 0.0097272789001452524,\n",
       " 0.012115324401854236,\n",
       " 0.0087466447830187848,\n",
       " 0.0091569946289050148,\n",
       " -0.007332266235352802,\n",
       " -0.0085368888854992862,\n",
       " -0.011145527458192158,\n",
       " -0.0084514461517346401,\n",
       " -0.0097398319244397184,\n",
       " -0.012116741943360617,\n",
       " -0.013500832748414328,\n",
       " -0.010976055717469504,\n",
       " -0.013833218574525168,\n",
       " -0.013471857833863548,\n",
       " -0.016200494766236598,\n",
       " -0.018917911338807398,\n",
       " -0.019478362274171167,\n",
       " -0.019994632911683373,\n",
       " -0.022822423744202906,\n",
       " -0.023185619735719021,\n",
       " -0.02547834892273074,\n",
       " -0.024108916854859646,\n",
       " -0.024789892578126249,\n",
       " -0.025320516014100371,\n",
       " -0.025005880928040802,\n",
       " -0.023754161071778596,\n",
       " -0.021572774887086213,\n",
       " -0.020006540107728302,\n",
       " -0.019687824630738558,\n",
       " -0.021751774978638949,\n",
       " -0.021521428871156042,\n",
       " -0.020505284309388465,\n",
       " -0.020782982444764442,\n",
       " -0.020314147567750282,\n",
       " -0.018761034774781533,\n",
       " -0.020748991584779095,\n",
       " -0.020649038887025187,\n",
       " -0.020819549751283001,\n",
       " -0.019202560806275679,\n",
       " -0.016330753326417282,\n",
       " -0.015051947593690232,\n",
       " -0.011421611022950487,\n",
       " -0.0093250394821179673,\n",
       " -0.0083778625488293932,\n",
       " -0.0050922260284436511,\n",
       " -0.0044432003021252931,\n",
       " -0.0014837453842175795,\n",
       " 0.0010172840118395482,\n",
       " 0.0027715751647936493,\n",
       " 0.0044827726364123008,\n",
       " 0.004625607490538277,\n",
       " 0.0054802785873400341,\n",
       " 0.0039315790176378844,\n",
       " 0.0040414743423449152,\n",
       " 0.0035356168746935477,\n",
       " 0.0033404462814318279,\n",
       " 0.0040221727371203041,\n",
       " 0.0018770498275744053,\n",
       " -3.6537933350888879e-05,\n",
       " 0.00035962314605584943,\n",
       " 0.00074059009551873892,\n",
       " 0.0020867036819445187,\n",
       " 0.0045145118713366076,\n",
       " 0.0065069349288927603,\n",
       " 0.0082985748291002793,\n",
       " 0.0094229610443102383,\n",
       " 0.010454477310179379,\n",
       " 0.011873963165281918,\n",
       " 0.01366241607665887,\n",
       " 0.015272729110716487,\n",
       " 0.016309533119200373,\n",
       " 0.016853294563292168,\n",
       " 0.017049268913267752,\n",
       " 0.01948119544982781,\n",
       " 0.0087261201858507588,\n",
       " 0.0043663000106798598,\n",
       " 0.0005825050353990965,\n",
       " -0.0017456447601331302,\n",
       " -0.0055102432250989522,\n",
       " -0.012827756881715165,\n",
       " -0.017315583515168535,\n",
       " -0.023472624874116291,\n",
       " -0.025520877933503498,\n",
       " -0.026687905979157794,\n",
       " -0.02669846525192391,\n",
       " -0.026052499866486899,\n",
       " -0.02622006921768319,\n",
       " -0.024754798793794032,\n",
       " -0.024182783412934658,\n",
       " -0.022429058170319913,\n",
       " -0.022765692996980069,\n",
       " -0.021850748920441983,\n",
       " -0.022505313205720304,\n",
       " -0.023636135578156831,\n",
       " -0.02312400465011728,\n",
       " -0.023391040229798678,\n",
       " -0.023146504592896824,\n",
       " -0.02404273986816538,\n",
       " -0.023145576286317235,\n",
       " -0.023006328773499854,\n",
       " -0.020513230514527688,\n",
       " -0.020858119964600932,\n",
       " -0.019612148284913433,\n",
       " -0.017187555503846538,\n",
       " -0.017315356254578959,\n",
       " -0.017894107246400247,\n",
       " -0.01553603153228892,\n",
       " -0.016181284332276714,\n",
       " -0.016436860942841899,\n",
       " -0.015050131702424419,\n",
       " -0.014008057689668072,\n",
       " -0.013247248935700787,\n",
       " -0.011285312366486919,\n",
       " -0.010454569149018659,\n",
       " -0.009025724887849227,\n",
       " -0.0085109675407422942,\n",
       " -0.007479003810883897,\n",
       " -0.0076041308403028435,\n",
       " -0.0054633830070508916,\n",
       " -0.003945202064515492,\n",
       " -0.0042685686111463518,\n",
       " -0.013283542633057973,\n",
       " -0.014533412361146351,\n",
       " -0.019055874061585806,\n",
       " -0.023020646667481805,\n",
       " -0.021776741409303094,\n",
       " -0.024189877510072139,\n",
       " -0.026264201545716671,\n",
       " -0.026369905853272823,\n",
       " -0.025508267974854854,\n",
       " -0.025500118637086299,\n",
       " -0.026018763160706906,\n",
       " -0.025385410690308956,\n",
       " -0.026892287445069701,\n",
       " -0.028607133293153196,\n",
       " -0.028185967159272581,\n",
       " -0.027998135852815062,\n",
       " -0.027324788570405396,\n",
       " -0.024379321956635863,\n",
       " -0.026055736255647093,\n",
       " -0.024856005573274047,\n",
       " -0.0232568213462843,\n",
       " -0.022771083545686157,\n",
       " -0.022649155521394165,\n",
       " -0.022815643882752808,\n",
       " -0.021346169281007205,\n",
       " -0.019850924491883672,\n",
       " -0.020476869773866094,\n",
       " -0.019895617485047735,\n",
       " -0.019261538887025274,\n",
       " -0.019572135829926885,\n",
       " -0.019535764026643195,\n",
       " -0.01769788579940931,\n",
       " -0.017131157207490363,\n",
       " -0.017281862354279913,\n",
       " -0.015979762554170052,\n",
       " -0.016414444255830208,\n",
       " -0.015368142032624644,\n",
       " -0.016460043048859997,\n",
       " -0.016264519023896619,\n",
       " -0.015330624198914931,\n",
       " -0.015648821640016006,\n",
       " -0.014831343841554092,\n",
       " -0.015874159049989151,\n",
       " -0.015434417533875871,\n",
       " -0.014465172767640521,\n",
       " -0.013190341377259662,\n",
       " -0.013876656913758686,\n",
       " -0.014838577461244038,\n",
       " -0.015512273502351216,\n",
       " -0.015257496738435202,\n",
       " -0.013860136890412742,\n",
       " -0.012950994586945947,\n",
       " -0.012348453426362451,\n",
       " -0.011293891811372216,\n",
       " -0.011495096492768701,\n",
       " -0.021115947628022609,\n",
       " -0.021857972049714503,\n",
       " -0.021791541671754299,\n",
       " -0.02180620002746719,\n",
       " -0.021603974533082423,\n",
       " -0.022570640850068508,\n",
       " -0.023365747356416167,\n",
       " -0.022747844791413726,\n",
       " -0.021347704601289215,\n",
       " -0.020073617839814605,\n",
       " -0.018062703990937651,\n",
       " -0.017134834289552155,\n",
       " -0.017450959110261382,\n",
       " -0.01664632024765152,\n",
       " -0.018730254459382478,\n",
       " -0.014126065540315098,\n",
       " -0.014877473545075841,\n",
       " -0.013137131214143224,\n",
       " -0.015971063613892979,\n",
       " -0.017360615634919592,\n",
       " -0.014514546489716956,\n",
       " -0.014323157787324379,\n",
       " -0.01369237861633439,\n",
       " -0.012693973827363444,\n",
       " -0.01279755535125871,\n",
       " -0.013009682750703291,\n",
       " -0.01062780542373796,\n",
       " -0.0092306709289564672,\n",
       " -0.0074778141021742406,\n",
       " -0.0036273134231581275,\n",
       " -0.0017888248443617407,\n",
       " -0.00075703620910783564,\n",
       " 4.7641754136484339e-06,\n",
       " -0.0017783545494093496,\n",
       " -0.0048246109962477293,\n",
       " -0.0040317046165480224,\n",
       " -0.0025859004020704836,\n",
       " -0.0025919122695936773,\n",
       " -0.0012708616256727789,\n",
       " -0.00093126420974870734,\n",
       " 0.00022513885497907598,\n",
       " 0.00099923896789411398,\n",
       " 0.00030470676421979689,\n",
       " 0.0013803049087510455,\n",
       " 0.0045286027908311233,\n",
       " 0.0056496574401841502,\n",
       " 0.0071921136856065134,\n",
       " 0.0096188671112046563,\n",
       " 0.010525462722776921,\n",
       " 0.010757886409758121,\n",
       " 0.011347915935514956,\n",
       " 0.010963716030119447,\n",
       " 0.011556262874601868,\n",
       " 0.010923667430876281,\n",
       " 0.010164939594267394,\n",
       " 0.0097125387191758401,\n",
       " 0.010085775661467098,\n",
       " 0.012198755836485407,\n",
       " 0.012696896171568415,\n",
       " 0.013108765316008112,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543312448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
