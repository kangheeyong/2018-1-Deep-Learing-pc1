{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum infoGANs example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_infoGANs'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     44,
     76
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "train_epoch = 100\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.001\n",
    "c_size = 10\n",
    "\n",
    "\n",
    "def G(x,c,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        x_concat = tf.concat([x,c],3)\n",
    "        #x = (-1, 1, 1, 100)\n",
    "\n",
    "        conv1 = tf.layers.conv2d_transpose(x_concat,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "\n",
    "def Q_cat(x,reuse = False, name = 'Q_cat') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('Q_cat', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        fc0  = tf.reshape(x, (-1, 100))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[100, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "    fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "    \n",
    "    return tf.reshape(fc1, (-1,1,1,c_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "c = tf.placeholder(tf.float32,shape=(None,1,1,c_size),name = 'c')    #x_z = G(z,c)\n",
    "\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,c,name='G_sample') # G(z)\n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "Q_fake = Q_cat(D_enc(G_sample, isTrain,reuse=True), reuse=False, name='Q_fake')\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "Q_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(Q_fake + 1e-8), axis = (1,2,3)),name = 'Q_loss')\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "Q_vars = [var for var in T_vars if var.name.startswith('Q')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss + Q_loss, var_list=G_vars+Q_vars, name='G_optim')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -342.142, D_real_e : 63.397, D_fake_e : 45.728, G_e : 46.094, Q_e : 2.088, new_measure : 75.814, k_curr : -4.804522\n",
      "D_e : 36.147, D_real_e : 55.206, D_fake_e : 35.986, G_e : 36.755, Q_e : 2.102, new_measure : 58.593, k_curr : 0.483968\n",
      "D_e : 31.928, D_real_e : 43.775, D_fake_e : 29.873, G_e : 30.702, Q_e : 2.011, new_measure : 44.558, k_curr : 0.315623\n",
      "D_e : 29.243, D_real_e : 36.659, D_fake_e : 25.026, G_e : 25.686, Q_e : 1.832, new_measure : 37.316, k_curr : 0.246869\n",
      "D_e : 25.917, D_real_e : 30.500, D_fake_e : 20.880, G_e : 21.360, Q_e : 1.623, new_measure : 31.280, k_curr : 0.219230\n",
      "D_e : 23.089, D_real_e : 26.358, D_fake_e : 18.114, G_e : 18.470, Q_e : 1.438, new_measure : 27.136, k_curr : 0.165401\n",
      "D_e : 20.650, D_real_e : 23.141, D_fake_e : 15.895, G_e : 16.182, Q_e : 1.286, new_measure : 23.966, k_curr : 0.210980\n",
      "D_e : 18.306, D_real_e : 20.372, D_fake_e : 14.028, G_e : 14.266, Q_e : 1.162, new_measure : 21.128, k_curr : 0.195447\n",
      "D_e : 16.133, D_real_e : 17.706, D_fake_e : 12.195, G_e : 12.404, Q_e : 1.062, new_measure : 18.457, k_curr : 0.166371\n",
      "D_e : 14.035, D_real_e : 15.254, D_fake_e : 10.499, G_e : 10.707, Q_e : 0.977, new_measure : 16.089, k_curr : 0.085804\n",
      "D_e : 12.129, D_real_e : 13.053, D_fake_e : 8.938, G_e : 9.123, Q_e : 0.906, new_measure : 13.780, k_curr : 0.125000\n",
      "D_e : 10.213, D_real_e : 11.132, D_fake_e : 7.579, G_e : 7.780, Q_e : 0.843, new_measure : 11.639, k_curr : 0.159533\n",
      "D_e : 8.119, D_real_e : 9.007, D_fake_e : 6.088, G_e : 6.314, Q_e : 0.786, new_measure : 9.290, k_curr : 0.132750\n",
      "D_e : 7.285, D_real_e : 7.960, D_fake_e : 5.375, G_e : 5.577, Q_e : 0.735, new_measure : 8.216, k_curr : 0.120476\n",
      "D_e : 6.771, D_real_e : 7.349, D_fake_e : 4.970, G_e : 5.147, Q_e : 0.690, new_measure : 7.602, k_curr : 0.111858\n",
      "D_e : 6.441, D_real_e : 6.930, D_fake_e : 4.684, G_e : 4.860, Q_e : 0.649, new_measure : 7.189, k_curr : 0.086147\n",
      "D_e : 6.181, D_real_e : 6.596, D_fake_e : 4.472, G_e : 4.620, Q_e : 0.614, new_measure : 6.845, k_curr : 0.080476\n",
      "D_e : 6.000, D_real_e : 6.356, D_fake_e : 4.298, G_e : 4.447, Q_e : 0.581, new_measure : 6.577, k_curr : 0.086502\n",
      "D_e : 5.835, D_real_e : 6.164, D_fake_e : 4.178, G_e : 4.323, Q_e : 0.552, new_measure : 6.396, k_curr : 0.061745\n",
      "D_e : 5.706, D_real_e : 5.998, D_fake_e : 4.051, G_e : 4.194, Q_e : 0.526, new_measure : 6.239, k_curr : 0.074970\n",
      "D_e : 5.520, D_real_e : 5.863, D_fake_e : 3.952, G_e : 4.097, Q_e : 0.502, new_measure : 6.054, k_curr : 0.093006\n",
      "D_e : 5.410, D_real_e : 5.728, D_fake_e : 3.882, G_e : 4.022, Q_e : 0.480, new_measure : 5.952, k_curr : 0.057440\n",
      "D_e : 5.301, D_real_e : 5.601, D_fake_e : 3.760, G_e : 3.906, Q_e : 0.460, new_measure : 5.817, k_curr : 0.097108\n",
      "D_e : 5.221, D_real_e : 5.525, D_fake_e : 3.745, G_e : 3.876, Q_e : 0.441, new_measure : 5.742, k_curr : 0.072611\n",
      "D_e : 5.137, D_real_e : 5.421, D_fake_e : 3.653, G_e : 3.789, Q_e : 0.424, new_measure : 5.621, k_curr : 0.088735\n",
      "D_e : 5.099, D_real_e : 5.369, D_fake_e : 3.635, G_e : 3.769, Q_e : 0.409, new_measure : 5.569, k_curr : 0.059894\n",
      "D_e : 4.985, D_real_e : 5.272, D_fake_e : 3.560, G_e : 3.687, Q_e : 0.394, new_measure : 5.471, k_curr : 0.069746\n",
      "D_e : 4.955, D_real_e : 5.209, D_fake_e : 3.523, G_e : 3.646, Q_e : 0.380, new_measure : 5.404, k_curr : 0.069692\n",
      "D_e : 4.893, D_real_e : 5.161, D_fake_e : 3.477, G_e : 3.607, Q_e : 0.368, new_measure : 5.360, k_curr : 0.085609\n",
      "D_e : 4.831, D_real_e : 5.089, D_fake_e : 3.443, G_e : 3.565, Q_e : 0.356, new_measure : 5.291, k_curr : 0.078036\n",
      "D_e : 4.770, D_real_e : 5.027, D_fake_e : 3.400, G_e : 3.519, Q_e : 0.344, new_measure : 5.219, k_curr : 0.078505\n",
      "D_e : 4.742, D_real_e : 4.974, D_fake_e : 3.361, G_e : 3.482, Q_e : 0.334, new_measure : 5.157, k_curr : 0.077241\n",
      "D_e : 4.687, D_real_e : 4.940, D_fake_e : 3.330, G_e : 3.457, Q_e : 0.324, new_measure : 5.104, k_curr : 0.080112\n",
      "D_e : 4.656, D_real_e : 4.889, D_fake_e : 3.310, G_e : 3.424, Q_e : 0.315, new_measure : 5.068, k_curr : 0.075585\n",
      "D_e : 4.612, D_real_e : 4.830, D_fake_e : 3.272, G_e : 3.386, Q_e : 0.306, new_measure : 5.013, k_curr : 0.062182\n",
      "D_e : 4.576, D_real_e : 4.790, D_fake_e : 3.245, G_e : 3.352, Q_e : 0.297, new_measure : 4.953, k_curr : 0.065412\n",
      "D_e : 4.545, D_real_e : 4.761, D_fake_e : 3.217, G_e : 3.333, Q_e : 0.290, new_measure : 4.935, k_curr : 0.062875\n",
      "D_e : 4.507, D_real_e : 4.716, D_fake_e : 3.198, G_e : 3.299, Q_e : 0.282, new_measure : 4.886, k_curr : 0.068173\n",
      "D_e : 4.474, D_real_e : 4.674, D_fake_e : 3.178, G_e : 3.272, Q_e : 0.275, new_measure : 4.839, k_curr : 0.068337\n",
      "D_e : 4.411, D_real_e : 4.632, D_fake_e : 3.132, G_e : 3.237, Q_e : 0.268, new_measure : 4.806, k_curr : 0.083337\n",
      "D_e : 4.400, D_real_e : 4.632, D_fake_e : 3.137, G_e : 3.244, Q_e : 0.262, new_measure : 4.793, k_curr : 0.078061\n",
      "D_e : 4.381, D_real_e : 4.607, D_fake_e : 3.119, G_e : 3.229, Q_e : 0.256, new_measure : 4.773, k_curr : 0.066148\n",
      "D_e : 4.362, D_real_e : 4.558, D_fake_e : 3.085, G_e : 3.190, Q_e : 0.250, new_measure : 4.729, k_curr : 0.067327\n",
      "D_e : 4.323, D_real_e : 4.533, D_fake_e : 3.058, G_e : 3.176, Q_e : 0.244, new_measure : 4.708, k_curr : 0.060067\n",
      "D_e : 4.292, D_real_e : 4.504, D_fake_e : 3.037, G_e : 3.146, Q_e : 0.239, new_measure : 4.663, k_curr : 0.080568\n",
      "D_e : 4.262, D_real_e : 4.485, D_fake_e : 3.040, G_e : 3.147, Q_e : 0.234, new_measure : 4.646, k_curr : 0.058433\n",
      "D_e : 4.271, D_real_e : 4.472, D_fake_e : 3.020, G_e : 3.126, Q_e : 0.229, new_measure : 4.638, k_curr : 0.069350\n",
      "D_e : 4.228, D_real_e : 4.426, D_fake_e : 2.997, G_e : 3.100, Q_e : 0.224, new_measure : 4.582, k_curr : 0.064420\n",
      "D_e : 4.186, D_real_e : 4.392, D_fake_e : 2.969, G_e : 3.068, Q_e : 0.220, new_measure : 4.553, k_curr : 0.083707\n",
      "D_e : 4.175, D_real_e : 4.386, D_fake_e : 2.972, G_e : 3.076, Q_e : 0.215, new_measure : 4.549, k_curr : 0.067139\n",
      "D_e : 4.156, D_real_e : 4.342, D_fake_e : 2.945, G_e : 3.041, Q_e : 0.211, new_measure : 4.489, k_curr : 0.063079\n",
      "D_e : 4.146, D_real_e : 4.333, D_fake_e : 2.933, G_e : 3.031, Q_e : 0.207, new_measure : 4.495, k_curr : 0.068095\n",
      "D_e : 4.115, D_real_e : 4.302, D_fake_e : 2.920, G_e : 3.012, Q_e : 0.203, new_measure : 4.461, k_curr : 0.067101\n",
      "D_e : 4.094, D_real_e : 4.272, D_fake_e : 2.902, G_e : 2.990, Q_e : 0.200, new_measure : 4.421, k_curr : 0.067630\n",
      "D_e : 4.075, D_real_e : 4.266, D_fake_e : 2.892, G_e : 2.987, Q_e : 0.196, new_measure : 4.421, k_curr : 0.065167\n",
      "D_e : 4.074, D_real_e : 4.245, D_fake_e : 2.869, G_e : 2.968, Q_e : 0.193, new_measure : 4.406, k_curr : 0.073530\n",
      "D_e : 4.025, D_real_e : 4.215, D_fake_e : 2.860, G_e : 2.953, Q_e : 0.189, new_measure : 4.364, k_curr : 0.068071\n",
      "D_e : 4.024, D_real_e : 4.214, D_fake_e : 2.842, G_e : 2.949, Q_e : 0.186, new_measure : 4.366, k_curr : 0.069464\n",
      "D_e : 3.994, D_real_e : 4.169, D_fake_e : 2.827, G_e : 2.921, Q_e : 0.183, new_measure : 4.317, k_curr : 0.062428\n",
      "D_e : 3.991, D_real_e : 4.168, D_fake_e : 2.815, G_e : 2.916, Q_e : 0.180, new_measure : 4.317, k_curr : 0.067207\n",
      "D_e : 3.960, D_real_e : 4.153, D_fake_e : 2.811, G_e : 2.908, Q_e : 0.177, new_measure : 4.303, k_curr : 0.064423\n",
      "D_e : 3.934, D_real_e : 4.146, D_fake_e : 2.807, G_e : 2.904, Q_e : 0.174, new_measure : 4.287, k_curr : 0.060066\n",
      "D_e : 3.954, D_real_e : 4.144, D_fake_e : 2.792, G_e : 2.899, Q_e : 0.171, new_measure : 4.295, k_curr : 0.064299\n",
      "D_e : 3.935, D_real_e : 4.099, D_fake_e : 2.767, G_e : 2.869, Q_e : 0.169, new_measure : 4.253, k_curr : 0.065798\n",
      "D_e : 3.898, D_real_e : 4.085, D_fake_e : 2.765, G_e : 2.859, Q_e : 0.166, new_measure : 4.229, k_curr : 0.067734\n",
      "D_e : 3.908, D_real_e : 4.090, D_fake_e : 2.764, G_e : 2.859, Q_e : 0.164, new_measure : 4.246, k_curr : 0.079532\n",
      "D_e : 3.880, D_real_e : 4.068, D_fake_e : 2.757, G_e : 2.849, Q_e : 0.161, new_measure : 4.219, k_curr : 0.076728\n",
      "D_e : 3.861, D_real_e : 4.047, D_fake_e : 2.744, G_e : 2.837, Q_e : 0.159, new_measure : 4.185, k_curr : 0.063723\n",
      "D_e : 3.853, D_real_e : 4.035, D_fake_e : 2.734, G_e : 2.825, Q_e : 0.157, new_measure : 4.176, k_curr : 0.061340\n",
      "D_e : 3.836, D_real_e : 4.003, D_fake_e : 2.722, G_e : 2.801, Q_e : 0.155, new_measure : 4.140, k_curr : 0.064166\n",
      "D_e : 3.812, D_real_e : 3.994, D_fake_e : 2.705, G_e : 2.801, Q_e : 0.153, new_measure : 4.135, k_curr : 0.051254\n",
      "D_e : 3.799, D_real_e : 3.965, D_fake_e : 2.690, G_e : 2.770, Q_e : 0.150, new_measure : 4.098, k_curr : 0.067635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 3.796, D_real_e : 3.964, D_fake_e : 2.684, G_e : 2.773, Q_e : 0.148, new_measure : 4.112, k_curr : 0.071965\n",
      "D_e : 3.794, D_real_e : 3.949, D_fake_e : 2.671, G_e : 2.763, Q_e : 0.146, new_measure : 4.092, k_curr : 0.075175\n",
      "D_e : 3.759, D_real_e : 3.931, D_fake_e : 2.681, G_e : 2.757, Q_e : 0.145, new_measure : 4.070, k_curr : 0.060230\n",
      "D_e : 3.765, D_real_e : 3.920, D_fake_e : 2.658, G_e : 2.740, Q_e : 0.143, new_measure : 4.072, k_curr : 0.069679\n",
      "D_e : 3.768, D_real_e : 3.927, D_fake_e : 2.669, G_e : 2.752, Q_e : 0.141, new_measure : 4.079, k_curr : 0.062264\n",
      "D_e : 3.730, D_real_e : 3.880, D_fake_e : 2.630, G_e : 2.715, Q_e : 0.139, new_measure : 4.021, k_curr : 0.063213\n",
      "D_e : 3.727, D_real_e : 3.879, D_fake_e : 2.632, G_e : 2.716, Q_e : 0.137, new_measure : 4.006, k_curr : 0.059689\n",
      "D_e : 3.713, D_real_e : 3.877, D_fake_e : 2.635, G_e : 2.712, Q_e : 0.136, new_measure : 4.006, k_curr : 0.065147\n",
      "D_e : 3.718, D_real_e : 3.871, D_fake_e : 2.623, G_e : 2.710, Q_e : 0.134, new_measure : 4.009, k_curr : 0.064282\n",
      "D_e : 3.691, D_real_e : 3.843, D_fake_e : 2.615, G_e : 2.692, Q_e : 0.132, new_measure : 3.990, k_curr : 0.060269\n",
      "D_e : 3.675, D_real_e : 3.828, D_fake_e : 2.604, G_e : 2.679, Q_e : 0.131, new_measure : 3.963, k_curr : 0.060485\n",
      "D_e : 3.681, D_real_e : 3.832, D_fake_e : 2.604, G_e : 2.684, Q_e : 0.129, new_measure : 3.961, k_curr : 0.055984\n",
      "D_e : 3.655, D_real_e : 3.813, D_fake_e : 2.584, G_e : 2.666, Q_e : 0.128, new_measure : 3.940, k_curr : 0.065734\n",
      "D_e : 3.653, D_real_e : 3.804, D_fake_e : 2.585, G_e : 2.664, Q_e : 0.126, new_measure : 3.950, k_curr : 0.061967\n",
      "D_e : 3.634, D_real_e : 3.791, D_fake_e : 2.569, G_e : 2.650, Q_e : 0.125, new_measure : 3.933, k_curr : 0.071693\n",
      "D_e : 3.622, D_real_e : 3.773, D_fake_e : 2.568, G_e : 2.646, Q_e : 0.123, new_measure : 3.905, k_curr : 0.058400\n",
      "D_e : 3.608, D_real_e : 3.755, D_fake_e : 2.553, G_e : 2.628, Q_e : 0.122, new_measure : 3.895, k_curr : 0.060629\n",
      "D_e : 3.603, D_real_e : 3.753, D_fake_e : 2.558, G_e : 2.628, Q_e : 0.121, new_measure : 3.875, k_curr : 0.057617\n",
      "D_e : 3.601, D_real_e : 3.741, D_fake_e : 2.534, G_e : 2.614, Q_e : 0.119, new_measure : 3.887, k_curr : 0.069426\n",
      "D_e : 3.580, D_real_e : 3.732, D_fake_e : 2.541, G_e : 2.610, Q_e : 0.118, new_measure : 3.864, k_curr : 0.074759\n",
      "D_e : 3.569, D_real_e : 3.713, D_fake_e : 2.523, G_e : 2.606, Q_e : 0.117, new_measure : 3.831, k_curr : 0.056003\n",
      "D_e : 3.557, D_real_e : 3.710, D_fake_e : 2.520, G_e : 2.599, Q_e : 0.116, new_measure : 3.840, k_curr : 0.050430\n",
      "D_e : 3.549, D_real_e : 3.694, D_fake_e : 2.501, G_e : 2.581, Q_e : 0.114, new_measure : 3.824, k_curr : 0.062754\n",
      "D_e : 3.553, D_real_e : 3.690, D_fake_e : 2.511, G_e : 2.586, Q_e : 0.113, new_measure : 3.823, k_curr : 0.052345\n",
      "D_e : 3.539, D_real_e : 3.672, D_fake_e : 2.505, G_e : 2.571, Q_e : 0.112, new_measure : 3.791, k_curr : 0.049877\n",
      "D_e : 3.535, D_real_e : 3.665, D_fake_e : 2.489, G_e : 2.559, Q_e : 0.111, new_measure : 3.794, k_curr : 0.069581\n",
      "D_e : 3.516, D_real_e : 3.642, D_fake_e : 2.493, G_e : 2.563, Q_e : 0.110, new_measure : 3.771, k_curr : 0.031131\n",
      "D_e : 3.508, D_real_e : 3.639, D_fake_e : 2.478, G_e : 2.541, Q_e : 0.109, new_measure : 3.761, k_curr : 0.047689\n",
      "total time :  6723.177406072617\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb03bf3f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "\n",
    "    \n",
    "    one_hot = np.eye(c_size)\n",
    "    temp2 = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4])\n",
    "    test_c = one_hot[temp2].reshape([-1,1,1,c_size])\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    Q_error=[]\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            temp1 = np.random.randint(0,10,(batch_size))                                                                                                                                     \n",
    "            c_ = one_hot[temp1].reshape([-1,1,1,c_size])\n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, c: c_,k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e,Q_e = sess.run([G_optim, G_loss, Q_loss], {u : u_, z : z_, c : c_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            Q_error.append(Q_e)\n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, Q_e : %.3f, new_measure : %.3f, k_curr : %3f'\n",
    "              %(np.mean(D_error), np.mean(D_real_error),np.mean(D_fake_error), np.mean(G_error), np.mean(Q_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, c : test_c, isTrain : False})       \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "\n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_infoGANs/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20783333206176755,\n",
       " 0.42383360290527339,\n",
       " 0.64719684219360341,\n",
       " 0.87381250381469711,\n",
       " 1.1051272926330564,\n",
       " 1.3396434783935545,\n",
       " 1.57696813583374,\n",
       " 1.8163792419433591,\n",
       " 2.0575752830505367,\n",
       " 2.2998684692382807,\n",
       " 2.5438078536987301,\n",
       " 2.7886397895812984,\n",
       " 3.0342318458557123,\n",
       " 3.2801973266601556,\n",
       " 3.5259290275573725,\n",
       " 3.7698152847290034,\n",
       " 4.0128952369689941,\n",
       " 4.2546189880371097,\n",
       " 4.4958490600585943,\n",
       " 4.734632579803467,\n",
       " 4.9734721031188966,\n",
       " 5.2092538986206058,\n",
       " 5.4427842445373535,\n",
       " 5.6747840499877933,\n",
       " 5.9034831809997561,\n",
       " 6.1313041152954106,\n",
       " 6.3565226173400884,\n",
       " 6.5811616516113283,\n",
       " 6.8041447944641114,\n",
       " 7.0227180252075199,\n",
       " 7.2404988288879402,\n",
       " 7.4536422500610362,\n",
       " 7.6676529350280775,\n",
       " 7.8788218116760271,\n",
       " 8.0871982154846211,\n",
       " 8.2948508796691911,\n",
       " 8.4989925231933618,\n",
       " 8.7024910697937035,\n",
       " 8.9056545867919947,\n",
       " 9.1042294921875033,\n",
       " 9.3011904869079629,\n",
       " 9.4940096778869663,\n",
       " 9.6848709602355996,\n",
       " 9.8764002494812058,\n",
       " 10.065735645294193,\n",
       " 10.253561790466312,\n",
       " 10.437097579956058,\n",
       " 10.619346012115482,\n",
       " 10.798164058685307,\n",
       " 10.97565844726563,\n",
       " 11.148474540710454,\n",
       " 11.31930522537232,\n",
       " 11.484413909912115,\n",
       " 11.645083751678472,\n",
       " 11.803248874664312,\n",
       " 11.952390308380133,\n",
       " 12.097084300994879,\n",
       " 12.235222324371344,\n",
       " 12.369861309051521,\n",
       " 12.485939277648933,\n",
       " 12.592018028259284,\n",
       " 12.700631923675544,\n",
       " 12.797129756927498,\n",
       " 12.899319190979012,\n",
       " 13.004909072875986,\n",
       " 13.100105770111092,\n",
       " 13.204264888763436,\n",
       " 13.31600425338746,\n",
       " 13.443781650543222,\n",
       " 13.581607013702403,\n",
       " 13.710914699554454,\n",
       " 13.82934658432008,\n",
       " 13.912803653717052,\n",
       " 13.985181095123302,\n",
       " 14.06078660964967,\n",
       " 14.131156902313244,\n",
       " 14.197510501861583,\n",
       " 14.233643741607677,\n",
       " 14.281211696624768,\n",
       " 14.328069042205822,\n",
       " 14.382377140045177,\n",
       " 14.422571132659924,\n",
       " 14.47697117996217,\n",
       " 14.532411304473889,\n",
       " 14.572236759185804,\n",
       " 14.613756732940686,\n",
       " 14.654958957672132,\n",
       " 14.685946140289319,\n",
       " 14.717699222564709,\n",
       " 14.763952060699475,\n",
       " 14.787474117279066,\n",
       " 14.807647052764906,\n",
       " 14.834258113861098,\n",
       " 14.856653095245376,\n",
       " 14.87489178848268,\n",
       " 14.898922122955335,\n",
       " 14.913666263580335,\n",
       " 14.92107477951051,\n",
       " 14.922804645538342,\n",
       " 14.939228878021252,\n",
       " 14.952723987579358,\n",
       " 14.961588764190685,\n",
       " 14.972894641876232,\n",
       " 14.968722614288342,\n",
       " 14.965725551605237,\n",
       " 14.962276340484632,\n",
       " 14.968944881439221,\n",
       " 14.96344400405885,\n",
       " 14.954450847625745,\n",
       " 14.95776197433473,\n",
       " 14.947961254119885,\n",
       " 14.925339817047131,\n",
       " 14.902849063873303,\n",
       " 14.877010066986095,\n",
       " 14.863381961822521,\n",
       " 14.851423053741465,\n",
       " 14.83721902847291,\n",
       " 14.814191097259531,\n",
       " 14.792169727325449,\n",
       " 14.767839359283457,\n",
       " 14.737439685821542,\n",
       " 14.72092873764039,\n",
       " 14.701900997161875,\n",
       " 14.675896709442149,\n",
       " 14.650529369354258,\n",
       " 14.633857814788827,\n",
       " 14.618133060455332,\n",
       " 14.591961544036874,\n",
       " 14.572464176177988,\n",
       " 14.528582180023202,\n",
       " 14.491552761077889,\n",
       " 14.456002185821541,\n",
       " 14.410697566986093,\n",
       " 14.376140308380135,\n",
       " 14.342517765045175,\n",
       " 14.311377490997323,\n",
       " 14.282589504241951,\n",
       " 14.246872402191169,\n",
       " 14.21682194137574,\n",
       " 14.176850490570075,\n",
       " 14.138995220184333,\n",
       " 14.091150012969978,\n",
       " 14.048177043914801,\n",
       " 14.006801410675056,\n",
       " 13.959950031280524,\n",
       " 13.93048541641236,\n",
       " 13.888400318145758,\n",
       " 13.844499851226812,\n",
       " 13.787652019500738,\n",
       " 13.743837825775152,\n",
       " 13.701127529144292,\n",
       " 13.649015468597417,\n",
       " 13.598497959136967,\n",
       " 13.540516735076908,\n",
       " 13.494496837615971,\n",
       " 13.444064029693608,\n",
       " 13.38791956710816,\n",
       " 13.331552989959722,\n",
       " 13.257138668060307,\n",
       " 13.189160274505619,\n",
       " 13.127654865264896,\n",
       " 13.054803081512455,\n",
       " 12.969467578887944,\n",
       " 12.888809894561772,\n",
       " 12.814102443695074,\n",
       " 12.740961025238043,\n",
       " 12.662937984466559,\n",
       " 12.58904547500611,\n",
       " 12.511745388031011,\n",
       " 12.428398738861089,\n",
       " 12.353669033050542,\n",
       " 12.271151798248296,\n",
       " 12.180961551666265,\n",
       " 12.101583621978765,\n",
       " 12.006656848907475,\n",
       " 11.905128002166753,\n",
       " 11.805351108551031,\n",
       " 11.710487987518317,\n",
       " 11.608455204010015,\n",
       " 11.512619731903081,\n",
       " 11.406060619354253,\n",
       " 11.295360248565679,\n",
       " 11.17818591690064,\n",
       " 11.078024288177495,\n",
       " 10.960920925140385,\n",
       " 10.855023929595951,\n",
       " 10.733286945343021,\n",
       " 10.613272609710696,\n",
       " 10.487291992187503,\n",
       " 10.369436531066897,\n",
       " 10.244937591552738,\n",
       " 10.115258678436282,\n",
       " 9.9834587478637715,\n",
       " 9.8497871170043965,\n",
       " 9.7201949539184582,\n",
       " 9.5857089576721197,\n",
       " 9.4518719139099119,\n",
       " 9.3183081550598139,\n",
       " 9.177949253082275,\n",
       " 9.0382658576965333,\n",
       " 8.9058912963867183,\n",
       " 8.7614902114868158,\n",
       " 8.6268450660705565,\n",
       " 8.4799882316589361,\n",
       " 8.3421864013671883,\n",
       " 8.2013862609863288,\n",
       " 8.0539876213073729,\n",
       " 7.9150907821655272,\n",
       " 7.7616119461059565,\n",
       " 7.6056154594421379,\n",
       " 7.4579724540710437,\n",
       " 7.3036997528076162,\n",
       " 7.1439243774414054,\n",
       " 6.9812891120910638,\n",
       " 6.8067992858886708,\n",
       " 6.6338955726623521,\n",
       " 6.4596681175231918,\n",
       " 6.291061111450194,\n",
       " 6.1202879943847641,\n",
       " 5.949295257568358,\n",
       " 5.7775939178466782,\n",
       " 5.6125825538635237,\n",
       " 5.439427150726317,\n",
       " 5.2679723243713363,\n",
       " 5.0971659622192371,\n",
       " 4.9239939842224105,\n",
       " 4.7554659118652332,\n",
       " 4.588525482177733,\n",
       " 4.4293280372619614,\n",
       " 4.2593236122131337,\n",
       " 4.0795395431518546,\n",
       " 3.8998634681701652,\n",
       " 3.7218413124084462,\n",
       " 3.5504504165649404,\n",
       " 3.3775794792175282,\n",
       " 3.2097463607788077,\n",
       " 3.0267251663207997,\n",
       " 2.8478522796630847,\n",
       " 2.6730030021667468,\n",
       " 2.4879001426696763,\n",
       " 2.3067994041442859,\n",
       " 2.1158468322753894,\n",
       " 1.9356627120971668,\n",
       " 1.7487278823852528,\n",
       " 1.5710837860107412,\n",
       " 1.3713908805847157,\n",
       " 1.1800654525756824,\n",
       " 0.97987898254394401,\n",
       " 0.78617579650878766,\n",
       " 0.59089536285400246,\n",
       " 0.3992605018615708,\n",
       " 0.20614858627319188,\n",
       " 0.01795653533935393,\n",
       " -0.16868488311767735,\n",
       " -0.35697423171997233,\n",
       " -0.55531829833984547,\n",
       " -0.73600790023803886,\n",
       " -0.9213080329895037,\n",
       " -1.1032302589416521,\n",
       " -1.3166348991394061,\n",
       " -1.529226894378664,\n",
       " -1.7417672882080097,\n",
       " -1.9476390228271503,\n",
       " -2.1606720237731953,\n",
       " -2.3613535499572773,\n",
       " -2.5627909507751485,\n",
       " -2.7683422966003439,\n",
       " -2.9741035499572774,\n",
       " -3.1758930015563984,\n",
       " -3.3748027076721212,\n",
       " -3.569269977569582,\n",
       " -3.7511985816955584,\n",
       " -3.9237823333740254,\n",
       " -4.0855786056518575,\n",
       " -4.2399828758239764,\n",
       " -4.364096778869631,\n",
       " -4.4904724006652854,\n",
       " -4.607518665313723,\n",
       " -4.7186779937744161,\n",
       " -4.8045223579406757,\n",
       " -4.8799804763793961,\n",
       " -4.9403392982482925,\n",
       " -4.9796327476501476,\n",
       " -4.9824556770324717,\n",
       " -4.9588790588378915,\n",
       " -4.9075644607543953,\n",
       " -4.8093035736083989,\n",
       " -4.7115078659057623,\n",
       " -4.5841812629699712,\n",
       " -4.4475403480529794,\n",
       " -4.305903930664063,\n",
       " -4.1499649085998538,\n",
       " -3.9825553054809575,\n",
       " -3.806099212646485,\n",
       " -3.6294248886108407,\n",
       " -3.4488466911315925,\n",
       " -3.2546384544372566,\n",
       " -3.054450439453126,\n",
       " -2.8558969650268566,\n",
       " -2.6500615730285655,\n",
       " -2.4391042251586925,\n",
       " -2.2246244163513196,\n",
       " -2.009158515930177,\n",
       " -1.7915757770538343,\n",
       " -1.569830324172975,\n",
       " -1.34525124359131,\n",
       " -1.1161573371887221,\n",
       " -0.88778698158264302,\n",
       " -0.66065974617004541,\n",
       " -0.43504240989685206,\n",
       " -0.19941524887085113,\n",
       " 0.028799804687498459,\n",
       " 0.25940461921691738,\n",
       " 0.48422204780578459,\n",
       " 0.71359935379028161,\n",
       " 0.93642118835449062,\n",
       " 1.1383537483215316,\n",
       " 1.2891607742309554,\n",
       " 1.3661253585815414,\n",
       " 1.4129895973205551,\n",
       " 1.4415979881286605,\n",
       " 1.4630155792236312,\n",
       " 1.4776256103515608,\n",
       " 1.4911900711059554,\n",
       " 1.5004324455261213,\n",
       " 1.5206326560974104,\n",
       " 1.5250744323730452,\n",
       " 1.5266442794799788,\n",
       " 1.5260691680908187,\n",
       " 1.5230838394165023,\n",
       " 1.5090066528320296,\n",
       " 1.50403773498535,\n",
       " 1.5026996459960922,\n",
       " 1.5041304626464829,\n",
       " 1.5011135482788069,\n",
       " 1.5056962814331039,\n",
       " 1.4998267898559554,\n",
       " 1.5062338485717757,\n",
       " 1.5166960220336898,\n",
       " 1.5127475433349593,\n",
       " 1.5112241363525374,\n",
       " 1.5004082794189435,\n",
       " 1.5073112716674786,\n",
       " 1.503710678100584,\n",
       " 1.5050211944580059,\n",
       " 1.5079517517089824,\n",
       " 1.5078689117431621,\n",
       " 1.5191557159423807,\n",
       " 1.5186092681884744,\n",
       " 1.5262679138183572,\n",
       " 1.528033348083494,\n",
       " 1.5465052947998026,\n",
       " 1.555157356262205,\n",
       " 1.5599823379516582,\n",
       " 1.5505173110961894,\n",
       " 1.541852066040037,\n",
       " 1.5321236648559549,\n",
       " 1.5313520431518532,\n",
       " 1.5338987350463844,\n",
       " 1.5234703979492163,\n",
       " 1.529831130981443,\n",
       " 1.5324866714477514,\n",
       " 1.5504766616821264,\n",
       " 1.5366217727661107,\n",
       " 1.5363335952758763,\n",
       " 1.5257999114990208,\n",
       " 1.5239848022460911,\n",
       " 1.4933190917968724,\n",
       " 1.4659516906738255,\n",
       " 1.44082124328613,\n",
       " 1.4283516235351534,\n",
       " 1.4062495422363253,\n",
       " 1.374829818725583,\n",
       " 1.356630973815915,\n",
       " 1.3419191856384247,\n",
       " 1.3207706756591766,\n",
       " 1.3114265670776335,\n",
       " 1.2975816879272428,\n",
       " 1.2735632820129361,\n",
       " 1.2564968147277797,\n",
       " 1.2310905036926234,\n",
       " 1.214490055084225,\n",
       " 1.1894545097351039,\n",
       " 1.1635863189697231,\n",
       " 1.1438160133361781,\n",
       " 1.1119206390380822,\n",
       " 1.0841363105773889,\n",
       " 1.0692052383422814,\n",
       " 1.0369114723205528,\n",
       " 1.0251636505126913,\n",
       " 0.9836570549011191,\n",
       " 0.95768064880370696,\n",
       " 0.93286578369140227,\n",
       " 0.91448194885253509,\n",
       " 0.90535140609740805,\n",
       " 0.8908529167175252,\n",
       " 0.87085117340087481,\n",
       " 0.85187268829345286,\n",
       " 0.83560345077514231,\n",
       " 0.81038056182860907,\n",
       " 0.77988506317138251,\n",
       " 0.74879618835448791,\n",
       " 0.7154509162902789,\n",
       " 0.70173176574706597,\n",
       " 0.68313871002196824,\n",
       " 0.66006469345092333,\n",
       " 0.63352303695678269,\n",
       " 0.6210049552917436,\n",
       " 0.60474447631835493,\n",
       " 0.59098754882812055,\n",
       " 0.58308736038207554,\n",
       " 0.55336785507201691,\n",
       " 0.55371895980834507,\n",
       " 0.5498296661376908,\n",
       " 0.52270496368407748,\n",
       " 0.51148935317992705,\n",
       " 0.51251480102538605,\n",
       " 0.48257706069945827,\n",
       " 0.43913451766967304,\n",
       " 0.43271363830565934,\n",
       " 0.4289706916809034,\n",
       " 0.40655966186522952,\n",
       " 0.40432427215575684,\n",
       " 0.40180261993407707,\n",
       " 0.40814091110228995,\n",
       " 0.41705480194091293,\n",
       " 0.4179979248046824,\n",
       " 0.42719322204589333,\n",
       " 0.40784896850585428,\n",
       " 0.4234731903076121,\n",
       " 0.41830283355712378,\n",
       " 0.43381039428710422,\n",
       " 0.4360537452697702,\n",
       " 0.45532152175902796,\n",
       " 0.48640440750121544,\n",
       " 0.49778864288329544,\n",
       " 0.51966222381591254,\n",
       " 0.5121725502014105,\n",
       " 0.5299870872497503,\n",
       " 0.53229196929931089,\n",
       " 0.56080951309203542,\n",
       " 0.56797775650023852,\n",
       " 0.56526953124999435,\n",
       " 0.56027199172973063,\n",
       " 0.56145324325560952,\n",
       " 0.54620676803588297,\n",
       " 0.54604389953612709,\n",
       " 0.56555020904540443,\n",
       " 0.55967491149901771,\n",
       " 0.56994950485228912,\n",
       " 0.60617385864257234,\n",
       " 0.61528442382811921,\n",
       " 0.63734600067138092,\n",
       " 0.63553391647338286,\n",
       " 0.62569143295287499,\n",
       " 0.63466091918944723,\n",
       " 0.63113009643554097,\n",
       " 0.64218403625487686,\n",
       " 0.62415573501586308,\n",
       " 0.63352062606810922,\n",
       " 0.63305389785765998,\n",
       " 0.6351332130432068,\n",
       " 0.61930257415770873,\n",
       " 0.61789538955687862,\n",
       " 0.6224088249206482,\n",
       " 0.6449801216125427,\n",
       " 0.66380255126952514,\n",
       " 0.67803350067138057,\n",
       " 0.69340723800658566,\n",
       " 0.69097536468505238,\n",
       " 0.7003100280761656,\n",
       " 0.69597518539428083,\n",
       " 0.70299113082885112,\n",
       " 0.68881625366210308,\n",
       " 0.68673867416381207,\n",
       " 0.69560279846190776,\n",
       " 0.68803440856932963,\n",
       " 0.68052041244506201,\n",
       " 0.67427159500121436,\n",
       " 0.66924408340453467,\n",
       " 0.66771785736083344,\n",
       " 0.65795544433593112,\n",
       " 0.6425120925903256,\n",
       " 0.65274565124511075,\n",
       " 0.63582028961180992,\n",
       " 0.59827259445189773,\n",
       " 0.58292674636840158,\n",
       " 0.57347443389891917,\n",
       " 0.56089044189452464,\n",
       " 0.54897611999511053,\n",
       " 0.54702783966063784,\n",
       " 0.55002233123778621,\n",
       " 0.52217197418212213,\n",
       " 0.50560252380370407,\n",
       " 0.51067372894286422,\n",
       " 0.50185121917723918,\n",
       " 0.5001994209289482,\n",
       " 0.49638056182860635,\n",
       " 0.48916725158690705,\n",
       " 0.455454776763909,\n",
       " 0.46651828384398708,\n",
       " 0.47794231796263942,\n",
       " 0.47873299407958275,\n",
       " 0.46987926483153586,\n",
       " 0.45653558731078392,\n",
       " 0.4417766838073659,\n",
       " 0.44488427734374281,\n",
       " 0.45016000366210218,\n",
       " 0.44376017379760019,\n",
       " 0.45082531356810795,\n",
       " 0.45509308624266848,\n",
       " 0.45234099960326413,\n",
       " 0.43320870590209221,\n",
       " 0.43522922134398673,\n",
       " 0.42637289810179918,\n",
       " 0.3949266471862718,\n",
       " 0.3781480216979905,\n",
       " 0.3467682495117112,\n",
       " 0.3402783546447678,\n",
       " 0.32024925231932833,\n",
       " 0.31711121368407441,\n",
       " 0.29861241531371308,\n",
       " 0.30138257980345912,\n",
       " 0.30460711288451375,\n",
       " 0.2972311820983809,\n",
       " 0.30153556823729688,\n",
       " 0.32380538940428905,\n",
       " 0.34609037780760932,\n",
       " 0.35143702697753115,\n",
       " 0.37451437377928892,\n",
       " 0.37850098037718932,\n",
       " 0.38013066482543151,\n",
       " 0.39304602432250185,\n",
       " 0.41337985229491392,\n",
       " 0.42857108306883968,\n",
       " 0.43202458953856621,\n",
       " 0.42270729446410327,\n",
       " 0.41504269790648607,\n",
       " 0.4396808357238689,\n",
       " 0.44038176345824392,\n",
       " 0.45878531265257988,\n",
       " 0.47086098861693532,\n",
       " 0.46956604003905444,\n",
       " 0.46682093429564619,\n",
       " 0.47651853942870287,\n",
       " 0.48641131210326338,\n",
       " 0.47348932647704267,\n",
       " 0.48153275299071452,\n",
       " 0.49655226898192545,\n",
       " 0.48461275100707191,\n",
       " 0.48662200927733557,\n",
       " 0.48792070770262852,\n",
       " 0.48277532958983554,\n",
       " 0.48190201568602692,\n",
       " 0.48440549087523588,\n",
       " 0.49131876754759918,\n",
       " 0.48146937179564603,\n",
       " 0.46826495361327297,\n",
       " 0.47281500625609524,\n",
       " 0.48396781539916162,\n",
       " 0.48111906433104634,\n",
       " 0.47235074234007951,\n",
       " 0.46776882934569469,\n",
       " 0.47883475112914198,\n",
       " 0.47037196731566538,\n",
       " 0.46828031539916148,\n",
       " 0.46214423751830214,\n",
       " 0.45981159210204237,\n",
       " 0.44616233444213027,\n",
       " 0.45683438491820449,\n",
       " 0.45380301666258926,\n",
       " 0.4421829299926674,\n",
       " 0.43535993957518687,\n",
       " 0.41420731353758922,\n",
       " 0.42516067123412238,\n",
       " 0.41450798034667119,\n",
       " 0.42174695205687623,\n",
       " 0.4209771003723059,\n",
       " 0.41953043746947388,\n",
       " 0.41812712478636843,\n",
       " 0.42896500778197388,\n",
       " 0.43494051361083125,\n",
       " 0.42563837814330191,\n",
       " 0.42684059524535273,\n",
       " 0.43629362106322378,\n",
       " 0.44386367797850701,\n",
       " 0.43597569656371205,\n",
       " 0.45793717575072379,\n",
       " 0.47550612258910269,\n",
       " 0.49664733505248154,\n",
       " 0.50268885803221786,\n",
       " 0.51903934478758895,\n",
       " 0.52828293228148537,\n",
       " 0.52779132843016707,\n",
       " 0.53180428314208106,\n",
       " 0.53581795501708107,\n",
       " 0.53433842849730562,\n",
       " 0.53584904861449312,\n",
       " 0.54285985183714935,\n",
       " 0.54219663238524507,\n",
       " 0.53526670837401458,\n",
       " 0.53967415237425875,\n",
       " 0.54207117843627051,\n",
       " 0.53012726211546979,\n",
       " 0.54852157211302832,\n",
       " 0.55208805465697364,\n",
       " 0.54380615234374119,\n",
       " 0.53170170211791112,\n",
       " 0.50907825851439548,\n",
       " 0.49660317230223727,\n",
       " 0.49263135147093845,\n",
       " 0.48090898895262785,\n",
       " 0.47504151153563562,\n",
       " 0.47634094238280356,\n",
       " 0.47341262817381918,\n",
       " 0.46253703689574305,\n",
       " 0.44898112106322347,\n",
       " 0.45285518646239337,\n",
       " 0.45678440475462967,\n",
       " 0.45651194000243239,\n",
       " 0.46453319931029374,\n",
       " 0.46289609527586989,\n",
       " 0.45853734970091869,\n",
       " 0.46684724807738354,\n",
       " 0.45717515945433662,\n",
       " 0.45111082458495183,\n",
       " 0.45934734725951237,\n",
       " 0.46733713531493232,\n",
       " 0.47731256484984447,\n",
       " 0.47210407638548896,\n",
       " 0.46817923736571354,\n",
       " 0.45882915115355533,\n",
       " 0.47736247253417058,\n",
       " 0.46439870834349672,\n",
       " 0.46884947586058656,\n",
       " 0.473477268218985,\n",
       " 0.46681581497191466,\n",
       " 0.46341442108153375,\n",
       " 0.46738798904418022,\n",
       " 0.44526254272460009,\n",
       " 0.43925944137572309,\n",
       " 0.44017684555052777,\n",
       " 0.42164489746092815,\n",
       " 0.41196797943114299,\n",
       " 0.39883112716673869,\n",
       " 0.39501067733763712,\n",
       " 0.38366192626952189,\n",
       " 0.37683839416502968,\n",
       " 0.37558571243285194,\n",
       " 0.37010686111449254,\n",
       " 0.36556097412108435,\n",
       " 0.35796168136595741,\n",
       " 0.36005522537230505,\n",
       " 0.35924031829833042,\n",
       " 0.36842391967772492,\n",
       " 0.36733556747435575,\n",
       " 0.36071647262572293,\n",
       " 0.35284240722655297,\n",
       " 0.34887326431273458,\n",
       " 0.33547326660155291,\n",
       " 0.33405900955199236,\n",
       " 0.31866534423827164,\n",
       " 0.3227088584899806,\n",
       " 0.30453488922118177,\n",
       " 0.29722890853880873,\n",
       " 0.27696757125853527,\n",
       " 0.27237253952025398,\n",
       " 0.29319416809081056,\n",
       " 0.28673411560057616,\n",
       " 0.27021400833128906,\n",
       " 0.27732049560545896,\n",
       " 0.27634475708006834,\n",
       " 0.28372817230223629,\n",
       " 0.2970156669616601,\n",
       " 0.31644092559813469,\n",
       " 0.32542094421385737,\n",
       " 0.33540719223021476,\n",
       " 0.35332269287108387,\n",
       " 0.360569305419912,\n",
       " 0.3685157012939354,\n",
       " 0.37485253524779283,\n",
       " 0.36272537994383774,\n",
       " 0.37527356338499984,\n",
       " 0.38531405639647442,\n",
       " 0.39499918365477521,\n",
       " 0.39538447952269512,\n",
       " 0.39090174102782205,\n",
       " 0.40173450088499979,\n",
       " 0.4142783317565818,\n",
       " 0.42558234405516576,\n",
       " 0.43351598358153293,\n",
       " 0.42453155517577118,\n",
       " 0.41490149307249968,\n",
       " 0.41976795959471647,\n",
       " 0.43009645080565395,\n",
       " 0.43502449417113243,\n",
       " 0.42570343399046834,\n",
       " 0.41573128509520463,\n",
       " 0.39214252090453078,\n",
       " 0.39085115051268504,\n",
       " 0.38127347946165963,\n",
       " 0.37842134857176701,\n",
       " 0.37745850372313416,\n",
       " 0.38316761398314392,\n",
       " 0.39745448684691342,\n",
       " 0.40263433074950128,\n",
       " 0.40656577301024344,\n",
       " 0.40827394866942313,\n",
       " 0.41068215560912036,\n",
       " 0.42269480895995037,\n",
       " 0.42748363876341716,\n",
       " 0.42881532669066325,\n",
       " 0.43342645645140537,\n",
       " 0.43528293991087802,\n",
       " 0.4350038070678604,\n",
       " 0.44198533630370024,\n",
       " 0.44486422348021393,\n",
       " 0.43821414566038969,\n",
       " 0.42803037643431546,\n",
       " 0.41719267272948146,\n",
       " 0.40509336853026268,\n",
       " 0.40515131378172747,\n",
       " 0.40151454544066301,\n",
       " 0.40934772109984263,\n",
       " 0.40227116394041879,\n",
       " 0.41168688583372931,\n",
       " 0.40514380645750858,\n",
       " 0.40443716430662968,\n",
       " 0.38999193191527226,\n",
       " 0.39613678359984256,\n",
       " 0.39124176406859251,\n",
       " 0.39291920089720578,\n",
       " 0.39398062133787959,\n",
       " 0.38035723114012565,\n",
       " 0.38074549484251824,\n",
       " 0.38401583480833851,\n",
       " 0.38775769042967634,\n",
       " 0.39122217559813333,\n",
       " 0.40783889007567237,\n",
       " 0.39993676376341647,\n",
       " 0.39198630523680511,\n",
       " 0.38918769836424649,\n",
       " 0.3938344573974496,\n",
       " 0.38526314544676599,\n",
       " 0.38565560531615073,\n",
       " 0.3876643295287972,\n",
       " 0.37922564697264483,\n",
       " 0.39191524124144361,\n",
       " 0.39404776000975411,\n",
       " 0.38166447830199041,\n",
       " 0.39362360000609192,\n",
       " 0.39508471679686341,\n",
       " 0.39450947189329894,\n",
       " 0.41348907089232234,\n",
       " 0.40817725372313285,\n",
       " 0.40880836105345508,\n",
       " 0.39407404708861132,\n",
       " 0.39900141906737108,\n",
       " 0.39988318252562299,\n",
       " 0.39687067031859175,\n",
       " 0.38938070297240029,\n",
       " 0.39483223724364053,\n",
       " 0.39830897903441198,\n",
       " 0.40064502716063266,\n",
       " 0.40214382934569121,\n",
       " 0.36948010635374784,\n",
       " 0.37560284042357206,\n",
       " 0.38888336944578883,\n",
       " 0.39654253387449973,\n",
       " 0.38964504241942161,\n",
       " 0.39128680419920675,\n",
       " 0.38310402297972435,\n",
       " 0.38480494308470481,\n",
       " 0.36556705474852313,\n",
       " 0.3657274131774782,\n",
       " 0.36108351898192154,\n",
       " 0.37100805664061293,\n",
       " 0.37803133392332772,\n",
       " 0.39655601119993905,\n",
       " 0.3686448402404664,\n",
       " 0.34831874847410893,\n",
       " 0.33330346298216557,\n",
       " 0.3410916633605835,\n",
       " 0.34246495056151122,\n",
       " 0.34526491165159912,\n",
       " 0.34182583618162837,\n",
       " 0.33456366348265371,\n",
       " 0.34226551055906973,\n",
       " 0.34521323394774162,\n",
       " 0.34206257247923577,\n",
       " 0.35035059738157953,\n",
       " 0.33287480163572991,\n",
       " 0.32308185958861074,\n",
       " 0.33238511276243882,\n",
       " 0.32512199020384508,\n",
       " 0.32909179306029035,\n",
       " 0.34019005584715556,\n",
       " 0.32531458663939183,\n",
       " 0.3337485504150266,\n",
       " 0.32930199813841526,\n",
       " 0.31084329605101291,\n",
       " 0.29319752883909883,\n",
       " 0.2871909484863156,\n",
       " 0.27647685241697967,\n",
       " 0.26486764907835658,\n",
       " 0.27588551712034876,\n",
       " 0.26706243896483117,\n",
       " 0.26853992843626667,\n",
       " 0.26010224914549518,\n",
       " 0.26759909820555378,\n",
       " 0.27503542709349321,\n",
       " 0.27894578552244825,\n",
       " 0.26665879058836622,\n",
       " 0.27543868255613962,\n",
       " 0.28216038131712595,\n",
       " 0.28590525054930366,\n",
       " 0.28229834747313176,\n",
       " 0.29107592391966497,\n",
       " 0.29429845046995795,\n",
       " 0.28950682830809266,\n",
       " 0.29262127304075869,\n",
       " 0.30994738388060245,\n",
       " 0.30797324752806338,\n",
       " 0.30360511398314144,\n",
       " 0.32339195632933282,\n",
       " 0.29938175201414724,\n",
       " 0.30099972534178393,\n",
       " 0.30005466842650069,\n",
       " 0.29322270584105142,\n",
       " 0.30162642288206704,\n",
       " 0.31380963897703773,\n",
       " 0.30248377609251625,\n",
       " 0.29671988296507479,\n",
       " 0.30276866531370755,\n",
       " 0.29559454345701808,\n",
       " 0.31358705902098288,\n",
       " 0.31034996414183247,\n",
       " 0.31259101867674455,\n",
       " 0.31372077560423478,\n",
       " 0.31562326049803363,\n",
       " 0.30068018341063124,\n",
       " 0.29991961288450819,\n",
       " 0.31011617660521129,\n",
       " 0.30232242202757453,\n",
       " 0.3145159835815296,\n",
       " 0.32029946899412726,\n",
       " 0.32823157119749641,\n",
       " 0.32994034194944949,\n",
       " 0.32096622848509404,\n",
       " 0.33294491958616823,\n",
       " 0.32297822189329711,\n",
       " 0.3243656845092639,\n",
       " 0.31825678634642207,\n",
       " 0.32676945114134393,\n",
       " 0.3197660675048693,\n",
       " 0.32710841369627552,\n",
       " 0.32917885971067984,\n",
       " 0.32773830413817007,\n",
       " 0.33396239089964463,\n",
       " 0.35372862243650982,\n",
       " 0.36310582351683207,\n",
       " 0.3776975250244004,\n",
       " 0.3791736030578477,\n",
       " 0.39395610427855082,\n",
       " 0.398541599273668,\n",
       " 0.40609770202635354,\n",
       " 0.41075630187986911,\n",
       " 0.41133024215696873,\n",
       " 0.40560779953001558,\n",
       " 0.41091912841795503,\n",
       " 0.4113857688903671,\n",
       " 0.40691836547850185,\n",
       " 0.39113392639158778,\n",
       " 0.37981936645506431,\n",
       " 0.35403677749632406,\n",
       " 0.34021329116819904,\n",
       " 0.32528667068480061,\n",
       " 0.3168665657043318,\n",
       " 0.31744065093992752,\n",
       " 0.31329883956907789,\n",
       " 0.31618142700193919,\n",
       " 0.30657027053831615,\n",
       " 0.29995871353148024,\n",
       " 0.30051715469358958,\n",
       " 0.29775819396971265,\n",
       " 0.3041815032958845,\n",
       " 0.30935465621946845,\n",
       " 0.29993842315672431,\n",
       " 0.30903312683104067,\n",
       " 0.31887988281248597,\n",
       " 0.32096057891844298,\n",
       " 0.32924694824217343,\n",
       " 0.33183856582640192,\n",
       " 0.33325667572020073,\n",
       " 0.33310718917845267,\n",
       " 0.33055850601194875,\n",
       " 0.32737411880491746,\n",
       " 0.32156526184080614,\n",
       " 0.31064161300657761,\n",
       " 0.30908816146849166,\n",
       " 0.31050144195555218,\n",
       " 0.30687616729734907,\n",
       " 0.30114706802366742,\n",
       " 0.3071702766418315,\n",
       " 0.30437607192991745,\n",
       " 0.29196953582762253,\n",
       " 0.29620843505857952,\n",
       " 0.2947473335265971,\n",
       " 0.29066071701048379,\n",
       " 0.2914044685363627,\n",
       " 0.29009689331053262,\n",
       " 0.29137715148924354,\n",
       " 0.28568498611448767,\n",
       " 0.2835315818786478,\n",
       " 0.28899349975584504,\n",
       " 0.28982810211180204,\n",
       " 0.30070673370359891,\n",
       " 0.29744391250608915,\n",
       " 0.29994489288328641,\n",
       " 0.29603833389280787,\n",
       " 0.2949350471496438,\n",
       " 0.28971479797361843,\n",
       " 0.27688039398191922,\n",
       " 0.26557902526854027,\n",
       " 0.26854985046385277,\n",
       " 0.27305204010008322,\n",
       " 0.27824182128904806,\n",
       " 0.27521953201292504,\n",
       " 0.28229837799070823,\n",
       " 0.29446195602415548,\n",
       " 0.2927855567931984,\n",
       " 0.2903747215270851,\n",
       " 0.28901384735105973,\n",
       " 0.28139345169065932,\n",
       " 0.28670366668699721,\n",
       " 0.28932773208616713,\n",
       " 0.28703598403929209,\n",
       " 0.28936518478392098,\n",
       " 0.28971516036985845,\n",
       " 0.29805590438841312,\n",
       " 0.2936888771056983,\n",
       " 0.30646522521971198,\n",
       " 0.3104334602355811,\n",
       " 0.32204648590086432,\n",
       " 0.32294321060179204,\n",
       " 0.29272436523436041,\n",
       " 0.28517877197264163,\n",
       " 0.2690042266845557,\n",
       " 0.26761601638792482,\n",
       " 0.2760681457519385,\n",
       " 0.27640922927854983,\n",
       " 0.26805955886839355,\n",
       " 0.27095619201658694,\n",
       " 0.25707197570799317,\n",
       " 0.26009883880613766,\n",
       " 0.26344426345823724,\n",
       " 0.25702836990354971,\n",
       " 0.26077250289915516,\n",
       " 0.26243295288084456,\n",
       " 0.26288357162474102,\n",
       " 0.26150503540037578,\n",
       " 0.26779360961912574,\n",
       " 0.28470719146727025,\n",
       " 0.28202182769773898,\n",
       " 0.28841913223265109,\n",
       " 0.28611556625364715,\n",
       " 0.29195586776731902,\n",
       " 0.29269992828367641,\n",
       " 0.302008171081528,\n",
       " 0.31331722640989712,\n",
       " 0.32356526947019987,\n",
       " 0.31932458877561976,\n",
       " 0.32181151580809048,\n",
       " 0.31694164657591273,\n",
       " 0.33121356582640099,\n",
       " 0.32383226776121543,\n",
       " 0.32650394058226034,\n",
       " 0.31469792938230917,\n",
       " 0.30992219543455529,\n",
       " 0.30493240356443807,\n",
       " 0.27738211441038529,\n",
       " 0.26851064300535599,\n",
       " 0.27831940078733836,\n",
       " 0.26844081878660592,\n",
       " 0.26501127243040473,\n",
       " 0.25019485473631292,\n",
       " 0.23315409851072696,\n",
       " 0.23917110824583437,\n",
       " 0.23041553115843202,\n",
       " 0.23690282440184024,\n",
       " 0.23827935791014102,\n",
       " 0.24956094741819765,\n",
       " 0.25253361129759216,\n",
       " 0.26137563323973084,\n",
       " 0.26164401245115659,\n",
       " 0.26195212936399837,\n",
       " 0.27104904937742608,\n",
       " 0.27302195358274833,\n",
       " 0.26842490386961354,\n",
       " 0.28422420501707446,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
