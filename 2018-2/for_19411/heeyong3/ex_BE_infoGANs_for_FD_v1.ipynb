{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum infoGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_infoGANs_for_FD_v1'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     123
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_epoch = 100\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.001\n",
    "c_size = 10\n",
    "\n",
    "\n",
    "def G(x,c,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "        x_concat = tf.concat([x,c],3)\n",
    "        conv1 = tf.layers.conv2d_transpose(x_concat,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "        #\n",
    "        \n",
    "        fc0  = tf.reshape(conv4, (-1, 4*4*512))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[4*4*512, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "        fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "        \n",
    "        \n",
    "    r5 = tf.nn.tanh(tf.layers.batch_normalization(conv5,training=isTrain), name = name)#4*4*512\n",
    "  \n",
    "  \n",
    "    return r5, tf.reshape(fc1,(-1,1,1,c_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "def Q_cat(x,reuse = False, name = 'Q_cat') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('Q_cat', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        fc0  = tf.reshape(x, (-1, 100))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[100, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "    fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "    \n",
    "    return tf.reshape(fc1, (-1,1,1,c_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "c = tf.placeholder(tf.float32,shape=(None,1,1,c_size),name = 'c')    #x_z = G(z,c)\n",
    "\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,c,name='G_sample') # G(z)\n",
    "E_z, E_c = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z,E_c, isTrain, reuse=True, name ='re_image')\n",
    "re_z, re_c = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "\n",
    "\n",
    "re_z_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z - z)**2, axis=[1,2,3])) , name = 're_z_loss') \n",
    "re_c_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(re_c + 1e-8), axis = [1,2,3]),name = 're_c_loss')\n",
    "\n",
    "E_loss = tf.add(re_z_loss, re_c_loss, name = 'E_loss')                       \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "Q_fake = Q_cat(D_enc(G_sample, isTrain,reuse=True), reuse=False, name='Q_fake')\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "Q_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(Q_fake + 1e-8), axis = [1,2,3]),name = 'Q_loss')\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "Q_vars = [var for var in T_vars if var.name.startswith('Q')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss + Q_loss, var_list=G_vars+Q_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 1, 1, 10) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -81.848, D_real_e : 49.802, D_fake_e : 34.193, G_e : 34.495, Q_e : 1.878, new_measure : 62.941, k_curr : 1.025671\n",
      "D_e : 32.911, D_real_e : 38.195, D_fake_e : 26.527, G_e : 26.984, Q_e : 1.900, new_measure : 40.995, k_curr : 0.333299\n",
      "D_e : 25.747, D_real_e : 34.038, D_fake_e : 23.250, G_e : 23.858, Q_e : 1.790, new_measure : 35.275, k_curr : 0.247024\n",
      "D_e : 24.950, D_real_e : 29.372, D_fake_e : 20.231, G_e : 20.646, Q_e : 1.578, new_measure : 30.689, k_curr : 0.009459\n",
      "D_e : 22.574, D_real_e : 25.704, D_fake_e : 17.631, G_e : 17.931, Q_e : 1.380, new_measure : 27.133, k_curr : 0.183510\n",
      "D_e : 21.009, D_real_e : 22.371, D_fake_e : 15.512, G_e : 15.673, Q_e : 1.219, new_measure : 23.997, k_curr : 0.146353\n",
      "D_e : 18.839, D_real_e : 19.746, D_fake_e : 13.708, G_e : 13.819, Q_e : 1.089, new_measure : 21.145, k_curr : 0.154618\n",
      "D_e : 16.864, D_real_e : 17.295, D_fake_e : 12.051, G_e : 12.128, Q_e : 0.980, new_measure : 18.567, k_curr : 0.094461\n",
      "D_e : 15.476, D_real_e : 15.312, D_fake_e : 10.681, G_e : 10.743, Q_e : 0.887, new_measure : 16.857, k_curr : 0.025254\n",
      "D_e : 13.893, D_real_e : 13.618, D_fake_e : 9.464, G_e : 9.488, Q_e : 0.810, new_measure : 14.899, k_curr : 0.149136\n",
      "D_e : 12.376, D_real_e : 12.365, D_fake_e : 8.790, G_e : 8.881, Q_e : 0.744, new_measure : 13.696, k_curr : -0.482282\n",
      "D_e : 11.578, D_real_e : 11.289, D_fake_e : 7.712, G_e : 7.736, Q_e : 0.687, new_measure : 12.334, k_curr : -0.016663\n",
      "D_e : 10.243, D_real_e : 10.186, D_fake_e : 7.116, G_e : 7.195, Q_e : 0.638, new_measure : 11.101, k_curr : -0.197474\n",
      "D_e : 9.630, D_real_e : 9.756, D_fake_e : 6.715, G_e : 6.769, Q_e : 0.596, new_measure : 10.549, k_curr : -0.029958\n",
      "D_e : 9.205, D_real_e : 9.035, D_fake_e : 6.376, G_e : 6.409, Q_e : 0.559, new_measure : 9.940, k_curr : -0.267638\n",
      "D_e : 8.749, D_real_e : 8.813, D_fake_e : 5.989, G_e : 6.056, Q_e : 0.526, new_measure : 9.621, k_curr : 0.048425\n",
      "D_e : 8.523, D_real_e : 8.516, D_fake_e : 5.911, G_e : 5.963, Q_e : 0.498, new_measure : 9.212, k_curr : 0.043874\n",
      "D_e : 7.884, D_real_e : 8.023, D_fake_e : 5.549, G_e : 5.630, Q_e : 0.472, new_measure : 8.591, k_curr : 0.006071\n",
      "D_e : 7.628, D_real_e : 7.793, D_fake_e : 5.361, G_e : 5.446, Q_e : 0.449, new_measure : 8.326, k_curr : 0.030721\n",
      "D_e : 7.233, D_real_e : 7.385, D_fake_e : 5.092, G_e : 5.170, Q_e : 0.428, new_measure : 7.797, k_curr : 0.029641\n",
      "D_e : 6.765, D_real_e : 6.933, D_fake_e : 4.768, G_e : 4.855, Q_e : 0.409, new_measure : 7.248, k_curr : 0.025463\n",
      "D_e : 6.326, D_real_e : 6.457, D_fake_e : 4.424, G_e : 4.519, Q_e : 0.392, new_measure : 6.756, k_curr : 0.027537\n",
      "D_e : 5.952, D_real_e : 6.088, D_fake_e : 4.158, G_e : 4.255, Q_e : 0.376, new_measure : 6.355, k_curr : 0.045634\n",
      "D_e : 5.683, D_real_e : 5.838, D_fake_e : 3.993, G_e : 4.091, Q_e : 0.361, new_measure : 6.079, k_curr : 0.034767\n",
      "D_e : 5.541, D_real_e : 5.682, D_fake_e : 3.875, G_e : 3.979, Q_e : 0.347, new_measure : 5.922, k_curr : 0.032200\n",
      "D_e : 5.421, D_real_e : 5.552, D_fake_e : 3.785, G_e : 3.879, Q_e : 0.335, new_measure : 5.770, k_curr : 0.052621\n",
      "D_e : 5.345, D_real_e : 5.470, D_fake_e : 3.731, G_e : 3.835, Q_e : 0.323, new_measure : 5.687, k_curr : 0.036534\n",
      "D_e : 5.276, D_real_e : 5.392, D_fake_e : 3.678, G_e : 3.776, Q_e : 0.312, new_measure : 5.626, k_curr : 0.032636\n",
      "D_e : 5.179, D_real_e : 5.298, D_fake_e : 3.602, G_e : 3.705, Q_e : 0.302, new_measure : 5.526, k_curr : 0.043309\n",
      "D_e : 5.105, D_real_e : 5.203, D_fake_e : 3.538, G_e : 3.646, Q_e : 0.293, new_measure : 5.427, k_curr : 0.032492\n",
      "D_e : 5.060, D_real_e : 5.159, D_fake_e : 3.514, G_e : 3.618, Q_e : 0.284, new_measure : 5.384, k_curr : 0.011554\n",
      "D_e : 4.985, D_real_e : 5.088, D_fake_e : 3.453, G_e : 3.552, Q_e : 0.275, new_measure : 5.300, k_curr : 0.036899\n",
      "D_e : 4.937, D_real_e : 5.013, D_fake_e : 3.428, G_e : 3.519, Q_e : 0.267, new_measure : 5.252, k_curr : 0.009586\n",
      "D_e : 4.909, D_real_e : 4.984, D_fake_e : 3.389, G_e : 3.484, Q_e : 0.260, new_measure : 5.210, k_curr : 0.024351\n",
      "D_e : 4.878, D_real_e : 4.941, D_fake_e : 3.349, G_e : 3.455, Q_e : 0.252, new_measure : 5.168, k_curr : 0.036137\n",
      "D_e : 4.803, D_real_e : 4.877, D_fake_e : 3.317, G_e : 3.415, Q_e : 0.246, new_measure : 5.114, k_curr : 0.032370\n",
      "D_e : 4.749, D_real_e : 4.816, D_fake_e : 3.280, G_e : 3.379, Q_e : 0.239, new_measure : 5.047, k_curr : 0.011561\n",
      "D_e : 4.717, D_real_e : 4.796, D_fake_e : 3.238, G_e : 3.353, Q_e : 0.233, new_measure : 5.034, k_curr : 0.022510\n",
      "D_e : 4.683, D_real_e : 4.750, D_fake_e : 3.232, G_e : 3.330, Q_e : 0.227, new_measure : 4.966, k_curr : 0.007588\n",
      "D_e : 4.640, D_real_e : 4.715, D_fake_e : 3.188, G_e : 3.292, Q_e : 0.222, new_measure : 4.940, k_curr : 0.030305\n",
      "D_e : 4.605, D_real_e : 4.684, D_fake_e : 3.182, G_e : 3.285, Q_e : 0.216, new_measure : 4.907, k_curr : 0.012771\n",
      "D_e : 4.569, D_real_e : 4.649, D_fake_e : 3.124, G_e : 3.249, Q_e : 0.211, new_measure : 4.889, k_curr : 0.028313\n",
      "D_e : 4.529, D_real_e : 4.599, D_fake_e : 3.108, G_e : 3.217, Q_e : 0.207, new_measure : 4.827, k_curr : 0.035609\n",
      "D_e : 4.512, D_real_e : 4.580, D_fake_e : 3.104, G_e : 3.212, Q_e : 0.202, new_measure : 4.826, k_curr : 0.018324\n",
      "D_e : 4.453, D_real_e : 4.539, D_fake_e : 3.053, G_e : 3.174, Q_e : 0.198, new_measure : 4.751, k_curr : 0.028092\n",
      "D_e : 4.422, D_real_e : 4.513, D_fake_e : 3.047, G_e : 3.161, Q_e : 0.193, new_measure : 4.731, k_curr : 0.023009\n",
      "D_e : 4.429, D_real_e : 4.509, D_fake_e : 3.029, G_e : 3.151, Q_e : 0.189, new_measure : 4.730, k_curr : 0.037953\n",
      "D_e : 4.390, D_real_e : 4.480, D_fake_e : 2.999, G_e : 3.139, Q_e : 0.185, new_measure : 4.693, k_curr : 0.031866\n",
      "D_e : 4.363, D_real_e : 4.447, D_fake_e : 3.004, G_e : 3.119, Q_e : 0.182, new_measure : 4.651, k_curr : 0.015604\n",
      "D_e : 4.349, D_real_e : 4.425, D_fake_e : 2.978, G_e : 3.092, Q_e : 0.178, new_measure : 4.637, k_curr : 0.032711\n",
      "D_e : 4.314, D_real_e : 4.389, D_fake_e : 2.968, G_e : 3.078, Q_e : 0.175, new_measure : 4.608, k_curr : 0.015590\n",
      "D_e : 4.304, D_real_e : 4.372, D_fake_e : 2.952, G_e : 3.058, Q_e : 0.171, new_measure : 4.595, k_curr : 0.020176\n",
      "D_e : 4.264, D_real_e : 4.333, D_fake_e : 2.930, G_e : 3.032, Q_e : 0.168, new_measure : 4.533, k_curr : 0.021818\n",
      "D_e : 4.262, D_real_e : 4.324, D_fake_e : 2.921, G_e : 3.025, Q_e : 0.165, new_measure : 4.550, k_curr : 0.028810\n",
      "D_e : 4.225, D_real_e : 4.282, D_fake_e : 2.890, G_e : 3.002, Q_e : 0.162, new_measure : 4.501, k_curr : 0.016263\n",
      "D_e : 4.225, D_real_e : 4.288, D_fake_e : 2.890, G_e : 3.001, Q_e : 0.159, new_measure : 4.485, k_curr : 0.017070\n",
      "D_e : 4.180, D_real_e : 4.247, D_fake_e : 2.862, G_e : 2.974, Q_e : 0.156, new_measure : 4.447, k_curr : 0.013132\n",
      "D_e : 4.188, D_real_e : 4.242, D_fake_e : 2.864, G_e : 2.966, Q_e : 0.154, new_measure : 4.443, k_curr : 0.024390\n",
      "D_e : 4.162, D_real_e : 4.218, D_fake_e : 2.856, G_e : 2.954, Q_e : 0.151, new_measure : 4.415, k_curr : 0.021620\n",
      "D_e : 4.142, D_real_e : 4.193, D_fake_e : 2.839, G_e : 2.933, Q_e : 0.149, new_measure : 4.375, k_curr : 0.025613\n",
      "D_e : 4.111, D_real_e : 4.163, D_fake_e : 2.812, G_e : 2.918, Q_e : 0.146, new_measure : 4.386, k_curr : 0.014228\n",
      "D_e : 4.082, D_real_e : 4.140, D_fake_e : 2.805, G_e : 2.894, Q_e : 0.144, new_measure : 4.328, k_curr : 0.023277\n",
      "D_e : 4.071, D_real_e : 4.122, D_fake_e : 2.790, G_e : 2.886, Q_e : 0.142, new_measure : 4.349, k_curr : 0.020736\n",
      "D_e : 4.052, D_real_e : 4.106, D_fake_e : 2.774, G_e : 2.873, Q_e : 0.140, new_measure : 4.312, k_curr : 0.022435\n",
      "D_e : 4.040, D_real_e : 4.095, D_fake_e : 2.751, G_e : 2.866, Q_e : 0.138, new_measure : 4.298, k_curr : 0.023651\n",
      "D_e : 4.034, D_real_e : 4.093, D_fake_e : 2.739, G_e : 2.861, Q_e : 0.136, new_measure : 4.295, k_curr : 0.033595\n",
      "D_e : 4.001, D_real_e : 4.059, D_fake_e : 2.748, G_e : 2.846, Q_e : 0.134, new_measure : 4.270, k_curr : 0.020430\n",
      "D_e : 3.993, D_real_e : 4.040, D_fake_e : 2.741, G_e : 2.834, Q_e : 0.132, new_measure : 4.254, k_curr : 0.002323\n",
      "D_e : 3.978, D_real_e : 4.026, D_fake_e : 2.724, G_e : 2.814, Q_e : 0.130, new_measure : 4.224, k_curr : 0.013231\n",
      "D_e : 3.977, D_real_e : 4.024, D_fake_e : 2.713, G_e : 2.813, Q_e : 0.128, new_measure : 4.219, k_curr : 0.023370\n",
      "D_e : 3.967, D_real_e : 4.012, D_fake_e : 2.726, G_e : 2.812, Q_e : 0.126, new_measure : 4.212, k_curr : 0.011653\n",
      "D_e : 3.951, D_real_e : 3.994, D_fake_e : 2.714, G_e : 2.798, Q_e : 0.124, new_measure : 4.215, k_curr : 0.005347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : 3.936, D_real_e : 3.974, D_fake_e : 2.699, G_e : 2.778, Q_e : 0.123, new_measure : 4.199, k_curr : 0.013942\n",
      "D_e : 3.919, D_real_e : 3.965, D_fake_e : 2.686, G_e : 2.778, Q_e : 0.121, new_measure : 4.186, k_curr : 0.007370\n",
      "D_e : 3.896, D_real_e : 3.936, D_fake_e : 2.655, G_e : 2.749, Q_e : 0.120, new_measure : 4.155, k_curr : 0.024993\n",
      "D_e : 3.875, D_real_e : 3.921, D_fake_e : 2.661, G_e : 2.750, Q_e : 0.118, new_measure : 4.132, k_curr : 0.009455\n",
      "D_e : 3.872, D_real_e : 3.908, D_fake_e : 2.645, G_e : 2.734, Q_e : 0.116, new_measure : 4.117, k_curr : 0.012795\n",
      "D_e : 3.862, D_real_e : 3.904, D_fake_e : 2.656, G_e : 2.738, Q_e : 0.115, new_measure : 4.127, k_curr : -0.002689\n",
      "D_e : 3.843, D_real_e : 3.882, D_fake_e : 2.625, G_e : 2.711, Q_e : 0.114, new_measure : 4.097, k_curr : 0.014162\n",
      "D_e : 3.836, D_real_e : 3.876, D_fake_e : 2.628, G_e : 2.714, Q_e : 0.112, new_measure : 4.077, k_curr : 0.012299\n",
      "D_e : 3.824, D_real_e : 3.860, D_fake_e : 2.623, G_e : 2.706, Q_e : 0.111, new_measure : 4.104, k_curr : 0.002495\n",
      "D_e : 3.820, D_real_e : 3.856, D_fake_e : 2.608, G_e : 2.697, Q_e : 0.110, new_measure : 4.066, k_curr : 0.008470\n",
      "D_e : 3.802, D_real_e : 3.836, D_fake_e : 2.609, G_e : 2.689, Q_e : 0.108, new_measure : 4.064, k_curr : -0.001538\n",
      "D_e : 3.779, D_real_e : 3.811, D_fake_e : 2.575, G_e : 2.662, Q_e : 0.107, new_measure : 4.057, k_curr : 0.013530\n",
      "D_e : 3.787, D_real_e : 3.821, D_fake_e : 2.587, G_e : 2.674, Q_e : 0.106, new_measure : 4.042, k_curr : 0.015112\n",
      "D_e : 3.772, D_real_e : 3.803, D_fake_e : 2.588, G_e : 2.663, Q_e : 0.105, new_measure : 4.026, k_curr : 0.013611\n",
      "D_e : 3.776, D_real_e : 3.809, D_fake_e : 2.580, G_e : 2.664, Q_e : 0.103, new_measure : 4.020, k_curr : 0.019431\n",
      "D_e : 3.744, D_real_e : 3.782, D_fake_e : 2.561, G_e : 2.643, Q_e : 0.102, new_measure : 3.983, k_curr : 0.030667\n",
      "D_e : 3.740, D_real_e : 3.779, D_fake_e : 2.577, G_e : 2.654, Q_e : 0.101, new_measure : 3.986, k_curr : 0.004701\n",
      "D_e : 3.724, D_real_e : 3.766, D_fake_e : 2.556, G_e : 2.633, Q_e : 0.100, new_measure : 3.975, k_curr : 0.013074\n",
      "D_e : 3.722, D_real_e : 3.760, D_fake_e : 2.554, G_e : 2.638, Q_e : 0.099, new_measure : 3.997, k_curr : -0.002874\n",
      "D_e : 3.699, D_real_e : 3.735, D_fake_e : 2.536, G_e : 2.614, Q_e : 0.098, new_measure : 3.955, k_curr : -0.001122\n",
      "D_e : 3.696, D_real_e : 3.737, D_fake_e : 2.519, G_e : 2.607, Q_e : 0.097, new_measure : 3.941, k_curr : 0.023620\n",
      "D_e : 3.690, D_real_e : 3.729, D_fake_e : 2.530, G_e : 2.611, Q_e : 0.096, new_measure : 3.918, k_curr : 0.022404\n",
      "D_e : 3.674, D_real_e : 3.715, D_fake_e : 2.516, G_e : 2.598, Q_e : 0.095, new_measure : 3.897, k_curr : 0.030797\n",
      "D_e : 3.659, D_real_e : 3.698, D_fake_e : 2.509, G_e : 2.596, Q_e : 0.094, new_measure : 3.872, k_curr : 0.009791\n",
      "D_e : 3.660, D_real_e : 3.694, D_fake_e : 2.492, G_e : 2.582, Q_e : 0.093, new_measure : 3.887, k_curr : 0.020153\n",
      "D_e : 3.647, D_real_e : 3.684, D_fake_e : 2.501, G_e : 2.576, Q_e : 0.092, new_measure : 3.894, k_curr : 0.026622\n",
      "D_e : 3.642, D_real_e : 3.675, D_fake_e : 2.495, G_e : 2.575, Q_e : 0.091, new_measure : 3.886, k_curr : 0.021455\n",
      "D_e : 3.616, D_real_e : 3.654, D_fake_e : 2.481, G_e : 2.557, Q_e : 0.090, new_measure : 3.847, k_curr : 0.023578\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    tf.set_random_seed(int(time.time()))\n",
    "    \n",
    "    one_hot = np.eye(c_size)\n",
    "    temp2 = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4])\n",
    "    test_c = one_hot[temp2].reshape([-1,1,1,c_size])\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    Q_error=[]\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            temp1 = np.random.randint(0,10,(batch_size))                                                                                                                                     \n",
    "            c_ = one_hot[temp1].reshape([-1,1,1,c_size])\n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, c : c_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e,Q_e = sess.run([G_optim, G_loss,Q_loss], {u : u_, z : z_, c : c_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            Q_error.append(Q_e)\n",
    "\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, Q_e : %.3f, new_measure : %.3f, k_curr : %3f'\n",
    "              %(np.mean(D_error), np.mean(D_real_error),np.mean(D_fake_error), np.mean(G_error),\n",
    "                np.mean(Q_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, c : test_c, isTrain : False})       \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "    \n",
    "    for i in range(1000000) :\n",
    "        z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "        temp1 = np.random.randint(0,10,(batch_size))                                                                                                                                     \n",
    "        c_ = one_hot[temp1].reshape([-1,1,1,c_size])\n",
    "        _ , E_e = sess.run([E_optim, E_loss], {u : u_, z : z_, c : c_,isTrain : True})\n",
    "            \n",
    "    r = sess.run([re_image],feed_dict={u : test_normal_data[0:16],isTrain : False})        \n",
    "    mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/origin_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "    r = sess.run([re_image],feed_dict={u : test_anomalous_data[0:16],isTrain : False})        \n",
    "    mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/anomlous_{}.png'.format(str(epoch).zfill(3)))\n",
    "    \n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
