{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum infoGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     21,
     25,
     29,
     44,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_infoGANs_for_FD_v2'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     123
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_epoch = 50\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.0\n",
    "c_size = 10\n",
    "\n",
    "\n",
    "def G(x,c,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "        x_concat = tf.concat([x,c],3)\n",
    "        conv1 = tf.layers.conv2d_transpose(x_concat,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "        #\n",
    "        \n",
    "        fc0  = tf.reshape(conv4, (-1, 4*4*512))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[4*4*512, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "        fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "        \n",
    "        \n",
    "    r5 = tf.nn.tanh(tf.layers.batch_normalization(conv5,training=isTrain), name = name)#4*4*512\n",
    "  \n",
    "  \n",
    "    return r5, tf.reshape(fc1,(-1,1,1,c_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "def Q_cat(x,reuse = False, name = 'Q_cat') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('Q_cat', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        fc0  = tf.reshape(x, (-1, 100))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[100, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "    fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "    \n",
    "    return tf.reshape(fc1, (-1,1,1,c_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "c = tf.placeholder(tf.float32,shape=(None,1,1,c_size),name = 'c')    #x_z = G(z,c)\n",
    "\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,c, isTrain,name='G_sample') # G(z)\n",
    "E_z, E_c = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z,E_c, isTrain, reuse=True, name ='re_image')\n",
    "re_z, re_c = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "\n",
    "\n",
    "re_z_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z - z)**2, axis=[1,2,3])) , name = 're_z_loss') \n",
    "re_c_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(re_c + 1e-8), axis = [1,2,3]),name = 're_c_loss')\n",
    "re_image_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_image - u)**2, axis=[1,2,3])) , name = 're_image_loss') \n",
    "\n",
    "\n",
    "E_loss = tf.add(re_z_loss, re_c_loss, name = 'E_loss')                       \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "Q_fake = Q_cat(D_enc(G_sample, isTrain,reuse=True), reuse=False, name='Q_fake')\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "Q_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(Q_fake + 1e-8), axis = [1,2,3]),name = 'Q_loss')\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "Q_vars = [var for var in T_vars if var.name.startswith('Q')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss + Q_loss, var_list=G_vars+Q_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    E_AE_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(re_image_loss, var_list=E_vars, name='E_AE_optim')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -133.884, D_real_e : 51.030, D_fake_e : 35.110, G_e : 35.550, Q_e : 2.037, new_measure : 62.543, k_curr : 0.479352\n",
      "D_e : 27.642, D_real_e : 39.803, D_fake_e : 27.174, G_e : 27.931, Q_e : 2.083, new_measure : 41.046, k_curr : 0.286127\n",
      "D_e : 27.246, D_real_e : 34.744, D_fake_e : 23.661, G_e : 24.330, Q_e : 2.028, new_measure : 36.046, k_curr : 0.259173\n",
      "D_e : 25.013, D_real_e : 29.956, D_fake_e : 20.443, G_e : 20.948, Q_e : 1.913, new_measure : 31.515, k_curr : 0.317504\n",
      "D_e : 23.061, D_real_e : 26.127, D_fake_e : 18.040, G_e : 18.349, Q_e : 1.762, new_measure : 27.538, k_curr : 0.150595\n",
      "D_e : 20.643, D_real_e : 22.924, D_fake_e : 15.825, G_e : 16.037, Q_e : 1.601, new_measure : 24.283, k_curr : 0.177497\n",
      "D_e : 19.770, D_real_e : 19.976, D_fake_e : 13.996, G_e : 14.035, Q_e : 1.463, new_measure : 21.600, k_curr : 0.030642\n",
      "D_e : 16.865, D_real_e : 17.353, D_fake_e : 12.020, G_e : 12.130, Q_e : 1.342, new_measure : 18.592, k_curr : 0.077323\n",
      "D_e : 15.235, D_real_e : 15.279, D_fake_e : 10.577, G_e : 10.666, Q_e : 1.236, new_measure : 16.692, k_curr : 0.157511\n",
      "D_e : 13.360, D_real_e : 13.594, D_fake_e : 9.484, G_e : 9.548, Q_e : 1.139, new_measure : 14.769, k_curr : 0.068026\n",
      "D_e : 11.722, D_real_e : 12.077, D_fake_e : 8.377, G_e : 8.455, Q_e : 1.053, new_measure : 13.042, k_curr : 0.063296\n",
      "D_e : 10.806, D_real_e : 10.926, D_fake_e : 7.601, G_e : 7.650, Q_e : 0.978, new_measure : 11.756, k_curr : 0.058826\n",
      "D_e : 9.922, D_real_e : 10.253, D_fake_e : 7.105, G_e : 7.165, Q_e : 0.912, new_measure : 11.054, k_curr : 0.092643\n",
      "D_e : 9.116, D_real_e : 9.250, D_fake_e : 6.429, G_e : 6.492, Q_e : 0.854, new_measure : 9.902, k_curr : 0.045929\n",
      "D_e : 8.396, D_real_e : 8.673, D_fake_e : 5.985, G_e : 6.073, Q_e : 0.802, new_measure : 9.214, k_curr : 0.040810\n",
      "D_e : 7.997, D_real_e : 8.225, D_fake_e : 5.677, G_e : 5.759, Q_e : 0.756, new_measure : 8.628, k_curr : 0.037694\n",
      "D_e : 7.575, D_real_e : 7.840, D_fake_e : 5.396, G_e : 5.483, Q_e : 0.715, new_measure : 8.188, k_curr : 0.051904\n",
      "D_e : 6.929, D_real_e : 7.180, D_fake_e : 4.922, G_e : 5.022, Q_e : 0.679, new_measure : 7.468, k_curr : 0.062442\n",
      "D_e : 6.328, D_real_e : 6.539, D_fake_e : 4.479, G_e : 4.580, Q_e : 0.646, new_measure : 6.767, k_curr : 0.053838\n",
      "D_e : 6.016, D_real_e : 6.215, D_fake_e : 4.249, G_e : 4.351, Q_e : 0.615, new_measure : 6.435, k_curr : 0.052012\n",
      "D_e : 5.820, D_real_e : 6.012, D_fake_e : 4.104, G_e : 4.208, Q_e : 0.588, new_measure : 6.251, k_curr : 0.054561\n",
      "D_e : 5.605, D_real_e : 5.817, D_fake_e : 3.952, G_e : 4.074, Q_e : 0.563, new_measure : 6.042, k_curr : 0.050002\n",
      "D_e : 5.491, D_real_e : 5.697, D_fake_e : 3.873, G_e : 3.989, Q_e : 0.540, new_measure : 5.914, k_curr : 0.047214\n",
      "D_e : 5.395, D_real_e : 5.592, D_fake_e : 3.804, G_e : 3.912, Q_e : 0.518, new_measure : 5.803, k_curr : 0.052607\n",
      "D_e : 5.288, D_real_e : 5.492, D_fake_e : 3.725, G_e : 3.839, Q_e : 0.499, new_measure : 5.686, k_curr : 0.066991\n",
      "D_e : 5.213, D_real_e : 5.391, D_fake_e : 3.672, G_e : 3.778, Q_e : 0.481, new_measure : 5.623, k_curr : 0.054289\n",
      "D_e : 5.149, D_real_e : 5.294, D_fake_e : 3.613, G_e : 3.714, Q_e : 0.463, new_measure : 5.509, k_curr : 0.031042\n",
      "D_e : 5.103, D_real_e : 5.242, D_fake_e : 3.558, G_e : 3.666, Q_e : 0.447, new_measure : 5.459, k_curr : 0.040224\n",
      "D_e : 5.036, D_real_e : 5.169, D_fake_e : 3.526, G_e : 3.617, Q_e : 0.432, new_measure : 5.388, k_curr : 0.043721\n",
      "D_e : 4.976, D_real_e : 5.101, D_fake_e : 3.478, G_e : 3.570, Q_e : 0.418, new_measure : 5.300, k_curr : 0.044837\n",
      "D_e : 4.936, D_real_e : 5.047, D_fake_e : 3.443, G_e : 3.536, Q_e : 0.405, new_measure : 5.244, k_curr : 0.035757\n",
      "D_e : 4.852, D_real_e : 4.980, D_fake_e : 3.386, G_e : 3.484, Q_e : 0.393, new_measure : 5.188, k_curr : 0.042041\n",
      "D_e : 4.816, D_real_e : 4.941, D_fake_e : 3.371, G_e : 3.462, Q_e : 0.381, new_measure : 5.142, k_curr : 0.032116\n",
      "D_e : 4.766, D_real_e : 4.900, D_fake_e : 3.335, G_e : 3.427, Q_e : 0.370, new_measure : 5.111, k_curr : 0.040610\n",
      "D_e : 4.711, D_real_e : 4.851, D_fake_e : 3.306, G_e : 3.397, Q_e : 0.360, new_measure : 5.043, k_curr : 0.037914\n",
      "D_e : 4.678, D_real_e : 4.799, D_fake_e : 3.260, G_e : 3.357, Q_e : 0.350, new_measure : 4.999, k_curr : 0.043174\n",
      "D_e : 4.636, D_real_e : 4.770, D_fake_e : 3.249, G_e : 3.341, Q_e : 0.341, new_measure : 4.962, k_curr : 0.036089\n",
      "D_e : 4.606, D_real_e : 4.728, D_fake_e : 3.213, G_e : 3.304, Q_e : 0.332, new_measure : 4.912, k_curr : 0.052694\n",
      "D_e : 4.554, D_real_e : 4.686, D_fake_e : 3.196, G_e : 3.283, Q_e : 0.324, new_measure : 4.863, k_curr : 0.046110\n",
      "D_e : 4.520, D_real_e : 4.652, D_fake_e : 3.156, G_e : 3.256, Q_e : 0.316, new_measure : 4.819, k_curr : 0.047227\n",
      "D_e : 4.469, D_real_e : 4.604, D_fake_e : 3.130, G_e : 3.224, Q_e : 0.308, new_measure : 4.773, k_curr : 0.042811\n",
      "D_e : 4.466, D_real_e : 4.587, D_fake_e : 3.127, G_e : 3.215, Q_e : 0.301, new_measure : 4.759, k_curr : 0.032464\n",
      "D_e : 4.436, D_real_e : 4.551, D_fake_e : 3.086, G_e : 3.180, Q_e : 0.294, new_measure : 4.730, k_curr : 0.048207\n",
      "D_e : 4.424, D_real_e : 4.535, D_fake_e : 3.081, G_e : 3.176, Q_e : 0.287, new_measure : 4.726, k_curr : 0.041898\n",
      "D_e : 4.370, D_real_e : 4.511, D_fake_e : 3.065, G_e : 3.158, Q_e : 0.281, new_measure : 4.680, k_curr : 0.040776\n",
      "D_e : 4.342, D_real_e : 4.472, D_fake_e : 3.036, G_e : 3.131, Q_e : 0.275, new_measure : 4.637, k_curr : 0.038315\n",
      "D_e : 4.328, D_real_e : 4.462, D_fake_e : 3.023, G_e : 3.118, Q_e : 0.269, new_measure : 4.649, k_curr : 0.052497\n",
      "D_e : 4.298, D_real_e : 4.430, D_fake_e : 3.014, G_e : 3.106, Q_e : 0.264, new_measure : 4.604, k_curr : 0.038462\n",
      "D_e : 4.277, D_real_e : 4.403, D_fake_e : 2.974, G_e : 3.081, Q_e : 0.258, new_measure : 4.560, k_curr : 0.041821\n",
      "D_e : 4.237, D_real_e : 4.359, D_fake_e : 2.962, G_e : 3.052, Q_e : 0.253, new_measure : 4.525, k_curr : 0.039375\n",
      "total time :  4015.1869723796844\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f381afabcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.set_random_seed(int(time.time()))\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "\n",
    "    \n",
    "    one_hot = np.eye(c_size)\n",
    "    temp2 = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4])\n",
    "    test_c = one_hot[temp2].reshape([-1,1,1,c_size])\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    Q_error=[]\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            temp1 = np.random.randint(0,10,(batch_size))                                                                                                                                     \n",
    "            c_ = one_hot[temp1].reshape([-1,1,1,c_size])\n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, c : c_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e,Q_e = sess.run([G_optim, G_loss,Q_loss], {u : u_, z : z_, c : c_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            Q_error.append(Q_e)\n",
    "\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, Q_e : %.3f, new_measure : %.3f, k_curr : %3f'\n",
    "              %(np.mean(D_error), np.mean(D_real_error),np.mean(D_fake_error), np.mean(G_error),\n",
    "                np.mean(Q_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, c : test_c, isTrain : False})       \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "    \n",
    "    \n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_infoGANs_for_FD_v2/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 're_z:0' shape=(?, 1, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20506831359863278,\n",
       " 0.41824233245849607,\n",
       " 0.63876511383056633,\n",
       " 0.86219695663452134,\n",
       " 1.089921775817871,\n",
       " 1.3218008880615233,\n",
       " 1.5554747238159177,\n",
       " 1.7911625442504879,\n",
       " 2.0299262123107908,\n",
       " 2.2696944427490231,\n",
       " 2.5107357063293452,\n",
       " 2.7528066368103024,\n",
       " 2.9938249816894529,\n",
       " 3.2359802398681636,\n",
       " 3.4768087921142574,\n",
       " 3.7182810134887689,\n",
       " 3.9571514282226556,\n",
       " 4.1959382591247554,\n",
       " 4.432125900268554,\n",
       " 4.6658260421752926,\n",
       " 4.8977559967041016,\n",
       " 5.1282884635925292,\n",
       " 5.358193096160889,\n",
       " 5.585798343658448,\n",
       " 5.8117207336425789,\n",
       " 6.0343067932128918,\n",
       " 6.2557720947265638,\n",
       " 6.4742338447570811,\n",
       " 6.691497535705567,\n",
       " 6.9064852943420414,\n",
       " 7.1193995399475103,\n",
       " 7.3327551918029794,\n",
       " 7.5383402442932139,\n",
       " 7.7445830001831064,\n",
       " 7.9481809310913096,\n",
       " 8.1498644371032718,\n",
       " 8.3494293594360354,\n",
       " 8.5459281768798832,\n",
       " 8.7425396957397457,\n",
       " 8.9353193855285635,\n",
       " 9.1276151313781728,\n",
       " 9.3155047378540026,\n",
       " 9.4985561828613267,\n",
       " 9.677119285583494,\n",
       " 9.8543619384765595,\n",
       " 10.028142906188961,\n",
       " 10.195638263702389,\n",
       " 10.352893779754634,\n",
       " 10.499606101989741,\n",
       " 10.632611923217768,\n",
       " 10.749498855590815,\n",
       " 10.868550441741938,\n",
       " 10.963765739440912,\n",
       " 11.050030899047846,\n",
       " 11.132557640075678,\n",
       " 11.202412914276117,\n",
       " 11.280245952606196,\n",
       " 11.373880561828608,\n",
       " 11.478951904296871,\n",
       " 11.585553157806393,\n",
       " 11.681228870391841,\n",
       " 11.763552764892573,\n",
       " 11.858127155303951,\n",
       " 11.934733093261714,\n",
       " 12.003535774230953,\n",
       " 12.06292273330688,\n",
       " 12.128556930541988,\n",
       " 12.173640811920162,\n",
       " 12.215100574493404,\n",
       " 12.257452388763424,\n",
       " 12.29596022033691,\n",
       " 12.313092460632321,\n",
       " 12.32161644363403,\n",
       " 12.334455921173092,\n",
       " 12.361866935729976,\n",
       " 12.382485244750972,\n",
       " 12.359214542388912,\n",
       " 12.331917198181149,\n",
       " 12.307616828918453,\n",
       " 12.301882331848141,\n",
       " 12.281565109252925,\n",
       " 12.257091194152828,\n",
       " 12.221805236816403,\n",
       " 12.189557239532467,\n",
       " 12.157816741943355,\n",
       " 12.137049823760982,\n",
       " 12.121771274566646,\n",
       " 12.1017190208435,\n",
       " 12.064066978454584,\n",
       " 12.032163436889642,\n",
       " 11.988724193572992,\n",
       " 11.942706783294671,\n",
       " 11.905083461761468,\n",
       " 11.866182460784906,\n",
       " 11.816013427734369,\n",
       " 11.7732198562622,\n",
       " 11.708872585296623,\n",
       " 11.664796398162833,\n",
       " 11.619154037475578,\n",
       " 11.556147087097161,\n",
       " 11.489109046936028,\n",
       " 11.408342453002922,\n",
       " 11.335818706512443,\n",
       " 11.25726276779174,\n",
       " 11.167179988861076,\n",
       " 11.076864990234366,\n",
       " 10.9813728942871,\n",
       " 10.90893095779418,\n",
       " 10.804488693237294,\n",
       " 10.704176479339589,\n",
       " 10.585761226654041,\n",
       " 10.486050167083729,\n",
       " 10.392566841125477,\n",
       " 10.285681144714344,\n",
       " 10.182735046386707,\n",
       " 10.065144474029529,\n",
       " 9.9638297882079954,\n",
       " 9.8443495254516478,\n",
       " 9.7034083786010612,\n",
       " 9.5759007644653185,\n",
       " 9.4559941520690778,\n",
       " 9.3181520805358744,\n",
       " 9.192538753509508,\n",
       " 9.0571823921203478,\n",
       " 8.9201845893859719,\n",
       " 8.7906968803405618,\n",
       " 8.6482716751098483,\n",
       " 8.5050761260986185,\n",
       " 8.3678301544189306,\n",
       " 8.2283598518371441,\n",
       " 8.0904820556640491,\n",
       " 7.9433396530151237,\n",
       " 7.7951523551940785,\n",
       " 7.6454867362975945,\n",
       " 7.4912751197814815,\n",
       " 7.3292539100646845,\n",
       " 7.180200485229479,\n",
       " 7.0290827713012565,\n",
       " 6.8756639442443719,\n",
       " 6.7112295761108269,\n",
       " 6.5602877159118522,\n",
       " 6.3976256179809443,\n",
       " 6.2425352706909054,\n",
       " 6.0888237266540406,\n",
       " 5.9192117691039918,\n",
       " 5.7561769752502316,\n",
       " 5.5886286201476922,\n",
       " 5.4185337944030634,\n",
       " 5.2519696693420279,\n",
       " 5.0924505615234246,\n",
       " 4.9404703674316277,\n",
       " 4.7786188583373894,\n",
       " 4.6120688362121456,\n",
       " 4.4397648696899283,\n",
       " 4.2800280799865593,\n",
       " 4.1079163055419796,\n",
       " 3.9355046768188351,\n",
       " 3.7525649147033566,\n",
       " 3.5827098503112667,\n",
       " 3.4018231697082393,\n",
       " 3.2282220993041864,\n",
       " 3.0660297126769889,\n",
       " 2.8933763656616081,\n",
       " 2.7127402877807487,\n",
       " 2.5448097381591666,\n",
       " 2.3893901672363151,\n",
       " 2.2158924713134636,\n",
       " 2.0435413360595573,\n",
       " 1.8894463272094597,\n",
       " 1.7094387245178093,\n",
       " 1.5397189445495476,\n",
       " 1.3603536911010612,\n",
       " 1.1997143745422232,\n",
       " 1.0246133041381704,\n",
       " 0.85540521240233058,\n",
       " 0.67510316467283837,\n",
       " 0.4899384918212758,\n",
       " 0.31623665618895158,\n",
       " 0.13871478271483048,\n",
       " -0.049802440643323842,\n",
       " -0.23935236740113636,\n",
       " -0.40443904113770862,\n",
       " -0.55766556549073598,\n",
       " -0.72385891723634144,\n",
       " -0.8761534194946422,\n",
       " -1.0201341361999645,\n",
       " -1.1640547714233531,\n",
       " -1.3115583915710582,\n",
       " -1.4516482353210582,\n",
       " -1.5960621109008923,\n",
       " -1.7342509307861462,\n",
       " -1.8675754699707166,\n",
       " -1.9918967819214002,\n",
       " -2.1124872436523572,\n",
       " -2.2218532714843886,\n",
       " -2.3076034889221329,\n",
       " -2.4069542770385879,\n",
       " -2.4783244132995743,\n",
       " -2.581626583099379,\n",
       " -2.627737262725844,\n",
       " -2.6879902000427385,\n",
       " -2.7261777534485003,\n",
       " -2.7874412765503069,\n",
       " -2.8307124786377091,\n",
       " -2.844882568359389,\n",
       " -2.8515779876709124,\n",
       " -2.8225721664428849,\n",
       " -2.8089231338501115,\n",
       " -2.7842674179077287,\n",
       " -2.722844726562514,\n",
       " -2.6445032997131488,\n",
       " -2.5441893424987931,\n",
       " -2.4515262451172015,\n",
       " -2.3658815116882463,\n",
       " -2.2778965682983539,\n",
       " -2.1729805564880511,\n",
       " -2.0544704780578753,\n",
       " -1.918916706085219,\n",
       " -1.7692789688110491,\n",
       " -1.6459799766540666,\n",
       " -1.5446754570007464,\n",
       " -1.4269247589111469,\n",
       " -1.3461703910827778,\n",
       " -1.2187057723999164,\n",
       " -1.1112520828247212,\n",
       " -0.99365774154664499,\n",
       " -0.89094513702393996,\n",
       " -0.77858570480348099,\n",
       " -0.67801967620851034,\n",
       " -0.52938839530946247,\n",
       " -0.39243094444276327,\n",
       " -0.25400059700013633,\n",
       " -0.11419498252870078,\n",
       " 0.035343309402451561,\n",
       " 0.16697562599180701,\n",
       " 0.16842822074888708,\n",
       " 0.29495705604551792,\n",
       " 0.39494463920591827,\n",
       " 0.47714983558653346,\n",
       " 0.57850098228453151,\n",
       " 0.67115334510801783,\n",
       " 0.76568628883360379,\n",
       " 0.848449949264512,\n",
       " 0.89725647926329133,\n",
       " 0.85708015632627965,\n",
       " 0.84477252006529324,\n",
       " 0.80079756736753926,\n",
       " 0.7767202625274513,\n",
       " 0.74531984901426762,\n",
       " 0.71837164878843751,\n",
       " 0.66098503684996091,\n",
       " 0.61824746894834959,\n",
       " 0.57746999168394531,\n",
       " 0.53956589317320314,\n",
       " 0.52927582359312497,\n",
       " 0.49515491294859371,\n",
       " 0.47412689399717767,\n",
       " 0.43086546134947257,\n",
       " 0.3924281330108495,\n",
       " 0.36684840202330066,\n",
       " 0.34924368095396474,\n",
       " 0.34477839469908189,\n",
       " 0.3097487621307225,\n",
       " 0.29837796974180647,\n",
       " 0.29057262611387674,\n",
       " 0.25949011039732395,\n",
       " 0.2529628238677829,\n",
       " 0.26947738456724579,\n",
       " 0.2837090587615817,\n",
       " 0.30447184562681606,\n",
       " 0.31312522697447226,\n",
       " 0.33224621772764606,\n",
       " 0.34499401283262654,\n",
       " 0.35790595436094685,\n",
       " 0.36278926277159135,\n",
       " 0.38874759101866163,\n",
       " 0.40422767829893502,\n",
       " 0.41891336631773379,\n",
       " 0.44172445487974549,\n",
       " 0.47935226249693297,\n",
       " 0.50517481422422783,\n",
       " 0.48175625038145437,\n",
       " 0.49808527183531176,\n",
       " 0.50934622764585857,\n",
       " 0.54259866142271407,\n",
       " 0.54891463279722574,\n",
       " 0.5397643833160245,\n",
       " 0.55168779945371982,\n",
       " 0.55933180427549711,\n",
       " 0.54978817558287008,\n",
       " 0.54196369361875874,\n",
       " 0.52807554054258687,\n",
       " 0.50121919822691308,\n",
       " 0.49635562324522364,\n",
       " 0.47600816917417871,\n",
       " 0.4897699680328213,\n",
       " 0.50679985237120018,\n",
       " 0.48527357292173723,\n",
       " 0.45235833549497939,\n",
       " 0.46690265464781139,\n",
       " 0.45435119438169808,\n",
       " 0.45546284675596566,\n",
       " 0.44555930519102421,\n",
       " 0.4342285480499109,\n",
       " 0.4307527675628503,\n",
       " 0.43744893455503775,\n",
       " 0.45512534904478386,\n",
       " 0.47137725639341665,\n",
       " 0.46429693412779166,\n",
       " 0.46383394432066272,\n",
       " 0.47977738761900257,\n",
       " 0.46695511054991073,\n",
       " 0.48164141654966658,\n",
       " 0.47241463279722518,\n",
       " 0.47312392616270366,\n",
       " 0.46867391777036965,\n",
       " 0.45991155052183447,\n",
       " 0.44570070075987156,\n",
       " 0.44680295753477389,\n",
       " 0.45433661079405119,\n",
       " 0.44033592414854333,\n",
       " 0.43351549720762533,\n",
       " 0.45921336936949053,\n",
       " 0.45090962409971513,\n",
       " 0.4663237628936604,\n",
       " 0.46428618049619941,\n",
       " 0.4966551074981525,\n",
       " 0.49240592002867006,\n",
       " 0.47207383918760559,\n",
       " 0.47774431419370911,\n",
       " 0.47278676033018369,\n",
       " 0.47281562614439265,\n",
       " 0.47189748573301571,\n",
       " 0.47071605491636526,\n",
       " 0.46423404121397266,\n",
       " 0.45621925544737107,\n",
       " 0.44213656044004684,\n",
       " 0.45206531715391401,\n",
       " 0.42449616050718547,\n",
       " 0.41189903068540812,\n",
       " 0.31238049888609171,\n",
       " 0.29527581977842565,\n",
       " 0.28128819084165807,\n",
       " 0.25479161643980258,\n",
       " 0.2520356082916092,\n",
       " 0.26497789192198029,\n",
       " 0.24277283287046658,\n",
       " 0.24201030921934352,\n",
       " 0.26425449943540796,\n",
       " 0.27275067329405051,\n",
       " 0.28425147056577904,\n",
       " 0.30999646186826924,\n",
       " 0.3388903713226149,\n",
       " 0.36686146354673599,\n",
       " 0.37825100517271254,\n",
       " 0.34041917991636483,\n",
       " 0.33053614234922613,\n",
       " 0.32912052726743896,\n",
       " 0.31762293815611081,\n",
       " 0.32218544960020257,\n",
       " 0.30817056846616936,\n",
       " 0.31504859733579826,\n",
       " 0.33942479133604242,\n",
       " 0.34877840232847407,\n",
       " 0.35608720207212641,\n",
       " 0.37232812690733147,\n",
       " 0.36549944877622792,\n",
       " 0.38486979484556383,\n",
       " 0.38516485404966538,\n",
       " 0.39467632484434312,\n",
       " 0.41554338645933331,\n",
       " 0.41974246406553445,\n",
       " 0.44613596534727273,\n",
       " 0.45133384895322975,\n",
       " 0.46691910743711645,\n",
       " 0.47847609138487035,\n",
       " 0.48556809043882537,\n",
       " 0.49664152717588594,\n",
       " 0.50737086677549526,\n",
       " 0.53215882301328821,\n",
       " 0.5237150745391671,\n",
       " 0.50785792732237023,\n",
       " 0.49132439613340534,\n",
       " 0.50706599617002646,\n",
       " 0.4959495449065987,\n",
       " 0.50249265098570028,\n",
       " 0.47949707221983112,\n",
       " 0.48226543998716509,\n",
       " 0.48790175437925493,\n",
       " 0.47341873359678421,\n",
       " 0.46131383705137402,\n",
       " 0.44722620201109081,\n",
       " 0.42088778114317088,\n",
       " 0.4155847988128486,\n",
       " 0.43426158332822945,\n",
       " 0.44016607093809273,\n",
       " 0.45758278465269231,\n",
       " 0.46884267616270209,\n",
       " 0.47350547599790715,\n",
       " 0.49019027137754578,\n",
       " 0.5107112407684149,\n",
       " 0.53357103538511408,\n",
       " 0.55083272361753599,\n",
       " 0.55657981681821955,\n",
       " 0.56084113121030943,\n",
       " 0.58389793586729188,\n",
       " 0.5838208370208563,\n",
       " 0.61129313468931334,\n",
       " 0.60082149314878597,\n",
       " 0.60036591148374685,\n",
       " 0.5983515605926335,\n",
       " 0.59449685478208658,\n",
       " 0.55819571876524088,\n",
       " 0.55580351066587563,\n",
       " 0.55335465812681317,\n",
       " 0.5566947498321354,\n",
       " 0.52057539176939127,\n",
       " 0.51365933418272136,\n",
       " 0.48745451545713542,\n",
       " 0.47900382423399085,\n",
       " 0.45430843925474279,\n",
       " 0.47089768028257478,\n",
       " 0.46042568778989901,\n",
       " 0.45070543098447907,\n",
       " 0.45782006263731112,\n",
       " 0.47131092643735994,\n",
       " 0.46830210304258457,\n",
       " 0.48498121070860017,\n",
       " 0.48904460334776029,\n",
       " 0.49842668342588525,\n",
       " 0.50925581550596333,\n",
       " 0.51006301307676405,\n",
       " 0.52184573173521132,\n",
       " 0.54401933860776985,\n",
       " 0.55499347877500615,\n",
       " 0.57368221473692016,\n",
       " 0.57261055183408816,\n",
       " 0.58711244392393191,\n",
       " 0.5823062419891174,\n",
       " 0.56496052742002556,\n",
       " 0.56540470695493761,\n",
       " 0.56000446128843373,\n",
       " 0.56021322822568953,\n",
       " 0.54542026329038684,\n",
       " 0.53965299415586532,\n",
       " 0.53518517875669536,\n",
       " 0.51767135429380473,\n",
       " 0.46668847084043558,\n",
       " 0.45443595695493749,\n",
       " 0.46244454765317966,\n",
       " 0.4545713291168027,\n",
       " 0.40655766487119721,\n",
       " 0.39285083198545501,\n",
       " 0.37708823585508389,\n",
       " 0.36351381111143155,\n",
       " 0.33803886222837493,\n",
       " 0.33107419395444915,\n",
       " 0.33958069419858977,\n",
       " 0.34708901405332609,\n",
       " 0.3718008823394589,\n",
       " 0.37290927314756439,\n",
       " 0.39355090141294524,\n",
       " 0.38906763648984954,\n",
       " 0.40200702857969328,\n",
       " 0.40908675956724211,\n",
       " 0.42300783729551356,\n",
       " 0.42815795707700771,\n",
       " 0.42491576576231044,\n",
       " 0.43435515785215417,\n",
       " 0.4463767719268612,\n",
       " 0.42761244773862878,\n",
       " 0.43550424766538659,\n",
       " 0.44440369606016195,\n",
       " 0.43054368019102129,\n",
       " 0.43452746391294511,\n",
       " 0.44534655952451735,\n",
       " 0.44684488868711497,\n",
       " 0.4376622447967341,\n",
       " 0.45143533897398019,\n",
       " 0.45783599281309151,\n",
       " 0.45050981712339422,\n",
       " 0.46366261100767153,\n",
       " 0.45198287391660708,\n",
       " 0.46594745826719297,\n",
       " 0.45406001091001519,\n",
       " 0.45791530799863822,\n",
       " 0.44489779472349172,\n",
       " 0.45776029777524951,\n",
       " 0.45197323036191944,\n",
       " 0.48440370368955615,\n",
       " 0.48401718330381394,\n",
       " 0.49629858970640178,\n",
       " 0.49743771171567913,\n",
       " 0.50181067466733931,\n",
       " 0.49664311027524943,\n",
       " 0.49456453895566932,\n",
       " 0.47428422737119663,\n",
       " 0.47597525215147002,\n",
       " 0.46417940330503443,\n",
       " 0.438736612319927,\n",
       " 0.44194337272642115,\n",
       " 0.41703489494321799,\n",
       " 0.41330263710020038,\n",
       " 0.4115282993316457,\n",
       " 0.40911783409116714,\n",
       " 0.40432436180112807,\n",
       " 0.41667119026182142,\n",
       " 0.40601323890684093,\n",
       " 0.39647816276548348,\n",
       " 0.39155722999570808,\n",
       " 0.36397573661802252,\n",
       " 0.35465685081479986,\n",
       " 0.35373314094541508,\n",
       " 0.35583686256406744,\n",
       " 0.34393420219419435,\n",
       " 0.35886104774473143,\n",
       " 0.35707539558408685,\n",
       " 0.3660187931060595,\n",
       " 0.3971105670928759,\n",
       " 0.40211322593686999,\n",
       " 0.3959969806670946,\n",
       " 0.39432694435117666,\n",
       " 0.42019596290586414,\n",
       " 0.44579964256284654,\n",
       " 0.45306392860410627,\n",
       " 0.44746619224546369,\n",
       " 0.4466450443267625,\n",
       " 0.44185526466367653,\n",
       " 0.44984406852720188,\n",
       " 0.44280292320249481,\n",
       " 0.45227002525327603,\n",
       " 0.46658873939512174,\n",
       " 0.43842472267148891,\n",
       " 0.42420680427549279,\n",
       " 0.41425322151182092,\n",
       " 0.41248831748960413,\n",
       " 0.41648758506772909,\n",
       " 0.43199795722959433,\n",
       " 0.43293951225278771,\n",
       " 0.44603520774839317,\n",
       " 0.4421812763213912,\n",
       " 0.42895214653013142,\n",
       " 0.4187812480926314,\n",
       " 0.44017543220518018,\n",
       " 0.45503400230405716,\n",
       " 0.44987420463560013,\n",
       " 0.46063373374936967,\n",
       " 0.44309707832334427,\n",
       " 0.43227712821958453,\n",
       " 0.39706317329404739,\n",
       " 0.39436984443662554,\n",
       " 0.40165322685239702,\n",
       " 0.38422751426694779,\n",
       " 0.38296515083310989,\n",
       " 0.38976419639585402,\n",
       " 0.37435105323789503,\n",
       " 0.35728098106382278,\n",
       " 0.34940530586240676,\n",
       " 0.3062537670135298,\n",
       " 0.28612712287900832,\n",
       " 0.2957058811187544,\n",
       " 0.26571926307676219,\n",
       " 0.23585906791685007,\n",
       " 0.23466278266904733,\n",
       " 0.22346692085264105,\n",
       " 0.24361673927305119,\n",
       " 0.26102464866636171,\n",
       " 0.28452982521055115,\n",
       " 0.31042292213437928,\n",
       " 0.32736765861509215,\n",
       " 0.33220818519590267,\n",
       " 0.3438997097015179,\n",
       " 0.31924725532529719,\n",
       " 0.32311050605771907,\n",
       " 0.32420961570737727,\n",
       " 0.33934510231016046,\n",
       " 0.35259723091123468,\n",
       " 0.36770364189145932,\n",
       " 0.37829362297056091,\n",
       " 0.38026955223081482,\n",
       " 0.39732610893247494,\n",
       " 0.4189946231841839,\n",
       " 0.41192604637143976,\n",
       " 0.40239615058896905,\n",
       " 0.42173464775083425,\n",
       " 0.43644598197934986,\n",
       " 0.43269312095640067,\n",
       " 0.44205769157407643,\n",
       " 0.43620377159116624,\n",
       " 0.43030976295469164,\n",
       " 0.40489940452573653,\n",
       " 0.40685160255430097,\n",
       " 0.39524172019956461,\n",
       " 0.38388116264341227,\n",
       " 0.37554184150693765,\n",
       " 0.37210197639463294,\n",
       " 0.37473411750791419,\n",
       " 0.35322023582456458,\n",
       " 0.35167852210996492,\n",
       " 0.35459917259214263,\n",
       " 0.33913919639585355,\n",
       " 0.334708166122416,\n",
       " 0.33217838096616598,\n",
       " 0.33082599067685931,\n",
       " 0.31833221244809951,\n",
       " 0.31636959266660536,\n",
       " 0.33049907493589242,\n",
       " 0.32956841468808967,\n",
       " 0.32905296897886116,\n",
       " 0.33766244697568731,\n",
       " 0.32634819984433966,\n",
       " 0.3387551097869666,\n",
       " 0.35938340187070683,\n",
       " 0.34624946784971072,\n",
       " 0.34128441047666386,\n",
       " 0.33937426567075563,\n",
       " 0.33778631401059939,\n",
       " 0.34948456764219116,\n",
       " 0.34467858695981812,\n",
       " 0.3628934230804236,\n",
       " 0.35930990791318729,\n",
       " 0.371161581039408,\n",
       " 0.35615495491025762,\n",
       " 0.35369265556333379,\n",
       " 0.35010041999814823,\n",
       " 0.36078523826597048,\n",
       " 0.41339054298398803,\n",
       " 0.40358690834043331,\n",
       " 0.40977112007139033,\n",
       " 0.40932034492490593,\n",
       " 0.37452395820615592,\n",
       " 0.38779477500913445,\n",
       " 0.39923636436460319,\n",
       " 0.41405763435361681,\n",
       " 0.40089756965635115,\n",
       " 0.40188225364682967,\n",
       " 0.39487526893613628,\n",
       " 0.40341830253598981,\n",
       " 0.41319865608213235,\n",
       " 0.41386467933652688,\n",
       " 0.42270963096616554,\n",
       " 0.40883589744565768,\n",
       " 0.37247815132139012,\n",
       " 0.37701655006406587,\n",
       " 0.35626040840146816,\n",
       " 0.36777746772764003,\n",
       " 0.35324423789975917,\n",
       " 0.35056800270078453,\n",
       " 0.34484431648252284,\n",
       " 0.33972803688047204,\n",
       " 0.28043643760679038,\n",
       " 0.28280959510801107,\n",
       " 0.25757145881650712,\n",
       " 0.22525238990781571,\n",
       " 0.22967656517026686,\n",
       " 0.2464369983672883,\n",
       " 0.27108913612363594,\n",
       " 0.26050354194638986,\n",
       " 0.25903938484189765,\n",
       " 0.27716762351987612,\n",
       " 0.29633785438535459,\n",
       " 0.31298027229306946,\n",
       " 0.32333056831357726,\n",
       " 0.32025553703305965,\n",
       " 0.34490579795835263,\n",
       " 0.35684652519223936,\n",
       " 0.37060837745664366,\n",
       " 0.36031907081601866,\n",
       " 0.34776362800596006,\n",
       " 0.37606109046933894,\n",
       " 0.35292916679380182,\n",
       " 0.36216812705991508,\n",
       " 0.35423036766050098,\n",
       " 0.3739105701446318,\n",
       " 0.3806755199432158,\n",
       " 0.38369270515439741,\n",
       " 0.39828528022763959,\n",
       " 0.39543348884580365,\n",
       " 0.37501971244809856,\n",
       " 0.39219417762754194,\n",
       " 0.38375064659116498,\n",
       " 0.39770673561094039,\n",
       " 0.40094492149350874,\n",
       " 0.41371799278257121,\n",
       " 0.40847832298276648,\n",
       " 0.41092336845395788,\n",
       " 0.43230974769590125,\n",
       " 0.44502051353452426,\n",
       " 0.44916068458554964,\n",
       " 0.4421753368377469,\n",
       " 0.45257448387143828,\n",
       " 0.43141722297666285,\n",
       " 0.40696287345884058,\n",
       " 0.40073027229306907,\n",
       " 0.33021219825742454,\n",
       " 0.28409855079648699,\n",
       " 0.23525422477719987,\n",
       " 0.22784562492368424,\n",
       " 0.20940849494931899,\n",
       " 0.21624733543393812,\n",
       " 0.23176317405698499,\n",
       " 0.22081027030942638,\n",
       " 0.22211116600034431,\n",
       " 0.22137950325010014,\n",
       " 0.23896502494809818,\n",
       " 0.2375413684844751,\n",
       " 0.25295547676084229,\n",
       " 0.25438225746152587,\n",
       " 0.25886855506894774,\n",
       " 0.2618281536102075,\n",
       " 0.24179618263242428,\n",
       " 0.24312698554990472,\n",
       " 0.26353238868711171,\n",
       " 0.27028635215757069,\n",
       " 0.27965022468564688,\n",
       " 0.29134650611875235,\n",
       " 0.27039251899717032,\n",
       " 0.25679725074765858,\n",
       " 0.2593878078460472,\n",
       " 0.28528459358213115,\n",
       " 0.29647477912900616,\n",
       " 0.28603975486753153,\n",
       " 0.29503252601621316,\n",
       " 0.29299148368833228,\n",
       " 0.29477800178525609,\n",
       " 0.2991806125640647,\n",
       " 0.30763943672177951,\n",
       " 0.32497804450986545,\n",
       " 0.31720519065854708,\n",
       " 0.33185358238217988,\n",
       " 0.31262946128842983,\n",
       " 0.31027590751645717,\n",
       " 0.32299047279355675,\n",
       " 0.3256416110992208,\n",
       " 0.32939969444272665,\n",
       " 0.33203669166562705,\n",
       " 0.3458729076385274,\n",
       " 0.34311379814145709,\n",
       " 0.36122943687436726,\n",
       " 0.35618718910215047,\n",
       " 0.36728700065610553,\n",
       " 0.37658588981626179,\n",
       " 0.31293729591367392,\n",
       " 0.31537744331357626,\n",
       " 0.29511941337583208,\n",
       " 0.28432826042173048,\n",
       " 0.27173418617246287,\n",
       " 0.27521892356870309,\n",
       " 0.27760499000547068,\n",
       " 0.2752922878265156,\n",
       " 0.25993700981137885,\n",
       " 0.26593918037412301,\n",
       " 0.27277928733823431,\n",
       " 0.26781218338010443,\n",
       " 0.27852973937986031,\n",
       " 0.28486094284055363,\n",
       " 0.3098074569701923,\n",
       " 0.3300294151305927,\n",
       " 0.31777146530149109,\n",
       " 0.32372013092038754,\n",
       " 0.34261243438718442,\n",
       " 0.36702276992795591,\n",
       " 0.35808307647702814,\n",
       " 0.36478908157346368,\n",
       " 0.38082313919065114,\n",
       " 0.35409926223752614,\n",
       " 0.35380253982541676,\n",
       " 0.35779759597776051,\n",
       " 0.34289575958249685,\n",
       " 0.35610374832151054,\n",
       " 0.32822204208371758,\n",
       " 0.33300067901609059,\n",
       " 0.34468384933469409,\n",
       " 0.35540850448606126,\n",
       " 0.36257199096677417,\n",
       " 0.35555358886716482,\n",
       " 0.36274401473996754,\n",
       " 0.36422195434568044,\n",
       " 0.38209596633908866,\n",
       " 0.38513860702512381,\n",
       " 0.38359788513181325,\n",
       " 0.39695920562741871,\n",
       " 0.40119599533078781,\n",
       " 0.39296016311643234,\n",
       " 0.41234416198728191,\n",
       " 0.39695993232724769,\n",
       " 0.38548085212705235,\n",
       " 0.38387263298032381,\n",
       " 0.35855614280698395,\n",
       " 0.34474620246884913,\n",
       " 0.34667737388608544,\n",
       " 0.33326046562192529,\n",
       " 0.32570575523374168,\n",
       " 0.31528190612790669,\n",
       " 0.3195174789428481,\n",
       " 0.32030333709714498,\n",
       " 0.31653039169309227,\n",
       " 0.32702586746213524,\n",
       " 0.31530056381223287,\n",
       " 0.30967516708371723,\n",
       " 0.32195996475217425,\n",
       " 0.33875575256345353,\n",
       " 0.33574141311643202,\n",
       " 0.33500345993039687,\n",
       " 0.34306340408322888,\n",
       " 0.21336964797971325,\n",
       " 0.18534218597409799,\n",
       " 0.12600955963132454,\n",
       " 0.10341767883298468,\n",
       " 0.10921866226193976,\n",
       " 0.10203121376035283,\n",
       " 0.097110303878761031,\n",
       " 0.11267917442319461,\n",
       " 0.12287411499021121,\n",
       " 0.14920426559445923,\n",
       " 0.17227012252805296,\n",
       " 0.15535269927976192,\n",
       " 0.15494181442258417,\n",
       " 0.15233334541318475,\n",
       " 0.15784997749326285,\n",
       " 0.11016742897031362,\n",
       " 0.091227830886817518,\n",
       " 0.12349628639218858,\n",
       " 0.12606990623471787,\n",
       " 0.15634987068173933,\n",
       " 0.12093944740293074,\n",
       " 0.13957298469541121,\n",
       " 0.14052152061460066,\n",
       " 0.15427131843564557,\n",
       " 0.13393320846555276,\n",
       " 0.14713510894773046,\n",
       " 0.15747423171994723,\n",
       " 0.16667472457883392,\n",
       " 0.16077387809751068,\n",
       " 0.17629938316342864,\n",
       " 0.19836477851865322,\n",
       " 0.23221819877622157,\n",
       " 0.22532698822019129,\n",
       " 0.24694056320188071,\n",
       " 0.25917292785642171,\n",
       " 0.29358308029172442,\n",
       " 0.30892570114133378,\n",
       " 0.29736517143247143,\n",
       " 0.25785791587827223,\n",
       " 0.27904166030881422,\n",
       " 0.28756827163693921,\n",
       " 0.31491706466672431,\n",
       " 0.32099424743649968,\n",
       " 0.3299922370910407,\n",
       " 0.34142494583127508,\n",
       " 0.34468569564816959,\n",
       " 0.35689450645444398,\n",
       " 0.36020991706845767,\n",
       " 0.35133317375180728,\n",
       " 0.35759499931333072,\n",
       " 0.35399235343930729,\n",
       " 0.3467951889037848,\n",
       " 0.27625977897641663,\n",
       " 0.271089891433692,\n",
       " 0.25998003387448787,\n",
       " 0.2378493118285894,\n",
       " 0.24057541275022024,\n",
       " 0.24585373687741749,\n",
       " 0.22406917572019092,\n",
       " 0.21410377502439012,\n",
       " 0.22718030929563035,\n",
       " 0.068463069915747521,\n",
       " 0.061256881713843206,\n",
       " 0.058664333343481853,\n",
       " 0.025390407562231855,\n",
       " 0.033439445495581435,\n",
       " 0.039527599334692756,\n",
       " 0.070433486938452514,\n",
       " 0.0888678245544193,\n",
       " 0.099112997055029642,\n",
       " 0.10738896179196809,\n",
       " 0.10598953628537627,\n",
       " 0.12538754272458524,\n",
       " 0.1440410995483157,\n",
       " 0.11278302001950707,\n",
       " 0.13609345817563498,\n",
       " 0.15642144966123067,\n",
       " 0.16166904449460467,\n",
       " 0.17995859909055192,\n",
       " 0.20383400917050798,\n",
       " 0.24520913124082047,\n",
       " 0.31297898674008806,\n",
       " 0.32223339271542983,\n",
       " 0.31709593009946302,\n",
       " 0.33894446563718272,\n",
       " 0.3489423770904298,\n",
       " 0.36879450988767104,\n",
       " 0.38059041595456555,\n",
       " 0.39416250228879407,\n",
       " 0.41541909599301768,\n",
       " 0.43567538452146004,\n",
       " 0.46599144744870613,\n",
       " 0.44930883979794928,\n",
       " 0.43603983497617194,\n",
       " 0.42088735961911622,\n",
       " 0.43502914237973633,\n",
       " 0.42528579711911618,\n",
       " 0.43770822906491696,\n",
       " 0.42929481887814941,\n",
       " 0.41446687698361812,\n",
       " 0.406929191589331,\n",
       " 0.3699080677032226,\n",
       " 0.35811718940732412,\n",
       " 0.3585752010345214,\n",
       " 0.36259243965146476,\n",
       " 0.37046753311154773,\n",
       " 0.35123152160642079,\n",
       " 0.34729387664792472,\n",
       " 0.33702776527402334,\n",
       " 0.34076463317868644,\n",
       " 0.34773163986203603,\n",
       " 0.34255020904538563,\n",
       " 0.34481748390195299,\n",
       " 0.33801369667050768,\n",
       " 0.29800442504880353,\n",
       " 0.29163241004941387,\n",
       " 0.30454778289792461,\n",
       " 0.27820409202573221,\n",
       " 0.24716303253171365,\n",
       " 0.22545560836789527,\n",
       " 0.21799070167539036,\n",
       " 0.24447545814511693,\n",
       " 0.23175042724606904,\n",
       " 0.20579554748532683,\n",
       " 0.20487641143796353,\n",
       " 0.1879586906432858,\n",
       " 0.1877969551086178,\n",
       " 0.17681967735288048,\n",
       " 0.17918039512631798,\n",
       " 0.17492252349851034,\n",
       " 0.18748689651486777,\n",
       " 0.20719374847409627,\n",
       " 0.22591911315915483,\n",
       " 0.15960855865476029,\n",
       " 0.15231743240353959,\n",
       " 0.15270579528806105,\n",
       " 0.16890044784543409,\n",
       " 0.16973664474484815,\n",
       " 0.1823471965789546,\n",
       " 0.19595712471005811,\n",
       " 0.22549714469907176,\n",
       " 0.24400857543942819,\n",
       " 0.2443213081359614,\n",
       " 0.25981112289426217,\n",
       " 0.25321123504636178,\n",
       " 0.26713354110715276,\n",
       " 0.25675053596494085,\n",
       " 0.27276650428769478,\n",
       " 0.25736395835873971,\n",
       " 0.24520726013181096,\n",
       " 0.24211652946469669,\n",
       " 0.20098383522031191,\n",
       " 0.20255541229245544,\n",
       " 0.18480672073361751,\n",
       " 0.17970040702817316,\n",
       " 0.18469629669186946,\n",
       " 0.20660446739194269,\n",
       " 0.21832537078854911,\n",
       " 0.20473028945920338,\n",
       " 0.24491012954709399,\n",
       " 0.20947017478940355,\n",
       " 0.20071781158444749,\n",
       " 0.19960478401181564,\n",
       " 0.18024647903439861,\n",
       " 0.18399069213864666,\n",
       " 0.20167587852475505,\n",
       " 0.1993692245483146,\n",
       " 0.19779061317441321,\n",
       " 0.21476888656613685,\n",
       " 0.23589032745358801,\n",
       " 0.24574589729306553,\n",
       " 0.27856230926511144,\n",
       " 0.29247239685056065,\n",
       " 0.28736503410336828,\n",
       " 0.29094862556454987,\n",
       " 0.30053168869016023,\n",
       " 0.31185124778745021,\n",
       " 0.32371391868588772,\n",
       " 0.28497078132626857,\n",
       " 0.29162011337277738,\n",
       " 0.27043870353696192,\n",
       " 0.26564597511288968,\n",
       " 0.23722864150998438,\n",
       " 0.22217725563046775,\n",
       " 0.22955926704404198,\n",
       " 0.25582213401791892,\n",
       " 0.26758006858823141,\n",
       " 0.28368101882932029,\n",
       " 0.28687145233151756,\n",
       " 0.29583369636533102,\n",
       " 0.31091743850705467,\n",
       " 0.32209141540524799,\n",
       " 0.3306869449615224,\n",
       " 0.35766816711423233,\n",
       " 0.34462755203244522,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "stra = \"1 2 3 4 5 6 7 8\"   \n",
    "print(type(stra))                    # type is str  \n",
    "list_str =                # str을 공란()으로 쪼갬   \n",
    "print(type(list_str))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6', '7', '8']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [int (i) for i in stra.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
