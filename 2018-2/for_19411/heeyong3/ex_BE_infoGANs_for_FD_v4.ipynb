{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44
    ]
   },
   "source": [
    "# Boundary Equilibrimum infoGANs for Fault Detection example\n",
    "\n",
    "## 초기 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     21,
     25,
     29,
     33,
     44,
     63
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_data :  (5139, 64, 64, 1)\n",
      "test_anomalous_data :  (4861, 64, 64, 1)\n",
      "train_normal_data :  (28038, 64, 64, 1)\n",
      "train_anomalous_data :  (26962, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "file_dir = 'anoGANs_MNIST_data/'\n",
    "\n",
    "with gzip.open(file_dir + 'test_normal_data.pickle.gzip','rb') as f :\n",
    "    test_normal_data = pickle.load(f)\n",
    "    print('test_normal_data : ' ,test_normal_data.shape)\n",
    "\n",
    "with gzip.open(file_dir + 'test_anomalous_data.pickle.gzip','rb') as f :\n",
    "    test_anomalous_data = pickle.load(f)\n",
    "    print('test_anomalous_data : ',test_anomalous_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_normal_data.pickle.gzip','rb') as f :\n",
    "    train_normal_data = pickle.load(f)\n",
    "    print('train_normal_data : ', train_normal_data.shape)\n",
    "    \n",
    "with gzip.open(file_dir + 'train_anomalous_data.pickle.gzip','rb') as f :\n",
    "    train_anomalous_data = pickle.load(f)\n",
    "    print('train_anomalous_data : ',train_anomalous_data.shape )\n",
    "\n",
    "def idx_shuffle(x) : \n",
    "    l = x.shape[0]\n",
    "    idx = np.arange(l)\n",
    "    np.random.shuffle(idx)\n",
    "    shuffled_x = np.empty(x.shape)\n",
    "\n",
    "    for i in range(l):\n",
    "        shuffled_x[idx[i]] = x[i]\n",
    "    \n",
    "    return shuffled_x\n",
    "\n",
    "def mnist_4by4_save(samples,path):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)    \n",
    "    gs.update(wspace=0.05, hspace=0.05) #이미지 사이간격 조절\n",
    "  \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')    \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "   \n",
    "        plt.imshow(sample.reshape(64, 64), cmap='Greys_r',clim=(0.0,1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n",
    "    return None\n",
    "\n",
    "def gan_loss_graph_save(G_loss,D_loss,path):\n",
    "    x1 = range(len(G_loss))\n",
    "    x2 = range(len(D_loss))\n",
    "      \n",
    "    y1 = G_loss\n",
    "    y2 = D_loss\n",
    "  \n",
    "      \n",
    "    plt.plot(x1,y1,label='G_loss') \n",
    "    plt.plot(x2,y2,label='D_loss') \n",
    "  \n",
    "    plt.xlabel('weight per update')\n",
    "    plt.ylabel('loss')             \n",
    "    plt.legend(loc=4)              \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(path)              \n",
    "\n",
    "    return None\n",
    "\n",
    "file_name = 'ex_BE_infoGANs_for_FD_v4'\n",
    "\n",
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 정의\n",
    "\n",
    "D부분을 encoder와 discriminator로 나눈 이유는 encoder를 나중에 feature map으로 쓰기 위해서 편의상 나누어서 정의함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     123
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_epoch = 50\n",
    "batch_size = 100\n",
    "z_size = 100\n",
    "lam = 0.01\n",
    "gamma = 0.7\n",
    "k_curr = 0.0\n",
    "c_size = 10\n",
    "\n",
    "\n",
    "def G(x,c,isTrain = True, reuse = False, name = 'G') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('G',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 1, 1, 100)\n",
    "        x_concat = tf.concat([x,c],3)\n",
    "        conv1 = tf.layers.conv2d_transpose(x_concat,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(tf.layers.batch_normalization(conv1,training=isTrain))#4*4*512\n",
    "        \n",
    "        conv2 = tf.layers.conv2d_transpose(r1,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#8*8*256\n",
    "                \n",
    "        conv3 = tf.layers.conv2d_transpose(r2,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#16*16*128\n",
    "\n",
    "        conv4 = tf.layers.conv2d_transpose(r3,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#32*32*64\n",
    "\n",
    "        conv5 = tf.layers.conv2d_transpose(r4,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r5= tf.nn.tanh(conv5,name=name)#64*64*1\n",
    "  \n",
    "    return r5\n",
    "\n",
    "def E(x,isTrain = True, reuse = False, name = 'E') : #input = (minibatch * w * h * ch)\n",
    "    \n",
    "    # out size = (in size + 2*padding - kenel)/strides + 1    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "    with tf.variable_scope('E',reuse=reuse)  :\n",
    "        \n",
    "        #x = (-1, 64, 64, 1)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[7,7], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(r1,128,[7,7], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "                \n",
    "        conv3 = tf.layers.conv2d(r2,256,[7,7], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "\n",
    "        conv4 = tf.layers.conv2d(r3,512,[7,7], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain))#4*4*512\n",
    "\n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #1*1*100\n",
    "        \n",
    "        #\n",
    "        \n",
    "        fc0  = tf.reshape(conv4, (-1, 4*4*512))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[4*4*512, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "        fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "        \n",
    "        \n",
    "    r5 = tf.nn.tanh(tf.layers.batch_normalization(conv5,training=isTrain), name = name)#4*4*512\n",
    "  \n",
    "  \n",
    "    return r5, tf.reshape(fc1,(-1,1,1,c_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def D_enc(x,isTrain=True,reuse = False, name = 'D_enc') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_enc', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "\n",
    "        conv1 = tf.layers.conv2d(x,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init) \n",
    "        r1 = tf.nn.elu(conv1)#32*32*64\n",
    "\n",
    "   \n",
    "        conv2 = tf.layers.conv2d(r1,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r2 = tf.nn.elu(tf.layers.batch_normalization(conv2,training=isTrain))#16*16*128\n",
    "\n",
    "  \n",
    "        conv3 = tf.layers.conv2d(r2,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r3 = tf.nn.elu(tf.layers.batch_normalization(conv3,training=isTrain))#8*8*256\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(r3,512,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r4 = tf.nn.elu(tf.layers.batch_normalization(conv4,training=isTrain), name = name)#4*4*512\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(r4,100,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)    \n",
    "        r5 = tf.layers.batch_normalization(conv5,training=isTrain)\n",
    "    return tf.add(r5,0,name=name)\n",
    "\n",
    "def D_dec(x,isTrain=True,reuse = False, name = 'D_dec') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('D_dec', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        # 256*16*16\n",
    "        # 128*32*32\n",
    "        # 1*64*64\n",
    "        conv6 = tf.layers.conv2d_transpose(x,512,[4,4], strides=(1,1),padding = 'valid',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r6 = tf.nn.elu(tf.layers.batch_normalization(conv6,training=isTrain))#4*4*256\n",
    "        \n",
    "        conv7 = tf.layers.conv2d_transpose(r6,256,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r7 = tf.nn.elu(tf.layers.batch_normalization(conv7,training=isTrain))#8*8*256\n",
    "\n",
    "\n",
    "        conv8 = tf.layers.conv2d_transpose(r7,128,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r8 = tf.nn.elu(tf.layers.batch_normalization(conv8,training=isTrain))#16*16*128\n",
    "             \n",
    "        conv9 = tf.layers.conv2d_transpose(r8,64,[5,5], strides=(2,2),padding = 'same',\n",
    "                                kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        r9 = tf.nn.elu(tf.layers.batch_normalization(conv9,training=isTrain))#32*32*64\n",
    "          \n",
    "        conv10 = tf.layers.conv2d_transpose(r9,1,[5,5], strides=(2,2),padding = 'same',\n",
    "                kernel_initializer=w_init, bias_initializer=b_init) #64*64*1\n",
    "        \n",
    "    r10= tf.nn.tanh(conv10,name=name)#64*64*1\n",
    "    \n",
    "    return r10\n",
    "def Q_cat(x,reuse = False, name = 'Q_cat') :\n",
    "    \n",
    "    w_init = tf.truncated_normal_initializer(mean= 0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope('Q_cat', reuse=reuse) :\n",
    "        \n",
    "        #x = (-1,64,64,1)\n",
    "        # out size = (in size + 2*padding - kenel)/strides + 1   \n",
    "        fc0  = tf.reshape(x, (-1, 100))\n",
    "        \n",
    "        w1 = tf.get_variable('w1',[100, c_size],initializer=w_init)\n",
    "        b1 = tf.get_variable('b1',[c_size],initializer=b_init)\n",
    "        \n",
    "                                          \n",
    "    fc1 = tf.nn.softmax(tf.matmul(fc0,w1) + b1, name = name)\n",
    "    \n",
    "    return tf.reshape(fc1, (-1,1,1,c_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32,shape=(None,1,1,z_size),name = 'z')    #x_z = G(z)\n",
    "c = tf.placeholder(tf.float32,shape=(None,1,1,c_size),name = 'c')    #x_z = G(z,c)\n",
    "\n",
    "u = tf.placeholder(tf.float32, shape = (None, 64,64,1),name='u')      #u = x\n",
    "k = tf.placeholder(tf.float32, name = 'k')\n",
    "\n",
    "\n",
    "isTrain = tf.placeholder(dtype=tf.bool,name='isTrain')  # BN 설정 parameter\n",
    "\n",
    "\n",
    "G_sample = G(z,c,name='G_sample') # G(z)\n",
    "E_z, E_c = E(u,isTrain,name = 'E_z') \n",
    "\n",
    "re_image = G(E_z,E_c, isTrain, reuse=True, name ='re_image')\n",
    "re_z, re_c = E(G_sample, isTrain, reuse=True, name ='re_z')\n",
    "\n",
    "\n",
    "\n",
    "re_z_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_z - z)**2, axis=[1,2,3])) , name = 're_z_loss') \n",
    "re_c_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(re_c + 1e-8), axis = [1,2,3]),name = 're_c_loss')\n",
    "re_image_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((re_image - u)**2, axis=[1,2,3])) , name = 're_image_loss') \n",
    "\n",
    "\n",
    "E_loss = tf.add(re_z_loss, re_c_loss, name = 'E_loss')                       \n",
    "\n",
    "\n",
    "D_real = D_dec(D_enc(u, isTrain,reuse=False), isTrain, reuse=False, name = 'D_real')                       # D(x)\n",
    "D_fake = D_dec(D_enc(G_sample, isTrain,reuse=True), isTrain, reuse=True, name = 'D_fake')         # D(G(z))\n",
    "Q_fake = Q_cat(D_enc(G_sample, isTrain,reuse=True), reuse=False, name='Q_fake')\n",
    "\n",
    "#input = (minibatch * w * h * ch)\n",
    "D_real_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_real-u)**2, axis=[1,2,3])) , name = 'D_real_loss')             \n",
    "\n",
    "D_fake_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])),  name = 'D_fake_loss' )\n",
    "\n",
    "D_loss =  tf.add(D_real_loss, -k*D_fake_loss, name='D_loss')                                        \n",
    "\n",
    "G_loss =  tf.reduce_mean(tf.sqrt(tf.reduce_sum((D_fake - G_sample)**2, axis=[1,2,3])), name='G_loss')                             # E[-log(D(G(z)))]\n",
    "Q_loss = tf.reduce_mean(tf.reduce_sum(-c*tf.log(Q_fake + 1e-8), axis = [1,2,3]),name = 'Q_loss')\n",
    "\n",
    "                                                                                                                                \n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('D_dec') or var.name.startswith('D_enc')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('G')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('E')]\n",
    "Q_vars = [var for var in T_vars if var.name.startswith('Q')]\n",
    "\n",
    "    # When using the batchnormalization layers,\n",
    "    # it is necessary to manually add the update operations\n",
    "    # because the moving averages are not included in the graph\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) :        \n",
    "    D_optim = tf.train.AdamOptimizer(2e-5,beta1=0.5).minimize(D_loss, var_list=D_vars, name='D_optim') \n",
    "    G_optim = tf.train.AdamOptimizer(2e-4,beta1=0.5).minimize(G_loss + Q_loss, var_list=G_vars+Q_vars, name='G_optim')\n",
    "    E_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(E_loss, var_list=E_vars, name='E_optim')\n",
    "    E_AE_optim = tf.train.AdamOptimizer(2e-4,beta1=0.1).minimize(re_image_loss, var_list=E_vars, name='E_AE_optim')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_e : -116.499, D_real_e : 50.609, D_fake_e : 34.364, G_e : 34.529, Q_e : 2.009, new_measure : 65.489, k_curr : 2.512977\n",
      "D_e : 28.167, D_real_e : 39.758, D_fake_e : 28.072, G_e : 28.566, Q_e : 2.076, new_measure : 42.732, k_curr : 0.452689\n",
      "D_e : 27.253, D_real_e : 35.317, D_fake_e : 24.066, G_e : 24.739, Q_e : 2.049, new_measure : 36.549, k_curr : 0.402695\n",
      "D_e : 25.818, D_real_e : 30.807, D_fake_e : 21.120, G_e : 21.604, Q_e : 1.950, new_measure : 32.099, k_curr : 0.293517\n",
      "D_e : 23.680, D_real_e : 27.019, D_fake_e : 18.623, G_e : 18.932, Q_e : 1.810, new_measure : 28.457, k_curr : 0.240837\n",
      "D_e : 22.306, D_real_e : 23.964, D_fake_e : 16.610, G_e : 16.805, Q_e : 1.655, new_measure : 25.430, k_curr : 0.157643\n",
      "D_e : 19.843, D_real_e : 21.206, D_fake_e : 14.726, G_e : 14.887, Q_e : 1.507, new_measure : 22.570, k_curr : 0.038830\n",
      "D_e : 18.704, D_real_e : 18.688, D_fake_e : 13.072, G_e : 13.140, Q_e : 1.379, new_measure : 20.369, k_curr : -0.123474\n",
      "D_e : 15.984, D_real_e : 16.469, D_fake_e : 11.407, G_e : 11.477, Q_e : 1.269, new_measure : 17.734, k_curr : 0.019580\n",
      "D_e : 14.264, D_real_e : 14.429, D_fake_e : 10.047, G_e : 10.106, Q_e : 1.170, new_measure : 15.639, k_curr : 0.003354\n",
      "D_e : 12.493, D_real_e : 12.590, D_fake_e : 8.689, G_e : 8.786, Q_e : 1.083, new_measure : 13.709, k_curr : 0.077775\n",
      "D_e : 11.211, D_real_e : 11.440, D_fake_e : 7.951, G_e : 8.019, Q_e : 1.005, new_measure : 12.226, k_curr : 0.045953\n",
      "D_e : 10.435, D_real_e : 10.540, D_fake_e : 7.329, G_e : 7.390, Q_e : 0.938, new_measure : 11.316, k_curr : 0.011921\n",
      "D_e : 9.421, D_real_e : 9.638, D_fake_e : 6.669, G_e : 6.732, Q_e : 0.879, new_measure : 10.238, k_curr : 0.053805\n",
      "D_e : 8.759, D_real_e : 8.932, D_fake_e : 6.162, G_e : 6.248, Q_e : 0.827, new_measure : 9.510, k_curr : 0.066531\n",
      "D_e : 8.353, D_real_e : 8.596, D_fake_e : 5.948, G_e : 6.027, Q_e : 0.780, new_measure : 9.064, k_curr : 0.037914\n",
      "D_e : 7.840, D_real_e : 8.005, D_fake_e : 5.512, G_e : 5.593, Q_e : 0.739, new_measure : 8.462, k_curr : 0.069418\n",
      "D_e : 7.412, D_real_e : 7.619, D_fake_e : 5.264, G_e : 5.337, Q_e : 0.702, new_measure : 7.972, k_curr : 0.060907\n",
      "D_e : 6.891, D_real_e : 7.103, D_fake_e : 4.894, G_e : 4.980, Q_e : 0.668, new_measure : 7.365, k_curr : 0.038400\n",
      "D_e : 6.295, D_real_e : 6.485, D_fake_e : 4.441, G_e : 4.538, Q_e : 0.637, new_measure : 6.732, k_curr : 0.042300\n",
      "D_e : 5.979, D_real_e : 6.185, D_fake_e : 4.224, G_e : 4.325, Q_e : 0.609, new_measure : 6.428, k_curr : 0.056999\n",
      "D_e : 5.790, D_real_e : 5.978, D_fake_e : 4.077, G_e : 4.185, Q_e : 0.584, new_measure : 6.211, k_curr : 0.055246\n",
      "D_e : 5.661, D_real_e : 5.826, D_fake_e : 3.986, G_e : 4.083, Q_e : 0.560, new_measure : 6.058, k_curr : 0.043032\n",
      "D_e : 5.537, D_real_e : 5.689, D_fake_e : 3.889, G_e : 3.992, Q_e : 0.538, new_measure : 5.915, k_curr : 0.014541\n",
      "D_e : 5.416, D_real_e : 5.572, D_fake_e : 3.791, G_e : 3.891, Q_e : 0.518, new_measure : 5.793, k_curr : 0.040612\n",
      "D_e : 5.325, D_real_e : 5.498, D_fake_e : 3.736, G_e : 3.840, Q_e : 0.499, new_measure : 5.701, k_curr : 0.064327\n",
      "D_e : 5.248, D_real_e : 5.404, D_fake_e : 3.696, G_e : 3.791, Q_e : 0.481, new_measure : 5.616, k_curr : 0.040402\n",
      "D_e : 5.147, D_real_e : 5.320, D_fake_e : 3.616, G_e : 3.714, Q_e : 0.465, new_measure : 5.519, k_curr : 0.068365\n",
      "D_e : 5.077, D_real_e : 5.247, D_fake_e : 3.567, G_e : 3.680, Q_e : 0.450, new_measure : 5.451, k_curr : 0.049367\n",
      "D_e : 5.016, D_real_e : 5.178, D_fake_e : 3.524, G_e : 3.630, Q_e : 0.435, new_measure : 5.384, k_curr : 0.033383\n",
      "D_e : 4.923, D_real_e : 5.106, D_fake_e : 3.459, G_e : 3.569, Q_e : 0.421, new_measure : 5.311, k_curr : 0.046730\n",
      "D_e : 4.895, D_real_e : 5.053, D_fake_e : 3.425, G_e : 3.538, Q_e : 0.408, new_measure : 5.235, k_curr : 0.043639\n",
      "D_e : 4.839, D_real_e : 4.990, D_fake_e : 3.404, G_e : 3.495, Q_e : 0.396, new_measure : 5.184, k_curr : 0.038144\n",
      "D_e : 4.825, D_real_e : 4.960, D_fake_e : 3.365, G_e : 3.470, Q_e : 0.385, new_measure : 5.162, k_curr : 0.043702\n",
      "D_e : 4.750, D_real_e : 4.891, D_fake_e : 3.313, G_e : 3.423, Q_e : 0.374, new_measure : 5.087, k_curr : 0.047872\n",
      "D_e : 4.705, D_real_e : 4.863, D_fake_e : 3.297, G_e : 3.406, Q_e : 0.364, new_measure : 5.041, k_curr : 0.042518\n",
      "D_e : 4.659, D_real_e : 4.803, D_fake_e : 3.259, G_e : 3.365, Q_e : 0.354, new_measure : 4.992, k_curr : 0.034838\n",
      "D_e : 4.663, D_real_e : 4.802, D_fake_e : 3.252, G_e : 3.361, Q_e : 0.345, new_measure : 4.978, k_curr : 0.037930\n",
      "D_e : 4.593, D_real_e : 4.719, D_fake_e : 3.194, G_e : 3.303, Q_e : 0.336, new_measure : 4.904, k_curr : 0.038951\n",
      "D_e : 4.571, D_real_e : 4.686, D_fake_e : 3.178, G_e : 3.278, Q_e : 0.328, new_measure : 4.879, k_curr : 0.044942\n",
      "D_e : 4.557, D_real_e : 4.684, D_fake_e : 3.168, G_e : 3.282, Q_e : 0.320, new_measure : 4.872, k_curr : 0.036789\n",
      "D_e : 4.510, D_real_e : 4.623, D_fake_e : 3.150, G_e : 3.238, Q_e : 0.313, new_measure : 4.805, k_curr : 0.031096\n",
      "D_e : 4.470, D_real_e : 4.568, D_fake_e : 3.109, G_e : 3.195, Q_e : 0.306, new_measure : 4.761, k_curr : 0.037260\n",
      "D_e : 4.442, D_real_e : 4.548, D_fake_e : 3.092, G_e : 3.186, Q_e : 0.299, new_measure : 4.716, k_curr : 0.031174\n",
      "D_e : 4.417, D_real_e : 4.525, D_fake_e : 3.060, G_e : 3.161, Q_e : 0.292, new_measure : 4.691, k_curr : 0.048280\n",
      "D_e : 4.375, D_real_e : 4.495, D_fake_e : 3.050, G_e : 3.146, Q_e : 0.286, new_measure : 4.666, k_curr : 0.049286\n",
      "D_e : 4.361, D_real_e : 4.482, D_fake_e : 3.030, G_e : 3.137, Q_e : 0.280, new_measure : 4.659, k_curr : 0.050244\n",
      "D_e : 4.322, D_real_e : 4.447, D_fake_e : 3.014, G_e : 3.115, Q_e : 0.274, new_measure : 4.624, k_curr : 0.044651\n",
      "D_e : 4.311, D_real_e : 4.430, D_fake_e : 2.997, G_e : 3.100, Q_e : 0.269, new_measure : 4.603, k_curr : 0.046648\n",
      "D_e : 4.279, D_real_e : 4.401, D_fake_e : 2.980, G_e : 3.084, Q_e : 0.263, new_measure : 4.585, k_curr : 0.038051\n",
      "total time :  4158.209940433502\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7535dbbeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.set_random_seed(int(time.time()))\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "\n",
    "    \n",
    "    one_hot = np.eye(c_size)\n",
    "    temp2 = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4])\n",
    "    test_c = one_hot[temp2].reshape([-1,1,1,c_size])\n",
    "    test_z = np.random.uniform(-1,1,size=(16,1,1,z_size))\n",
    "    mnist_4by4_save(np.reshape(test_normal_data[0:16],(-1,64,64,1)),file_name + '/D_origin.png')    \n",
    "    mnist_4by4_save(np.reshape(test_anomalous_data[0:16],(-1,64,64,1)),file_name + '/anomalous.png')    \n",
    "    log_txt = open(file_name +'/log.txt','w')\n",
    "\n",
    "    hist_G = []\n",
    "    hist_D = []\n",
    "    G_error = []\n",
    "    D_error = []\n",
    "    Q_error=[]\n",
    "    E_error = []\n",
    "    D_fake_error = []\n",
    "    D_real_error = []\n",
    "    new_measure = []\n",
    "    new_k = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(train_epoch) :\n",
    "        \n",
    "        train_normal_data = idx_shuffle(train_normal_data) \n",
    "        \n",
    "        for iteration in range(train_normal_data.shape[0] // batch_size) : \n",
    "        \n",
    "            \n",
    "            train_images = train_normal_data[iteration*batch_size : (iteration+1)*batch_size]      \n",
    "            u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            z_ = np.random.uniform(-1,1,size=(batch_size,1,1,z_size))\n",
    "            temp1 = np.random.randint(0,10,(batch_size))                                                                                                                                     \n",
    "            c_ = one_hot[temp1].reshape([-1,1,1,c_size])\n",
    "        \n",
    "            _ , D_e,D_real_e,D_fake_e = sess.run([D_optim, D_loss,D_real_loss,D_fake_loss], {u : u_, z : z_, c : c_, k : k_curr,isTrain : True})\n",
    "            D_error.append(D_e)\n",
    "            D_real_error.append(np.maximum(0.0, D_real_e))\n",
    "            D_fake_error.append(np.maximum(0.0,D_fake_e))\n",
    "\n",
    "            #    train_images,train_labels = mnist.train.next_batch(100)    \n",
    "            #    u_ = np.reshape(train_images,(-1,64,64,1)) \n",
    "            #    z_ = np.random.normal(0,1,size=(100,1,1,100))\n",
    "   \n",
    "            _ , G_e,Q_e = sess.run([G_optim, G_loss,Q_loss], {u : u_, z : z_, c : c_, k : k_curr, isTrain : True}) \n",
    "            G_error.append(G_e)\n",
    "            Q_error.append(Q_e)\n",
    "\n",
    "            \n",
    "            k_curr = k_curr + lam * (gamma*D_real_e - G_e)\n",
    "            \n",
    "\n",
    "            \n",
    "            measure = D_real_e + np.abs(gamma*D_real_e - G_e)\n",
    "            \n",
    "            new_measure.append(measure)\n",
    "            new_k.append(k_curr)\n",
    "        hist_D.append(np.mean(D_error)) \n",
    "        hist_G.append(np.mean(G_error))\n",
    "\n",
    "        print('D_e : %.3f, D_real_e : %.3f, D_fake_e : %.3f, G_e : %.3f, Q_e : %.3f, new_measure : %.3f, k_curr : %3f'\n",
    "              %(np.mean(D_error), np.mean(D_real_error),np.mean(D_fake_error), np.mean(G_error),\n",
    "                np.mean(Q_error),np.mean(new_measure),k_curr))\n",
    "        log_txt.write('D_e : %.6f, D_real_e : %.6f, D_fake_e : %.6f, G_e : %.6f\\n'%(np.mean(D_error),\n",
    "            np.mean(D_real_error), np.mean(D_fake_error), np.mean(G_error)))\n",
    "      \n",
    "        r = sess.run([G_sample],feed_dict={z : test_z, c : test_c, isTrain : False})       \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/result_{}.png'.format(str(epoch).zfill(3)))\n",
    "\n",
    "        r = sess.run([D_real],feed_dict={u : test_normal_data[0:16], isTrain : False})        \n",
    "        mnist_4by4_save(np.reshape(r,(-1,64,64,1)),file_name + '/D_{}.png'.format(str(epoch).zfill(3)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "\n",
    "        G_error = []\n",
    "        D_error = []       \n",
    "        D_fake_error = []     \n",
    "        D_real_error = []\n",
    "        new_measure = []\n",
    "    \n",
    "    \n",
    "    log_txt.close()\n",
    "    gan_loss_graph_save(G_loss = hist_G,D_loss=hist_D,path = file_name + '/loss_graph.png')   \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,file_name + '/para.cktp')\n",
    "\n",
    "    end = time.time()-start\n",
    "\n",
    "    print(\"total time : \",end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ex_BE_infoGANs_for_FD_v4/para.cktp\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(file_name) :\n",
    "    os.mkdir(file_name)\n",
    "\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "new_saver = tf.train.import_meta_graph(file_name + '/para.cktp.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(file_name + '/'))\n",
    "\n",
    "\n",
    "z = sess.graph.get_tensor_by_name(\"z:0\")\n",
    "u = sess.graph.get_tensor_by_name(\"u:0\")\n",
    "\n",
    "isTrain = sess.graph.get_tensor_by_name(\"isTrain:0\")\n",
    "\n",
    "G_sample = sess.graph.get_tensor_by_name(\"G_sample:0\")\n",
    "\n",
    "D_real = sess.graph.get_tensor_by_name('D_real:0')                       # D(x)\n",
    "D_fake = sess.graph.get_tensor_by_name('D_fake:0')         # D(G(z))\n",
    "\n",
    "\n",
    "D_real_loss = sess.graph.get_tensor_by_name('D_real_loss:0')\n",
    "D_fake_loss = sess.graph.get_tensor_by_name('D_fake_loss:0')\n",
    "\n",
    "D_loss = sess.graph.get_tensor_by_name(\"D_loss:0\")\n",
    "G_loss = sess.graph.get_tensor_by_name(\"G_loss:0\")\n",
    "\n",
    "\n",
    "D_optim = sess.graph.get_operation_by_name(\"D_optim\")\n",
    "G_optim = sess.graph.get_operation_by_name(\"G_optim\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 're_z:0' shape=(?, 1, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20559981155395504,\n",
       " 0.42167232513427727,\n",
       " 0.64357150268554675,\n",
       " 0.87060523605346662,\n",
       " 1.1006398124694823,\n",
       " 1.3354404716491697,\n",
       " 1.5736074485778806,\n",
       " 1.8146699905395505,\n",
       " 2.0586228370666499,\n",
       " 2.3020437164306635,\n",
       " 2.5467698173522941,\n",
       " 2.7921664314270012,\n",
       " 3.0381669540405265,\n",
       " 3.2831703529357901,\n",
       " 3.5271912422180165,\n",
       " 3.7717378463745108,\n",
       " 4.0155483016967768,\n",
       " 4.2584535255432119,\n",
       " 4.4979576568603505,\n",
       " 4.7357731971740709,\n",
       " 4.9746050491332996,\n",
       " 5.2105871238708481,\n",
       " 5.4427100410461406,\n",
       " 5.6722306098937967,\n",
       " 5.9021061096191385,\n",
       " 6.1297304344177226,\n",
       " 6.3539888954162578,\n",
       " 6.5769291496276834,\n",
       " 6.7984302406311015,\n",
       " 7.0161357765197732,\n",
       " 7.2317463493347143,\n",
       " 7.4450649375915505,\n",
       " 7.6563996810913064,\n",
       " 7.8643955802917462,\n",
       " 8.0704077720642076,\n",
       " 8.275120887756346,\n",
       " 8.4754046478271476,\n",
       " 8.6755895423889147,\n",
       " 8.8735893898010243,\n",
       " 9.0703680229187,\n",
       " 9.264922863006591,\n",
       " 9.4553695220947258,\n",
       " 9.6430978622436516,\n",
       " 9.8314633026123044,\n",
       " 10.01767081451416,\n",
       " 10.199271152496337,\n",
       " 10.380825653076171,\n",
       " 10.557407009124754,\n",
       " 10.730475185394285,\n",
       " 10.900133052825925,\n",
       " 11.066396076202389,\n",
       " 11.226742961883541,\n",
       " 11.381771007537838,\n",
       " 11.530525074005123,\n",
       " 11.67196518707275,\n",
       " 11.80241338729858,\n",
       " 11.924396999359127,\n",
       " 12.03305773544311,\n",
       " 12.126539875030513,\n",
       " 12.211727264404292,\n",
       " 12.290765636444087,\n",
       " 12.355215175628658,\n",
       " 12.426375110626216,\n",
       " 12.490942703247066,\n",
       " 12.554528820037838,\n",
       " 12.632462863922115,\n",
       " 12.726212230682368,\n",
       " 12.795584201812739,\n",
       " 12.865671318054194,\n",
       " 12.928183372497553,\n",
       " 12.98705690002441,\n",
       " 13.041326072692867,\n",
       " 13.091997093200678,\n",
       " 13.144312644958491,\n",
       " 13.184585292816157,\n",
       " 13.218067665100094,\n",
       " 13.246773963928218,\n",
       " 13.264851318359371,\n",
       " 13.279926349639888,\n",
       " 13.294587043762203,\n",
       " 13.302213581085201,\n",
       " 13.308310134887691,\n",
       " 13.308675136566158,\n",
       " 13.307239032745358,\n",
       " 13.293157878875729,\n",
       " 13.274797573089595,\n",
       " 13.246902568817134,\n",
       " 13.214203224182125,\n",
       " 13.182046169281001,\n",
       " 13.124385547637935,\n",
       " 13.084916915893551,\n",
       " 13.027088790893551,\n",
       " 12.964827873229977,\n",
       " 12.908074935913083,\n",
       " 12.859374916076657,\n",
       " 12.793264617919919,\n",
       " 12.745386886596677,\n",
       " 12.689878211975095,\n",
       " 12.629288360595702,\n",
       " 12.552096847534179,\n",
       " 12.471090446472166,\n",
       " 12.392541126251219,\n",
       " 12.302293487548827,\n",
       " 12.191276214599608,\n",
       " 12.11016547012329,\n",
       " 12.022773761749267,\n",
       " 11.926790855407715,\n",
       " 11.816520282745362,\n",
       " 11.709514705657959,\n",
       " 11.601207828521728,\n",
       " 11.49395778274536,\n",
       " 11.378477787017822,\n",
       " 11.270876609802245,\n",
       " 11.13481809234619,\n",
       " 11.013021373748778,\n",
       " 10.897370147705077,\n",
       " 10.759657276153563,\n",
       " 10.626601177215575,\n",
       " 10.473972358703612,\n",
       " 10.338734916687011,\n",
       " 10.195081569671631,\n",
       " 10.060103477478027,\n",
       " 9.903067527770995,\n",
       " 9.7744355773925768,\n",
       " 9.6359472351074213,\n",
       " 9.4819909286499016,\n",
       " 9.3402045288085933,\n",
       " 9.1989852867126451,\n",
       " 9.0510464668273904,\n",
       " 8.913443759918211,\n",
       " 8.7612875900268534,\n",
       " 8.5974496879577611,\n",
       " 8.4347231788635231,\n",
       " 8.2932193832397445,\n",
       " 8.1407530517578106,\n",
       " 7.9730803565978983,\n",
       " 7.7978105964660624,\n",
       " 7.6560094413757307,\n",
       " 7.4958130760192851,\n",
       " 7.3489550628662093,\n",
       " 7.1737143745422349,\n",
       " 7.0063465919494616,\n",
       " 6.8285532493591292,\n",
       " 6.6655288467407212,\n",
       " 6.490717281341551,\n",
       " 6.3272802810668924,\n",
       " 6.1528032951354961,\n",
       " 5.97812997817993,\n",
       " 5.7908877677917463,\n",
       " 5.6073207550048814,\n",
       " 5.4499510536193831,\n",
       " 5.2675711402893048,\n",
       " 5.0841713752746562,\n",
       " 4.9085383720397928,\n",
       " 4.7375039825439433,\n",
       " 4.5891361732482894,\n",
       " 4.4115115318298326,\n",
       " 4.2341578445434553,\n",
       " 4.0304643211364732,\n",
       " 3.8346422195434555,\n",
       " 3.6428599472045882,\n",
       " 3.4582663841247543,\n",
       " 3.2584946250915512,\n",
       " 3.0655900497436508,\n",
       " 2.8775840301513655,\n",
       " 2.692198856353758,\n",
       " 2.5059500999450668,\n",
       " 2.3071976623535142,\n",
       " 2.1167544250488266,\n",
       " 1.9090003738403305,\n",
       " 1.7073631782531722,\n",
       " 1.5221205101013167,\n",
       " 1.3491771965026838,\n",
       " 1.1481322097778301,\n",
       " 0.94173426437377739,\n",
       " 0.75050397109985156,\n",
       " 0.54868401336669725,\n",
       " 0.35983799743652145,\n",
       " 0.17424218749999798,\n",
       " -0.017069995880128974,\n",
       " -0.21067647171020709,\n",
       " -0.41887345504760948,\n",
       " -0.62968757247925011,\n",
       " -0.82791962814331266,\n",
       " -1.0213349647521994,\n",
       " -1.2196915016174339,\n",
       " -1.4135946273803732,\n",
       " -1.6116100845336936,\n",
       " -1.7962277565002462,\n",
       " -1.9770825309753439,\n",
       " -2.1744922409057637,\n",
       " -2.3730091857910174,\n",
       " -2.5618623313903828,\n",
       " -2.7607710189819357,\n",
       " -2.950173110961916,\n",
       " -3.1272935791015644,\n",
       " -3.312357551574709,\n",
       " -3.4944367675781272,\n",
       " -3.6833954467773458,\n",
       " -3.8742315826416034,\n",
       " -4.0619704818725602,\n",
       " -4.2306192779541032,\n",
       " -4.3928989028930685,\n",
       " -4.5629906806945826,\n",
       " -4.7374653625488303,\n",
       " -4.8887986221313495,\n",
       " -5.0574890327453632,\n",
       " -5.2127697868347189,\n",
       " -5.3559008216857933,\n",
       " -5.4960897636413595,\n",
       " -5.611913333892824,\n",
       " -5.7131779632568378,\n",
       " -5.8237157211303732,\n",
       " -5.9094624099731465,\n",
       " -5.9724725227355977,\n",
       " -6.0473657226562523,\n",
       " -6.1028753662109398,\n",
       " -6.1457354164123563,\n",
       " -6.2042026939392114,\n",
       " -6.2648643760681173,\n",
       " -6.2585525131225603,\n",
       " -6.2615580139160176,\n",
       " -6.2435017738342307,\n",
       " -6.1767905693054219,\n",
       " -6.1708616333007829,\n",
       " -6.1414834403991714,\n",
       " -6.0574982833862316,\n",
       " -6.0025377464294447,\n",
       " -5.899110877990724,\n",
       " -5.7930156211853037,\n",
       " -5.686772590637208,\n",
       " -5.5482435798645033,\n",
       " -5.425706748962404,\n",
       " -5.2869305686950696,\n",
       " -5.1648203697204602,\n",
       " -5.0316259307861344,\n",
       " -4.8940674438476579,\n",
       " -4.7606795997619642,\n",
       " -4.5961961936950697,\n",
       " -4.4629719734191911,\n",
       " -4.3141278743743916,\n",
       " -4.181518018722536,\n",
       " -4.0025399475097672,\n",
       " -3.8160391311645525,\n",
       " -3.6224572334289569,\n",
       " -3.4251700954437272,\n",
       " -3.2335004138946548,\n",
       " -3.0840424423217789,\n",
       " -2.890515596389772,\n",
       " -2.685418621063234,\n",
       " -2.4806578330993667,\n",
       " -2.2796414909362808,\n",
       " -2.071933113098146,\n",
       " -1.9928101119995132,\n",
       " -1.8161983394622818,\n",
       " -1.6705696010589615,\n",
       " -1.5206518917083756,\n",
       " -1.3391867885589614,\n",
       " -1.1559195442199721,\n",
       " -0.95425222206115867,\n",
       " -0.75196839714050445,\n",
       " -0.58686206054687651,\n",
       " -0.39142901611328279,\n",
       " -0.19425009536743318,\n",
       " -0.018310199737550381,\n",
       " 0.17091029167175137,\n",
       " 0.3704745121002182,\n",
       " 0.57078419113159029,\n",
       " 0.74361781311035002,\n",
       " 0.94714443397521819,\n",
       " 1.1394531879425034,\n",
       " 1.2761086368560777,\n",
       " 1.4637772407531724,\n",
       " 1.6462061882019028,\n",
       " 1.8227174892425522,\n",
       " 1.9923011627197249,\n",
       " 2.1407787532806379,\n",
       " 2.2925979537963848,\n",
       " 2.4126959495544416,\n",
       " 2.5129773254394512,\n",
       " 2.5927854995727517,\n",
       " 2.5959644737243632,\n",
       " 2.6002677421569804,\n",
       " 2.5901604728698708,\n",
       " 2.5559486732482886,\n",
       " 2.5156815910339332,\n",
       " 2.4536960411071753,\n",
       " 2.3773016014099095,\n",
       " 2.2716066207885715,\n",
       " 2.1757774467468232,\n",
       " 2.0462646141052216,\n",
       " 1.9069590263366669,\n",
       " 1.7663521842956513,\n",
       " 1.627105442047116,\n",
       " 1.4982513313293426,\n",
       " 1.3713226966857879,\n",
       " 1.2522070541381805,\n",
       " 1.1215852775573698,\n",
       " 1.00225556182861,\n",
       " 0.88588240432738929,\n",
       " 0.75129460906982093,\n",
       " 0.63814514923095378,\n",
       " 0.52329340362548504,\n",
       " 0.42027546691894208,\n",
       " 0.31685401916503581,\n",
       " 0.19599597930907875,\n",
       " 0.090664371490475223,\n",
       " -0.0019988861084017373,\n",
       " -0.067048683166507222,\n",
       " -0.13935708618164394,\n",
       " -0.21387520980835295,\n",
       " -0.31741148376465178,\n",
       " -0.37331495285034516,\n",
       " -0.45432854843139986,\n",
       " -0.48962041091919284,\n",
       " -0.51623468780517923,\n",
       " -0.50591926193237657,\n",
       " -0.57273756408691756,\n",
       " -0.63022499847412461,\n",
       " -0.67378005218506209,\n",
       " -0.71544576644897817,\n",
       " -0.73254757308960317,\n",
       " -0.7286380844116247,\n",
       " -0.69716917800903688,\n",
       " -0.63162936401367553,\n",
       " -0.57260370635986702,\n",
       " -0.51533685684204478,\n",
       " -0.44499929809570693,\n",
       " -0.36420453262329483,\n",
       " -0.28555489730835348,\n",
       " -0.20157354736328514,\n",
       " -0.093083934783939448,\n",
       " 0.0016476364135703109,\n",
       " 0.090451519012447232,\n",
       " 0.17411388778686129,\n",
       " 0.25657094955443938,\n",
       " 0.29874300384521085,\n",
       " 0.38957968902587492,\n",
       " 0.43989705657958583,\n",
       " 0.52454589843749599,\n",
       " 0.56860231018065999,\n",
       " 0.617863441467281,\n",
       " 0.59241115570067948,\n",
       " 0.62866165161132403,\n",
       " 0.60660241699218342,\n",
       " 0.58997481536864826,\n",
       " 0.56898127365111895,\n",
       " 0.53439297485351145,\n",
       " 0.48303107833861886,\n",
       " 0.42661961364745676,\n",
       " 0.38847399902343333,\n",
       " 0.35770589065551339,\n",
       " 0.31167110443114815,\n",
       " 0.28385604858398017,\n",
       " 0.23068584442138249,\n",
       " 0.21785267639159731,\n",
       " 0.12635722732543517,\n",
       " 0.097603538513179294,\n",
       " 0.051066028594966388,\n",
       " 0.02496321868896053,\n",
       " -0.0049138336181684117,\n",
       " -0.045112197875980947,\n",
       " -0.0661115074157759,\n",
       " -0.050558776855473199,\n",
       " -0.07737768173218218,\n",
       " -0.073056831359867722,\n",
       " -0.083848117828373606,\n",
       " -0.084765769958500561,\n",
       " -0.054842121124272079,\n",
       " -0.034497718811039663,\n",
       " -0.005109230041508428,\n",
       " 0.011181999206538441,\n",
       " 0.052272178649897791,\n",
       " 0.078748126983638006,\n",
       " 0.096427257537837222,\n",
       " 0.1311784210205032,\n",
       " 0.16553467941283717,\n",
       " 0.200846042633052,\n",
       " 0.24986605072021018,\n",
       " 0.27363923645019061,\n",
       " 0.27708043289184098,\n",
       " 0.29210735702514173,\n",
       " 0.29378346633910657,\n",
       " 0.32814448165893079,\n",
       " 0.30890452194213391,\n",
       " 0.31835345077514171,\n",
       " 0.31404355239867687,\n",
       " 0.27418393325805185,\n",
       " 0.26807893371581548,\n",
       " 0.24879263305663576,\n",
       " 0.24940834045409668,\n",
       " 0.22989556884765136,\n",
       " 0.20791825103759276,\n",
       " 0.19352774810790524,\n",
       " 0.21509131240844231,\n",
       " 0.19370249557494623,\n",
       " 0.20008942794799309,\n",
       " 0.1838297348022411,\n",
       " 0.19043813323974107,\n",
       " 0.19952313232421373,\n",
       " 0.19382579803466293,\n",
       " 0.1904752693176219,\n",
       " 0.20120914077758281,\n",
       " 0.22359760284423319,\n",
       " 0.23631573486327612,\n",
       " 0.22400236511229954,\n",
       " 0.24024124526977025,\n",
       " 0.26058016204833473,\n",
       " 0.25903464889525857,\n",
       " 0.27792992019652807,\n",
       " 0.29245167541503392,\n",
       " 0.28916861343383277,\n",
       " 0.28218321228026827,\n",
       " 0.30649305343627409,\n",
       " 0.31613586044310998,\n",
       " 0.24664267730712364,\n",
       " 0.2059715385436959,\n",
       " 0.18773922348021932,\n",
       " 0.18450984191893999,\n",
       " 0.15963797378539504,\n",
       " 0.14414958953856885,\n",
       " 0.1539002799987739,\n",
       " 0.162116920471186,\n",
       " 0.16188851547240671,\n",
       " 0.1516356964111274,\n",
       " 0.15593951034545356,\n",
       " 0.11453687667846135,\n",
       " 0.12080742645263125,\n",
       " 0.10207293701171327,\n",
       " 0.10254932785033631,\n",
       " 0.094394359588617541,\n",
       " 0.12068924713134212,\n",
       " 0.11658034896850029,\n",
       " 0.10595080566405692,\n",
       " 0.090355468749994394,\n",
       " 0.12861595535277756,\n",
       " 0.13318406677245528,\n",
       " 0.17388821029662518,\n",
       " 0.20852159881591228,\n",
       " 0.19972613906859782,\n",
       " 0.22710750961303139,\n",
       " 0.21641881179808994,\n",
       " 0.22954693603515047,\n",
       " 0.22952735137938873,\n",
       " 0.24405035018920315,\n",
       " 0.25638275527953519,\n",
       " 0.26613435745238673,\n",
       " 0.28229577636718162,\n",
       " 0.30506159210204487,\n",
       " 0.33640012741088277,\n",
       " 0.36676960754393939,\n",
       " 0.40448156356810933,\n",
       " 0.41422040939330462,\n",
       " 0.42749162673949603,\n",
       " 0.42782713699340225,\n",
       " 0.40648584365844131,\n",
       " 0.35310993957518932,\n",
       " 0.36174465942382211,\n",
       " 0.36436874771117561,\n",
       " 0.37592378234862678,\n",
       " 0.39124443817138066,\n",
       " 0.40742604446410524,\n",
       " 0.40462443161010131,\n",
       " 0.41626198959349975,\n",
       " 0.39311071395873409,\n",
       " 0.3991130561828552,\n",
       " 0.40425050354003289,\n",
       " 0.39386987304686882,\n",
       " 0.38401499557494501,\n",
       " 0.40813859939574576,\n",
       " 0.41656259918212268,\n",
       " 0.42556568908690784,\n",
       " 0.42968167114257189,\n",
       " 0.42898062515258167,\n",
       " 0.43687027740477896,\n",
       " 0.44181903839110709,\n",
       " 0.43387989425658557,\n",
       " 0.44231175613402696,\n",
       " 0.44932508468627302,\n",
       " 0.45379509735106793,\n",
       " 0.43486591720580425,\n",
       " 0.4358415527343687,\n",
       " 0.41263282394408546,\n",
       " 0.40820554733275732,\n",
       " 0.40610762786864602,\n",
       " 0.4050064125060972,\n",
       " 0.40674359893798195,\n",
       " 0.40424557876586281,\n",
       " 0.4074600868225034,\n",
       " 0.41798904418944677,\n",
       " 0.44182905578612641,\n",
       " 0.44567564010619476,\n",
       " 0.46745865631102873,\n",
       " 0.4547888946533139,\n",
       " 0.44925726699828455,\n",
       " 0.45049947357177084,\n",
       " 0.44965719223021811,\n",
       " 0.43952585601805988,\n",
       " 0.4521934204101497,\n",
       " 0.46673606872557938,\n",
       " 0.45872063064574536,\n",
       " 0.49042764282225904,\n",
       " 0.49119390106200511,\n",
       " 0.49605217742919261,\n",
       " 0.50416312789916329,\n",
       " 0.51118035125731753,\n",
       " 0.51954882431029603,\n",
       " 0.53899198150634098,\n",
       " 0.55342490386962218,\n",
       " 0.54713993072509093,\n",
       " 0.545933158874505,\n",
       " 0.53959773635863584,\n",
       " 0.52255810165404593,\n",
       " 0.51802269744872365,\n",
       " 0.53766625213622365,\n",
       " 0.54031686401366508,\n",
       " 0.52119336700438779,\n",
       " 0.52845829010009093,\n",
       " 0.51199232864379207,\n",
       " 0.51536204910277639,\n",
       " 0.48250869750975878,\n",
       " 0.45170026397704394,\n",
       " 0.42612846374511032,\n",
       " 0.42138342666625289,\n",
       " 0.41442898559569624,\n",
       " 0.41667456054686808,\n",
       " 0.40135723876952434,\n",
       " 0.40404536819457315,\n",
       " 0.38988301086425087,\n",
       " 0.36582628631591102,\n",
       " 0.36170897674559849,\n",
       " 0.35217880249022737,\n",
       " 0.34867959594725861,\n",
       " 0.35199269485472928,\n",
       " 0.36448283386229763,\n",
       " 0.37373085784911403,\n",
       " 0.33403367996215111,\n",
       " 0.31691290664672139,\n",
       " 0.31108932876586198,\n",
       " 0.29958280944823501,\n",
       " 0.2796811523437428,\n",
       " 0.29912015533446545,\n",
       " 0.30223486709594005,\n",
       " 0.32584021759032478,\n",
       " 0.3355704917907642,\n",
       " 0.35862154388427003,\n",
       " 0.38444146728514894,\n",
       " 0.38368871688842043,\n",
       " 0.39936660385131106,\n",
       " 0.38879653167723877,\n",
       " 0.39373671340941646,\n",
       " 0.40922328948973874,\n",
       " 0.4112600479125903,\n",
       " 0.41766641616820555,\n",
       " 0.44484035491942625,\n",
       " 0.44174075317382078,\n",
       " 0.45130222320555902,\n",
       " 0.44534503936766839,\n",
       " 0.44547159957885002,\n",
       " 0.45268924331664295,\n",
       " 0.46282362747191641,\n",
       " 0.47526167678832265,\n",
       " 0.45629610061644765,\n",
       " 0.45243024444579333,\n",
       " 0.45576711273192616,\n",
       " 0.43352418136595933,\n",
       " 0.43419929504393784,\n",
       " 0.42001574325560775,\n",
       " 0.415218196868889,\n",
       " 0.42241928863524641,\n",
       " 0.42200931930541241,\n",
       " 0.45492281341551982,\n",
       " 0.45941323852538307,\n",
       " 0.46700096130370339,\n",
       " 0.4761230850219651,\n",
       " 0.45902425384520729,\n",
       " 0.45639820480345922,\n",
       " 0.46245830154418188,\n",
       " 0.46883380126952368,\n",
       " 0.47108232879637912,\n",
       " 0.46486666870116428,\n",
       " 0.43934471511840062,\n",
       " 0.43080165481566624,\n",
       " 0.42062189483641815,\n",
       " 0.42189524459838101,\n",
       " 0.42522232818602751,\n",
       " 0.41426223373412319,\n",
       " 0.41545546340941614,\n",
       " 0.3999150962829513,\n",
       " 0.39545491409300987,\n",
       " 0.39126257324217978,\n",
       " 0.36770011901854693,\n",
       " 0.35953318023680864,\n",
       " 0.34743294525145707,\n",
       " 0.31776520156859572,\n",
       " 0.29544168853758984,\n",
       " 0.26477993011473827,\n",
       " 0.24663038253783398,\n",
       " 0.2472164726257246,\n",
       " 0.25788910675048043,\n",
       " 0.25211656951903511,\n",
       " 0.24627068710326358,\n",
       " 0.25362776565550965,\n",
       " 0.24957117080687682,\n",
       " 0.25753891754149594,\n",
       " 0.28264661026000176,\n",
       " 0.28888917922972829,\n",
       " 0.26815795898436695,\n",
       " 0.26284281921385916,\n",
       " 0.26993411636351733,\n",
       " 0.26480912017821456,\n",
       " 0.26618253326415203,\n",
       " 0.28553392028807784,\n",
       " 0.28016434860228678,\n",
       " 0.27342351913451329,\n",
       " 0.29055859756468905,\n",
       " 0.28599888992308747,\n",
       " 0.30941414642333159,\n",
       " 0.32424079895018704,\n",
       " 0.34276829147338039,\n",
       " 0.33866486740111473,\n",
       " 0.35819765853881003,\n",
       " 0.37205136108397602,\n",
       " 0.36652066040038228,\n",
       " 0.35247680282591937,\n",
       " 0.37822677230834123,\n",
       " 0.37985089874266742,\n",
       " 0.40050397109984515,\n",
       " 0.42703609848021623,\n",
       " 0.42142620468138808,\n",
       " 0.41666469573973769,\n",
       " 0.40007469940184703,\n",
       " 0.37516861724852674,\n",
       " 0.36145263671874156,\n",
       " 0.37153263854979623,\n",
       " 0.36137265777587041,\n",
       " 0.35408869171141727,\n",
       " 0.35076404571532349,\n",
       " 0.35159944152831174,\n",
       " 0.34809163665770626,\n",
       " 0.35713513183592893,\n",
       " 0.3642275657653723,\n",
       " 0.37223837280272581,\n",
       " 0.36801703643797967,\n",
       " 0.34799825286864372,\n",
       " 0.34694177246092889,\n",
       " 0.34650730133055779,\n",
       " 0.34119859695433707,\n",
       " 0.31955260467528435,\n",
       " 0.33090657424925896,\n",
       " 0.32855076980589959,\n",
       " 0.35479541778563589,\n",
       " 0.28656772232054795,\n",
       " 0.28881693267821396,\n",
       " 0.29863927459715928,\n",
       " 0.28153795623778427,\n",
       " 0.25415302276610457,\n",
       " 0.24907406997679793,\n",
       " 0.24048522186278426,\n",
       " 0.24055662918089948,\n",
       " 0.2500463867187413,\n",
       " 0.28268212127684672,\n",
       " 0.28595591354369243,\n",
       " 0.31156629943846781,\n",
       " 0.30811619186400491,\n",
       " 0.31082934188841893,\n",
       " 0.30745957946776459,\n",
       " 0.32797240447997161,\n",
       " 0.31967203903197355,\n",
       " 0.28585405731200281,\n",
       " 0.29553180313109462,\n",
       " 0.29944363403319424,\n",
       " 0.3095432472228915,\n",
       " 0.32657086944579189,\n",
       " 0.32257402420043052,\n",
       " 0.34508507537840899,\n",
       " 0.35715527725218826,\n",
       " 0.33436056900023514,\n",
       " 0.3398236808776765,\n",
       " 0.34623232269286203,\n",
       " 0.36143380737303782,\n",
       " 0.35682556915282299,\n",
       " 0.3558798103332429,\n",
       " 0.34175526046752019,\n",
       " 0.35566476058959046,\n",
       " 0.37530182266234435,\n",
       " 0.36528326416014711,\n",
       " 0.36125164031981505,\n",
       " 0.38445438003539117,\n",
       " 0.37853657150267628,\n",
       " 0.36539226531981495,\n",
       " 0.36767061996459033,\n",
       " 0.36620492553710005,\n",
       " 0.35454638671874067,\n",
       " 0.33865967178343792,\n",
       " 0.32612568283080118,\n",
       " 0.33048361968993201,\n",
       " 0.35384072113036169,\n",
       " 0.36063323974608436,\n",
       " 0.34465148544310581,\n",
       " 0.34163544845580113,\n",
       " 0.32720417404173863,\n",
       " 0.32933400726317419,\n",
       " 0.30844841003417028,\n",
       " 0.31309841156004919,\n",
       " 0.28120893096922883,\n",
       " 0.2668744850158597,\n",
       " 0.12817546463011747,\n",
       " 0.12468952560423854,\n",
       " 0.1109719200134182,\n",
       " 0.11077968597411153,\n",
       " 0.11168546676634786,\n",
       " 0.14007739257811544,\n",
       " 0.16207020950316425,\n",
       " 0.18151637649535174,\n",
       " 0.19830498123167983,\n",
       " 0.19518228530882825,\n",
       " 0.19943967437743174,\n",
       " 0.19458558654784189,\n",
       " 0.21610047149657235,\n",
       " 0.21963000106810557,\n",
       " 0.25950279235838875,\n",
       " 0.29870696640013678,\n",
       " 0.31673590087889653,\n",
       " 0.32507674789427737,\n",
       " 0.32096347045897461,\n",
       " 0.32194356536864255,\n",
       " 0.30242031478880854,\n",
       " 0.32989604187010735,\n",
       " 0.33295746612547844,\n",
       " 0.31492036437987292,\n",
       " 0.3205615425109764,\n",
       " 0.33539001083373027,\n",
       " 0.34644076538084939,\n",
       " 0.33222175216673805,\n",
       " 0.31691202926634743,\n",
       " 0.31465820693968727,\n",
       " 0.32447105026244116,\n",
       " 0.32838272857665013,\n",
       " 0.34464459609984349,\n",
       " 0.33368448257445282,\n",
       " 0.34609211730956024,\n",
       " 0.34012987136839812,\n",
       " 0.34854272460936492,\n",
       " 0.3682732696533102,\n",
       " 0.38501571655272421,\n",
       " 0.37445558166502885,\n",
       " 0.38759597015379837,\n",
       " 0.38773797607420851,\n",
       " 0.39260184097289014,\n",
       " 0.38986051177977488,\n",
       " 0.41532481384276315,\n",
       " 0.39481597137450142,\n",
       " 0.38841059112547799,\n",
       " 0.41458387374876898,\n",
       " 0.41621696472166936,\n",
       " 0.40925572967528262,\n",
       " 0.38173628616331973,\n",
       " 0.38354042434691343,\n",
       " 0.3873968963622943,\n",
       " 0.37942536544798766,\n",
       " 0.37892891693114195,\n",
       " 0.38590031814574155,\n",
       " 0.36967543029784117,\n",
       " 0.376077125549306,\n",
       " 0.37502991485594661,\n",
       " 0.39386832046507747,\n",
       " 0.39691712951659114,\n",
       " 0.39063256454466727,\n",
       " 0.40693503952025317,\n",
       " 0.40687740707396408,\n",
       " 0.42135624694823165,\n",
       " 0.41922079849242111,\n",
       " 0.42219726943968672,\n",
       " 0.42094531631468673,\n",
       " 0.36709466171263594,\n",
       " 0.34907712173460859,\n",
       " 0.31378306961058511,\n",
       " 0.29631872940062415,\n",
       " 0.27898246002196203,\n",
       " 0.26455464172362214,\n",
       " 0.25903590011595612,\n",
       " 0.25402951812743069,\n",
       " 0.2388142814636123,\n",
       " 0.23890784072874902,\n",
       " 0.22961222076414939,\n",
       " 0.21948121261595599,\n",
       " 0.22159343719481339,\n",
       " 0.23476446151732316,\n",
       " 0.25272624206541888,\n",
       " 0.25467527389525285,\n",
       " 0.24893217849730359,\n",
       " 0.25907838821410045,\n",
       " 0.25024250030516493,\n",
       " 0.25298119354246956,\n",
       " 0.23967774581908088,\n",
       " 0.2120575904846082,\n",
       " 0.21247721481322146,\n",
       " 0.22776666259764525,\n",
       " 0.22108879089354366,\n",
       " 0.21883091735838739,\n",
       " 0.2263921585082897,\n",
       " 0.24272057723997911,\n",
       " 0.25443363571165878,\n",
       " 0.22343962097166853,\n",
       " 0.24199600982664898,\n",
       " 0.24947314071654156,\n",
       " 0.27201968383787944,\n",
       " 0.28427352905272318,\n",
       " 0.31207574462889504,\n",
       " 0.28924322319029638,\n",
       " 0.29227529716490575,\n",
       " 0.26696210670470066,\n",
       " 0.25735516166685884,\n",
       " 0.27453147315977872,\n",
       " 0.30457403755186857,\n",
       " 0.31675686454771818,\n",
       " 0.34286284446715176,\n",
       " 0.32760078620909511,\n",
       " 0.35814615821837242,\n",
       " 0.3528431510925179,\n",
       " 0.36460441207884603,\n",
       " 0.38717226409910965,\n",
       " 0.40077413940428541,\n",
       " 0.40074981307982249,\n",
       " 0.41105932617186347,\n",
       " 0.41878153228758613,\n",
       " 0.44370655822752753,\n",
       " 0.43572811508177556,\n",
       " 0.42466030502318181,\n",
       " 0.42891423416136537,\n",
       " 0.42089778518675597,\n",
       " 0.43247616577147274,\n",
       " 0.42290792846678521,\n",
       " 0.41504185867308402,\n",
       " 0.40806691360472463,\n",
       " 0.41096728134154104,\n",
       " 0.41187656021116992,\n",
       " 0.39639019393919722,\n",
       " 0.40269479370116013,\n",
       " 0.39610657501219526,\n",
       " 0.37477532577513473,\n",
       " 0.37546148300169718,\n",
       " 0.37692266464232216,\n",
       " 0.36251999664305457,\n",
       " 0.3601245040893436,\n",
       " 0.33443527984617949,\n",
       " 0.32819161033629174,\n",
       " 0.31173767280577414,\n",
       " 0.29189663887022721,\n",
       " 0.26942559051512466,\n",
       " 0.27689709091185316,\n",
       " 0.27551394271849378,\n",
       " 0.2778284454345582,\n",
       " 0.27327817344664312,\n",
       " 0.28321875953673104,\n",
       " 0.29335617256163338,\n",
       " 0.28860869789122323,\n",
       " 0.30656878852843028,\n",
       " 0.30074547004698493,\n",
       " 0.328427125930774,\n",
       " 0.33864108085631106,\n",
       " 0.32761935806273196,\n",
       " 0.34720496749876711,\n",
       " 0.3386251850128052,\n",
       " 0.33265690422056887,\n",
       " 0.32238541603087162,\n",
       " 0.33566325569151612,\n",
       " 0.32554576873778074,\n",
       " 0.32051251602171626,\n",
       " 0.29959502792357173,\n",
       " 0.30781850814818107,\n",
       " 0.30490332412718496,\n",
       " 0.30335988616942128,\n",
       " 0.29853295516966538,\n",
       " 0.29417911911009509,\n",
       " 0.30112825775145252,\n",
       " 0.29589354324339584,\n",
       " 0.2993340644836302,\n",
       " 0.30945679092405987,\n",
       " 0.29150890731810286,\n",
       " 0.27096294021605205,\n",
       " 0.26853927040098857,\n",
       " 0.27067997169493385,\n",
       " 0.27202047157286352,\n",
       " 0.26253184700010962,\n",
       " 0.26431998634337134,\n",
       " 0.28733262825010963,\n",
       " 0.29738850212095924,\n",
       " 0.30400419807432838,\n",
       " 0.30354464149473853,\n",
       " 0.30682158660887426,\n",
       " 0.29476802062987034,\n",
       " 0.28233895111082735,\n",
       " 0.26790887641905486,\n",
       " 0.27536480140684783,\n",
       " 0.269298555374133,\n",
       " 0.27175189208983125,\n",
       " 0.2466615467071408,\n",
       " 0.26236380577086149,\n",
       " 0.26927087974547087,\n",
       " 0.26635512542723355,\n",
       " 0.23911183929442104,\n",
       " 0.24165262603758508,\n",
       " 0.22489668273924524,\n",
       " 0.22518909072874718,\n",
       " 0.17736526107786826,\n",
       " 0.18208222007750205,\n",
       " 0.16138233566282917,\n",
       " 0.17198801422117876,\n",
       " 0.20158272171019243,\n",
       " 0.19228349304197953,\n",
       " 0.19953271102904005,\n",
       " 0.19286828422545119,\n",
       " 0.18915902137755078,\n",
       " 0.17978995704649609,\n",
       " 0.17082022476195016,\n",
       " 0.16775125503538765,\n",
       " 0.17258959960936227,\n",
       " 0.1752447528838984,\n",
       " 0.19156357002257027,\n",
       " 0.20108834266661321,\n",
       " 0.19506471061705266,\n",
       " 0.1996034679412714,\n",
       " 0.19275277328489931,\n",
       " 0.22468232345579775,\n",
       " 0.22515277099608094,\n",
       " 0.22612821197508484,\n",
       " 0.22978396415709165,\n",
       " 0.23919567108153011,\n",
       " 0.24161916923521662,\n",
       " 0.25293656730650566,\n",
       " 0.24778501319883964,\n",
       " 0.15955097961424491,\n",
       " 0.1778243236541619,\n",
       " 0.17345174407957692,\n",
       " 0.16721276664732593,\n",
       " 0.14687315559385913,\n",
       " 0.1621553573608269,\n",
       " 0.15852408981321947,\n",
       " 0.160432149887072,\n",
       " 0.16657657051085129,\n",
       " 0.10484935951231611,\n",
       " 0.10264209747313151,\n",
       " 0.085647958755480119,\n",
       " 0.087939395904527956,\n",
       " 0.090722894668566048,\n",
       " 0.05262210273741369,\n",
       " 0.06257151222227697,\n",
       " 0.089893093109117778,\n",
       " 0.11366274070738437,\n",
       " 0.11956092643736482,\n",
       " 0.13573259925840972,\n",
       " 0.1563232212066519,\n",
       " 0.18246659660338038,\n",
       " 0.16704073905943506,\n",
       " 0.18120960426329247,\n",
       " 0.15660191154478659,\n",
       " 0.16912229347227681,\n",
       " 0.19183236122130023,\n",
       " 0.17481855773924457,\n",
       " 0.18718597221373184,\n",
       " 0.20000104331968885,\n",
       " 0.22344212341307262,\n",
       " 0.23038362693785289,\n",
       " 0.23879270172117806,\n",
       " 0.2319485321044788,\n",
       " 0.24873025894163697,\n",
       " 0.27150356101988404,\n",
       " 0.24013491821287719,\n",
       " 0.23877799606321898,\n",
       " 0.23057550430296506,\n",
       " 0.23596066665648066,\n",
       " 0.24161717987059197,\n",
       " 0.23931776809691033,\n",
       " 0.25862932205198846,\n",
       " 0.25134636688231071,\n",
       " 0.24580252838133412,\n",
       " 0.26209900665281849,\n",
       " 0.28246334648130966,\n",
       " 0.29950598335264755,\n",
       " 0.3014907112121446,\n",
       " 0.31518824386595318,\n",
       " 0.28644219398497173,\n",
       " 0.28060054588316508,\n",
       " 0.26640822029112404,\n",
       " 0.21190995979307717,\n",
       " 0.19865820121763769,\n",
       " 0.21020452880858007,\n",
       " 0.20263975143431245,\n",
       " 0.16895886611937105,\n",
       " 0.11157918548582613,\n",
       " 0.1175968894958359,\n",
       " 0.11874285888670502,\n",
       " 0.11548861312864836,\n",
       " 0.11366293907164152,\n",
       " 0.12073970413206631,\n",
       " 0.13447089767454679,\n",
       " 0.14150517463682705,\n",
       " 0.14843205833433679,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_real_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
