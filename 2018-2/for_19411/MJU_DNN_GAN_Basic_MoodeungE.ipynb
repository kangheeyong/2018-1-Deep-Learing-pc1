{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GAN Basic\n",
    "\n",
    "- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks(https://arxiv.org/pdf/1511.06434.pdf)\n",
    "\n",
    "<img src=\"./GAN.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla GAN with Multi GPUs + Naming Layers using OrderedDict\n",
    "# Code by GunhoChoi\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import os\n",
    " \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set Hyperparameters\n",
    "# change num_gpu to the number of gpus you want to use\n",
    "\n",
    "epoch = 50\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "num_gpus = 1\n",
    "z_size = 50\n",
    "middle_size = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "\n",
    "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "\n",
    "# Set Data Loader(input pipeline)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator receives random noise z and create 1x28x28 image\n",
    "# we can name each layer using OrderedDict\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "                        ('fc1',nn.Linear(z_size,middle_size)),\n",
    "                        ('bn1',nn.BatchNorm1d(middle_size)),\n",
    "                        ('act1',nn.ReLU()),\n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "                        ('fc2', nn.Linear(middle_size,784)),\n",
    "                        ('tanh', nn.Tanh()),\n",
    "        ]))\n",
    "    def forward(self,z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size,1,28,28)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator receives 1x28x28 image and returns a float number 0~1\n",
    "# we can name each layer using OrderedDict\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "                        ('fc1',nn.Linear(784,middle_size)),\n",
    "                        ('bn1',nn.BatchNorm1d(middle_size)),\n",
    "                        ('act1',nn.LeakyReLU()),  \n",
    "            \n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "                        ('fc2', nn.Linear(middle_size,1)),\n",
    "                        ('act2', nn.Sigmoid()),\n",
    "        ]))\n",
    "                                    \n",
    "    def forward(self,x):\n",
    "        out = x.view(batch_size, -1)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Put instances on Multi-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put class objects on Multiple GPUs using \n",
    "# torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
    "# device_ids: default all devices / output_device: default device 0 \n",
    "# along with .cuda()\n",
    "\n",
    "generator = nn.DataParallel(Generator()).to(device)\n",
    "discriminator = nn.DataParallel(Discriminator()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function, optimizers, and labels for training\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "gen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
    "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
    "\n",
    "ones_label = torch.ones(batch_size,1).to(device)\n",
    "zeros_label = torch.zeros(batch_size,1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Restore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------model not restored--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model restore if any\n",
    "\n",
    "try:\n",
    "    generator, discriminator = torch.load('./model/vanilla_gan.pkl')\n",
    "    print(\"\\n--------model restored--------\\n\")\n",
    "except:\n",
    "    print(\"\\n--------model not restored--------\\n\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2540, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5051, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "0th iteration gen_loss: 0.2540019452571869 dis_loss: 0.505119800567627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1326, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5208, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "0th iteration gen_loss: 0.13257287442684174 dis_loss: 0.5208386778831482\n",
      "tensor(0.1283, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5244, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "1th iteration gen_loss: 0.1282747983932495 dis_loss: 0.5243715047836304\n",
      "tensor(0.1253, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5157, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "1th iteration gen_loss: 0.1253025084733963 dis_loss: 0.5156772136688232\n",
      "tensor(0.1261, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5141, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "2th iteration gen_loss: 0.12609431147575378 dis_loss: 0.514056384563446\n",
      "tensor(0.1323, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4976, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "2th iteration gen_loss: 0.13225622475147247 dis_loss: 0.4976121187210083\n",
      "tensor(0.1330, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4933, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "3th iteration gen_loss: 0.13295504450798035 dis_loss: 0.4933001399040222\n",
      "tensor(0.1421, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4712, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "3th iteration gen_loss: 0.1420915275812149 dis_loss: 0.4712487459182739\n",
      "tensor(0.1441, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4653, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "4th iteration gen_loss: 0.14414235949516296 dis_loss: 0.4652981162071228\n",
      "tensor(0.1573, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4316, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "4th iteration gen_loss: 0.15726222097873688 dis_loss: 0.4315962791442871\n",
      "tensor(0.1596, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4258, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "5th iteration gen_loss: 0.1596299409866333 dis_loss: 0.4258303940296173\n",
      "tensor(0.1748, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3938, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "5th iteration gen_loss: 0.1747506558895111 dis_loss: 0.39381343126296997\n",
      "tensor(0.1769, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3857, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "6th iteration gen_loss: 0.17690810561180115 dis_loss: 0.3856567442417145\n",
      "tensor(0.1899, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3541, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "6th iteration gen_loss: 0.18987038731575012 dis_loss: 0.35406139492988586\n",
      "tensor(0.1954, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3473, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "7th iteration gen_loss: 0.195409893989563 dis_loss: 0.34725481271743774\n",
      "tensor(0.2069, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3242, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "7th iteration gen_loss: 0.20685862004756927 dis_loss: 0.32420358061790466\n",
      "tensor(0.2076, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3250, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "8th iteration gen_loss: 0.20761078596115112 dis_loss: 0.3249765634536743\n",
      "tensor(0.2162, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3054, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "8th iteration gen_loss: 0.21618589758872986 dis_loss: 0.30537235736846924\n",
      "tensor(0.2203, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3035, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "9th iteration gen_loss: 0.22028286755084991 dis_loss: 0.303475022315979\n",
      "tensor(0.2274, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2939, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "9th iteration gen_loss: 0.22742128372192383 dis_loss: 0.2939193546772003\n",
      "tensor(0.2268, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2927, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "10th iteration gen_loss: 0.2267857789993286 dis_loss: 0.2927178740501404\n",
      "tensor(0.2332, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2831, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "10th iteration gen_loss: 0.23322564363479614 dis_loss: 0.28306254744529724\n",
      "tensor(0.2295, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2785, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "11th iteration gen_loss: 0.2295454889535904 dis_loss: 0.27847540378570557\n",
      "tensor(0.2345, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2782, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "11th iteration gen_loss: 0.23454895615577698 dis_loss: 0.2782275676727295\n",
      "tensor(0.2348, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2772, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "12th iteration gen_loss: 0.23484373092651367 dis_loss: 0.27715781331062317\n",
      "tensor(0.2353, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2736, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "12th iteration gen_loss: 0.23532292246818542 dis_loss: 0.2735937237739563\n",
      "tensor(0.2400, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2741, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "13th iteration gen_loss: 0.24001702666282654 dis_loss: 0.2741119861602783\n",
      "tensor(0.2415, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2690, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "13th iteration gen_loss: 0.24154335260391235 dis_loss: 0.2689628005027771\n",
      "tensor(0.2386, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2712, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "14th iteration gen_loss: 0.23856262862682343 dis_loss: 0.2711734473705292\n",
      "tensor(0.2368, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2693, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "14th iteration gen_loss: 0.23682045936584473 dis_loss: 0.26930442452430725\n",
      "tensor(0.2428, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2698, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "15th iteration gen_loss: 0.24282242357730865 dis_loss: 0.269835501909256\n",
      "tensor(0.2390, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2622, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "15th iteration gen_loss: 0.23902763426303864 dis_loss: 0.2621803283691406\n",
      "tensor(0.2417, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2652, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "16th iteration gen_loss: 0.24172832071781158 dis_loss: 0.26519668102264404\n",
      "tensor(0.2449, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2634, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "16th iteration gen_loss: 0.24488787353038788 dis_loss: 0.2633973956108093\n",
      "tensor(0.2443, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2636, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "17th iteration gen_loss: 0.24428415298461914 dis_loss: 0.26360023021698\n",
      "tensor(0.2486, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2620, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "17th iteration gen_loss: 0.2485925704240799 dis_loss: 0.2619916498661041\n",
      "tensor(0.2416, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2654, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "18th iteration gen_loss: 0.24159198999404907 dis_loss: 0.26540473103523254\n",
      "tensor(0.2427, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2623, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "18th iteration gen_loss: 0.24273720383644104 dis_loss: 0.2622906565666199\n",
      "tensor(0.2467, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2619, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "19th iteration gen_loss: 0.24674484133720398 dis_loss: 0.26187947392463684\n",
      "tensor(0.2453, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2630, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "19th iteration gen_loss: 0.24532565474510193 dis_loss: 0.2630257308483124\n",
      "tensor(0.2428, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2603, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "20th iteration gen_loss: 0.24281832575798035 dis_loss: 0.2602996528148651\n",
      "tensor(0.2450, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2616, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "20th iteration gen_loss: 0.24498096108436584 dis_loss: 0.2616347372531891\n",
      "tensor(0.2459, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2574, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "21th iteration gen_loss: 0.24590736627578735 dis_loss: 0.2574487030506134\n",
      "tensor(0.2469, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2590, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "21th iteration gen_loss: 0.24691787362098694 dis_loss: 0.25899529457092285\n",
      "tensor(0.2458, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2588, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "22th iteration gen_loss: 0.2457900494337082 dis_loss: 0.25875502824783325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2467, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2597, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "22th iteration gen_loss: 0.24673984944820404 dis_loss: 0.2597169578075409\n",
      "tensor(0.2439, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2573, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "23th iteration gen_loss: 0.24390514194965363 dis_loss: 0.25725656747817993\n",
      "tensor(0.2525, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2576, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "23th iteration gen_loss: 0.2525014579296112 dis_loss: 0.2576148211956024\n",
      "tensor(0.2433, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2561, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "24th iteration gen_loss: 0.2432975023984909 dis_loss: 0.25613516569137573\n",
      "tensor(0.2479, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2593, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "24th iteration gen_loss: 0.24792388081550598 dis_loss: 0.2592852711677551\n",
      "tensor(0.2489, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2572, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "25th iteration gen_loss: 0.24885942041873932 dis_loss: 0.25722765922546387\n",
      "tensor(0.2460, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2565, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "25th iteration gen_loss: 0.2459774762392044 dis_loss: 0.256547212600708\n",
      "tensor(0.2479, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2569, device='cuda:0', grad_fn=<ThAddBackward>)\n",
      "26th iteration gen_loss: 0.24792928993701935 dis_loss: 0.25694504380226135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c71af3c9fea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;31m# PIL image mode: L, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j,(image,label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        \n",
    "        # discriminator\n",
    "\n",
    "        z = init.normal(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        dis_real = discriminator.forward(image)\n",
    "        dis_loss = torch.sum(loss_func(dis_fake,zeros_label)) + torch.sum(loss_func(dis_real,ones_label))\n",
    "        dis_loss.backward(retain_graph=True)\n",
    "        \n",
    "        dis_optim.step()\n",
    "        dis_optim.zero_grad()\n",
    "        \n",
    "        # generator\n",
    "\n",
    "        \n",
    "        z = init.normal(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        gen_loss = torch.sum(loss_func(dis_fake,ones_label)) # fake classified as real\n",
    "        gen_loss.backward()\n",
    "        \n",
    "        gen_optim.step()\n",
    "        gen_optim.zero_grad()\n",
    "    \n",
    "       \n",
    "        # model save\n",
    "        if j % 100 == 0:\n",
    "            print(gen_loss,dis_loss)\n",
    "        \n",
    "            print(\"{}th iteration gen_loss: {} dis_loss: {}\".format(i,gen_loss.data,dis_loss.data))\n",
    "            v_utils.save_image(gen_fake.data[0:25],\"gen_{}_{}.png\".format(i,j), nrow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
