{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no normalizing version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('data.txt')\n",
    "\n",
    "i = 1\n",
    "\n",
    "\n",
    "txt_file_name = 'pr_v3_GB_rand_5_20.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#txt_file_name = 'sample_0.txt'\n",
    "\n",
    "\n",
    "def test_1(func, x, y, k, file_name = \"default\") :\n",
    "    \n",
    "\n",
    "    \n",
    "    L = x.shape[0]\n",
    "       \n",
    "    if k >= L :\n",
    "        print('error\\n')\n",
    "        return -1\n",
    "    \n",
    "    if L%k == 0 :\n",
    "        d = int(L/k) - 1\n",
    "    else :\n",
    "        d = int(L/k)\n",
    "      \n",
    "    mse = 0.0\n",
    "    \n",
    "    \n",
    "    minimum = 1000.0\n",
    "    maximum = -1000.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0,d) :\n",
    "        \n",
    "        x_test = x[i*k:(i+1)*k]\n",
    "        y_test = y[i*k:(i+1)*k]\n",
    "        \n",
    "        x_train = np.concatenate([x[0 : i*k], x[(i+1)*k : L]], axis=0)\n",
    "        y_train = np.concatenate([y[0 : i*k], y[(i+1)*k : L]], axis=0)\n",
    "        \n",
    "        func.fit(x_train, y_train)\n",
    "        #print(\"%d MSE : %f\" %(i+1,mean_squared_error(y_test, func.predict(x_test))))\n",
    "      \n",
    "        temp = mean_squared_error(y_test, func.predict(x_test))\n",
    "        \n",
    "   \n",
    "        \n",
    "        if temp < minimum :\n",
    "            minimum_train = np.concatenate([x_train, np.reshape(y_train,(-1,1))], axis=1)\n",
    "            minimum_test = np.concatenate([x_test, np.reshape(y_test,(-1,1))], axis=1)\n",
    "            minimum = temp\n",
    "            \n",
    "        if temp > maximum :\n",
    "            maximum_train = np.concatenate([x_train,np.reshape(y_train,(-1,1))],axis=1)\n",
    "            maximum_test = np.concatenate([x_test,np.reshape(y_test,(-1,1))],axis=1)\n",
    "            maximum = temp\n",
    "            \n",
    "        mse = mse + temp\n",
    "        \n",
    "    x_test = x[d*k:L]\n",
    "    y_test = y[d*k:L]\n",
    "    x_train = x[0:d*k]\n",
    "    y_train = y[0:d*k]\n",
    "    \n",
    "    \n",
    "    \n",
    "    func.fit(x_train, y_train)\n",
    "    \n",
    "    #print(\"%d MSE : %f\"%(d+1, mean_squared_error(y_test, func.predict(x_test))))\n",
    "    \n",
    "    temp = mean_squared_error(y_test, func.predict(x_test))\n",
    "    \n",
    "    if temp < minimum :\n",
    "        minimum_train = np.concatenate([x_train,np.reshape(y_train,(-1,1))],axis=1)\n",
    "        minimum_test = np.concatenate([x_test,np.reshape(y_test,(-1,1))],axis=1)\n",
    "        minimum = temp\n",
    "            \n",
    "    if temp > maximum :\n",
    "        \n",
    "        maximum_train = np.concatenate([x_train, np.reshape(y_train,(-1,1))],axis=1)\n",
    "        maximum_test = np.concatenate([x_test,np.reshape(y_test,(-1,1))],axis=1)\n",
    "        maximum = temp\n",
    "    \n",
    "    mse = mse + temp\n",
    "    \n",
    "    \n",
    "    f=open(txt_file_name,'ab')\n",
    "\n",
    "    np.savetxt(f, np.array([[L, maximum, minimum]]),fmt = '%.6f')\n",
    "    print(\"%d %.4f %.4f\"%(L,maximum, minimum))\n",
    "    \n",
    "    if abs(maximum - minimum) < 0.1 :\n",
    "        \n",
    "        s = \"pr_v3_GB_rand_5_20_%d.txt\"%i\n",
    "        np.savetxt(s, np.concatenate([x,np.reshape(y,(-1,1))],axis=1) ,fmt = '%.6f')\n",
    "        i = i + 1\n",
    "    \n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    return mse/(d+1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 5.2382 0.8843\n",
      "27 4.5187 0.0360\n",
      "28 0.6718 0.0327\n",
      "28 4.1629 0.0907\n",
      "27 5.8487 0.0264\n",
      "21 1.8118 0.1166\n",
      "30 5.8574 0.1220\n",
      "23 3.8423 0.0724\n",
      "22 4.1752 0.0391\n",
      "27 5.8487 0.0264\n",
      "20 5.1775 0.0352\n",
      "28 4.1629 0.0907\n",
      "28 4.2542 0.0223\n",
      "28 5.1312 0.0843\n",
      "22 4.7844 0.4879\n",
      "25 2.2077 0.0399\n",
      "20 3.4034 0.2359\n",
      "22 9.0091 0.1623\n",
      "23 0.5799 0.0481\n",
      "23 3.4128 0.2331\n",
      "17 2.2545 0.7185\n",
      "31 5.9803 0.0381\n",
      "28 5.1312 0.0843\n",
      "28 1.7650 0.0987\n",
      "25 2.2077 0.0399\n",
      "24 5.1600 0.1049\n",
      "18 1.8112 0.3948\n",
      "17 23.0162 0.1303\n",
      "25 5.1831 0.0550\n",
      "20 1.3787 0.1879\n",
      "23 3.8423 0.0724\n",
      "18 1.7041 0.0925\n",
      "31 4.5906 0.0461\n",
      "31 2.0013 0.0911\n",
      "30 7.8010 0.0224\n",
      "18 3.1800 0.0877\n",
      "19 4.9361 0.5288\n",
      "17 2.2054 0.0528\n",
      "18 1.7041 0.0925\n",
      "30 5.8574 0.1220\n",
      "25 5.1831 0.0550\n",
      "24 5.1600 0.1049\n",
      "31 7.3250 0.2176\n",
      "31 5.9803 0.0381\n",
      "20 5.6906 0.2117\n",
      "27 6.0432 0.1014\n",
      "20 4.6273 0.3834\n",
      "23 8.4714 0.1858\n",
      "31 7.3250 0.2176\n",
      "18 8.6029 0.0934\n",
      "18 4.9936 0.1116\n",
      "22 6.3050 0.1487\n",
      "22 4.1752 0.0391\n",
      "28 0.6718 0.0327\n",
      "22 4.1752 0.0391\n",
      "21 1.5855 0.0732\n",
      "30 4.5581 0.0299\n",
      "25 5.3320 0.0618\n",
      "26 4.7334 0.3124\n",
      "17 20.4107 0.1422\n",
      "23 3.4128 0.2331\n",
      "20 5.6058 0.9603\n",
      "31 6.1273 0.1105\n",
      "26 4.7334 0.3124\n",
      "25 6.2792 0.0432\n",
      "26 4.8313 0.0193\n",
      "20 5.6058 0.9603\n",
      "23 3.1520 0.2050\n",
      "30 7.8010 0.0224\n",
      "23 3.1520 0.2050\n",
      "23 0.5799 0.0481\n",
      "25 4.5086 0.0527\n",
      "23 4.4261 0.1037\n",
      "28 4.2769 0.0455\n",
      "21 8.4233 0.1283\n",
      "30 4.5121 0.0149\n",
      "26 4.8313 0.0193\n",
      "20 1.3787 0.1879\n",
      "22 9.0091 0.1623\n",
      "22 6.3050 0.1487\n",
      "20 5.1775 0.0352\n",
      "30 4.5121 0.0149\n",
      "24 6.1193 0.1398\n",
      "23 0.5799 0.0481\n",
      "25 6.2792 0.0432\n",
      "22 11.7826 0.2995\n",
      "26 1.2775 0.0527\n",
      "17 5.6464 0.7185\n",
      "24 5.1600 0.1049\n",
      "19 4.4691 0.5522\n",
      "26 4.7334 0.3124\n",
      "24 4.6198 0.0990\n",
      "21 3.9025 0.1729\n",
      "31 4.7504 0.0839\n",
      "31 6.1273 0.1105\n",
      "20 5.6906 0.2117\n",
      "17 2.1095 0.0000\n",
      "24 4.3833 0.0756\n",
      "29 4.6724 0.0112\n",
      "17 20.4107 0.1422\n",
      "24 5.1600 0.1049\n",
      "19 1.5215 0.1906\n",
      "17 4.3411 0.0030\n",
      "31 7.3250 0.2176\n",
      "28 6.1693 0.0768\n",
      "18 2.5336 0.1220\n",
      "31 5.8174 0.0529\n",
      "24 4.2702 0.0222\n",
      "31 6.1273 0.1105\n",
      "22 4.7844 0.4879\n",
      "19 4.9361 0.5288\n",
      "29 4.6724 0.0112\n",
      "17 4.2372 0.1418\n",
      "19 4.9361 0.5288\n",
      "19 4.6420 0.1915\n",
      "31 4.5906 0.0461\n",
      "29 4.4129 0.0357\n",
      "19 1.5215 0.1906\n",
      "29 4.7682 0.0217\n",
      "23 7.8810 0.2038\n",
      "23 8.4714 0.1858\n",
      "30 5.7494 0.0486\n",
      "26 5.1083 0.0054\n",
      "20 1.3787 0.1879\n",
      "31 4.7504 0.0839\n",
      "17 4.6207 0.0109\n",
      "26 5.1083 0.0054\n",
      "17 6.4716 0.2795\n",
      "20 3.4034 0.2359\n",
      "19 3.5634 0.0894\n",
      "23 5.4853 0.2204\n",
      "29 0.8150 0.0478\n",
      "25 6.2792 0.0432\n",
      "26 4.3917 0.1021\n",
      "22 2.4417 0.0254\n",
      "17 6.4716 0.2795\n",
      "24 2.0610 0.1348\n",
      "22 11.7826 0.2995\n",
      "19 4.4691 0.5522\n",
      "22 2.8269 0.1204\n",
      "19 3.5634 0.0894\n",
      "19 4.2422 0.1471\n",
      "28 1.8028 0.0735\n",
      "29 5.0275 0.1448\n",
      "31 6.1273 0.1105\n",
      "23 2.0626 0.0602\n",
      "19 3.5634 0.0894\n",
      "22 2.4417 0.0254\n",
      "26 1.2775 0.0527\n",
      "30 2.5170 0.0534\n",
      "18 6.0991 0.3054\n",
      "22 6.3050 0.1487\n",
      "23 2.0626 0.0602\n",
      "31 7.3250 0.2176\n",
      "26 4.8313 0.0193\n",
      "17 2.1095 0.0000\n",
      "25 4.5086 0.0527\n",
      "30 4.6981 0.0444\n",
      "21 6.0537 0.0030\n",
      "18 1.8112 0.3948\n",
      "29 1.8558 0.0498\n",
      "24 4.6198 0.0990\n",
      "26 4.3917 0.1021\n",
      "23 7.8810 0.2038\n",
      "31 5.8174 0.0529\n",
      "30 4.4339 0.0457\n",
      "23 1.5323 0.1649\n",
      "26 5.9962 0.0872\n",
      "31 5.8174 0.0529\n",
      "20 4.2421 0.0478\n",
      "22 11.7826 0.2995\n",
      "29 4.4129 0.0357\n",
      "25 4.0402 0.0213\n",
      "31 7.3250 0.2176\n",
      "22 2.4417 0.0254\n",
      "17 4.2372 0.1418\n",
      "25 5.3320 0.0618\n",
      "31 7.3250 0.2176\n",
      "22 7.5045 0.1325\n",
      "31 7.3250 0.2176\n",
      "23 4.4261 0.1037\n",
      "28 1.8028 0.0735\n",
      "26 4.8313 0.0193\n",
      "17 5.9207 0.4960\n",
      "17 6.4716 0.2795\n",
      "22 7.5045 0.1325\n",
      "27 6.0432 0.1014\n",
      "26 4.8313 0.0193\n",
      "30 5.9643 0.0860\n",
      "30 8.6989 0.0752\n",
      "29 4.7181 0.0453\n",
      "25 16.5970 0.5019\n",
      "24 5.0514 0.5528\n",
      "22 2.4417 0.0254\n",
      "24 4.6198 0.0990\n",
      "25 4.0402 0.0213\n",
      "17 4.2372 0.1418\n",
      "24 4.6198 0.0990\n",
      "22 6.3941 0.2559\n",
      "20 4.2421 0.0478\n",
      "29 4.3059 0.0212\n",
      "29 4.4129 0.0357\n",
      "30 8.6989 0.0752\n",
      "29 4.4915 0.0160\n",
      "25 6.2792 0.0432\n",
      "27 5.6690 0.0908\n",
      "28 4.6854 0.0470\n",
      "30 4.3966 0.0102\n",
      "22 4.7035 0.3404\n",
      "30 5.9643 0.0860\n",
      "26 5.3162 0.0384\n",
      "30 4.4081 0.0041\n",
      "29 2.3488 0.0811\n",
      "28 1.3841 0.0948\n",
      "27 4.6234 0.0776\n",
      "24 4.2702 0.0222\n",
      "25 5.3322 0.1099\n",
      "26 5.9962 0.0872\n",
      "30 4.4339 0.0457\n",
      "31 4.7363 0.2549\n",
      "27 4.6234 0.0776\n",
      "31 5.8174 0.0529\n",
      "18 6.1414 0.0065\n",
      "30 8.6989 0.0752\n",
      "31 4.4895 0.0328\n",
      "24 4.2702 0.0222\n",
      "29 4.7682 0.0217\n",
      "22 4.7035 0.3404\n",
      "19 1.4528 0.1645\n",
      "19 1.4528 0.1645\n",
      "18 5.7597 0.5414\n",
      "26 4.6302 0.1006\n",
      "17 2.1095 0.0000\n",
      "17 5.9207 0.4960\n",
      "24 4.2702 0.0222\n",
      "24 4.9766 0.1998\n",
      "22 11.7826 0.2995\n",
      "29 1.4289 0.0319\n",
      "20 4.4288 0.1338\n",
      "22 0.5451 0.1303\n",
      "28 1.3841 0.0948\n",
      "17 4.3411 0.0030\n",
      "29 4.4839 0.0392\n",
      "22 6.1020 0.0750\n",
      "31 4.4895 0.0328\n",
      "25 5.3322 0.1099\n",
      "29 2.3488 0.0811\n",
      "22 4.9709 0.0122\n",
      "24 5.0514 0.5528\n",
      "30 4.3966 0.0102\n",
      "17 3.1365 0.0445\n",
      "30 5.1037 0.0697\n",
      "23 7.8810 0.2038\n",
      "26 5.9962 0.0872\n",
      "17 2.4467 0.0002\n",
      "22 6.3456 0.1173\n",
      "31 1.7761 0.0705\n",
      "18 6.0991 0.3054\n",
      "25 4.0402 0.0213\n",
      "30 4.3966 0.0102\n",
      "19 4.1966 0.0198\n",
      "27 8.0241 0.0738\n",
      "24 3.2752 0.0586\n",
      "29 2.1657 0.0594\n",
      "22 6.3941 0.2559\n",
      "26 4.6302 0.1006\n",
      "22 6.4681 0.1426\n",
      "17 3.1365 0.0445\n",
      "20 2.7510 0.1080\n",
      "29 2.1657 0.0594\n",
      "18 4.4483 0.0385\n",
      "25 4.0402 0.0213\n",
      "28 6.2698 0.0289\n",
      "20 3.9696 0.2501\n",
      "29 4.7387 0.0838\n",
      "19 6.3037 1.0330\n",
      "22 4.9709 0.0122\n",
      "29 2.1657 0.0594\n",
      "17 4.8692 0.0471\n",
      "30 4.6915 0.0948\n",
      "17 3.8680 0.0243\n",
      "17 2.1070 0.0827\n",
      "22 3.0982 0.1346\n",
      "26 5.3162 0.0384\n",
      "20 4.4140 0.0298\n",
      "23 3.3247 0.0145\n",
      "26 4.6302 0.1006\n",
      "18 5.7597 0.5414\n",
      "17 2.0016 0.0632\n",
      "27 4.8028 0.0178\n",
      "17 2.1070 0.0827\n",
      "30 6.3201 0.0009\n",
      "17 3.1365 0.0445\n",
      "28 4.5698 0.1515\n",
      "19 4.9931 0.1769\n",
      "31 4.1523 0.0526\n",
      "28 4.3510 0.4225\n",
      "24 4.5791 0.1097\n",
      "26 5.9962 0.0872\n",
      "22 4.7035 0.3404\n",
      "31 4.7363 0.2549\n",
      "28 6.2698 0.0289\n",
      "18 6.0991 0.3054\n",
      "28 1.6071 0.0283\n",
      "30 4.4081 0.0041\n",
      "28 1.1829 0.0895\n",
      "17 2.4467 0.0002\n",
      "25 5.3555 0.0638\n",
      "28 4.5698 0.1515\n",
      "18 5.1765 0.1174\n",
      "26 10.6088 0.0441\n",
      "21 6.0537 0.0030\n",
      "29 4.4839 0.0392\n",
      "28 4.3510 0.4225\n",
      "21 4.0639 0.0001\n",
      "24 2.0920 0.1383\n",
      "21 4.0639 0.0001\n",
      "27 4.8028 0.0178\n",
      "26 10.6088 0.0441\n",
      "24 5.3984 0.7486\n",
      "25 4.6663 0.0274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "params = {'n_estimators': 500, 'max_depth': 6, 'min_samples_split': 4,\n",
    "          'learning_rate': 0.005, 'loss': 'ls'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 10000) : \n",
    "\n",
    "    data = np.genfromtxt('data.txt')\n",
    "    np.random.seed(int(time.time())+np.random.randint(1,100))\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    aaa = np.random.randint(5,20)\n",
    "    \n",
    "    X_train = data[: - aaa,0:6]\n",
    "    Y_train = data[: - aaa,6]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    GBR = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "\n",
    "\n",
    "    mse = test_1(GBR, X_train, Y_train, k = 4, file_name = \"Gradient_Boosting_Regression\")\n",
    "\n",
    "#print(\"MSE: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.random.randint(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = 'test'\n",
    "\n",
    "#f=open('temp.txt','ab')\n",
    "\n",
    "#np.savetxt(f, np.array([c]),fmt = '%s')\n",
    "#np.savetxt(f, d, delimiter=' ',fmt = '%.3f')\n",
    "\n",
    "#f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "\n",
    "print(\"sdsd_%d\"%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
